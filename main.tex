                                                                \documentclass[10pt,a4paper,oneside]{book}
\usepackage[a4paper,includeheadfoot,top=5mm,bottom=10mm,left=10mm,right=10mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{cmap} % Поддержка поиска русских слов в PDF (pdflatex)
\usepackage{comment}
%\usepackage{pdfsync} % синхронизация

\usepackage{amsmath,amsthm,amssymb,amscd,array}
\usepackage{latexsym}
\usepackage{stmaryrd} % Для знака нормальной подгруппы
\usepackage{misccorr} % российская полиграфия
\usepackage{indentfirst}% Красная строка в первом абзаце
\usepackage{ccaption} % Заголовки таблиц и рисунков
\usepackage{fancyhdr} % колонтитулы
\usepackage{hyperref} % гиперссылки

\usepackage{rotating} % Поворот текста
\usepackage{graphicx} % Вставка изображений
\usepackage{xcolor}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{arrows,backgrounds,patterns,matrix,shapes,fit,calc,shadows,plotmarks}
\graphicspath{ {./} } % относительно main.tex

\usepackage{arydshln} % штрихованные линии в массивах
\usepackage{mathtools} % выравнивание в матрицах
\usepackage{multirow} % слияние в столбце
\usepackage{multicol} % нумерация в нескольких колонках


\newtheorem{uprz}{\color{violet!100!black} Упражнение}
\newtheorem{predl}{\color{blue!50!black} Предложение}
\newtheorem{komment}{\color{green!50!blue} Комментарий}
\newtheorem{conj}{Гипотеза}
\newtheorem{notation}{\color{yellow!30!red} Обозначение}


\theoremstyle{definition}
\newtheorem{kit}{Кит}
\newtheorem*{rem}{\color{green!50!blue}Замечание}
\newtheorem{zad}{\color{violet!100!black}Задача}
\newtheorem*{defn}{\color{yellow!30!red} Определение}
\newtheorem*{fact}{Факт}
\newtheorem{thm}{\color{red!40!black}Теорема}
\newtheorem*{thmm}{\color{red!40!black} Теорема}
\newtheorem{lem}{\color{green!50!black}Лемма}
\newtheorem{cor}{\color{green!45!black}Следствие}
\newtheorem{utvr}{\color{blue!50!black}Утверждение}


\newcommand\tikznode[3][]%
   {\tikz[remember picture,baseline=(#2.base)]
      \node[minimum size=0pt,outer sep=0pt,#1](#2){#3};%
   }
\tikzset{>=stealth}


\hypersetup{
    colorlinks,
    linkcolor={blue!50!black},
    citecolor={blue!50!black},
    urlcolor={red!80!black}
}
% цвета для ссылок


\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother


\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\proofname}{Доказательство}
\renewcommand{\mod}{\,\operatorname{mod}\,}
\renewcommand{\Re}{\operatorname{Re}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\ovl}{\overline}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\K}{\operatorname{K_0}}
\newcommand{\witt}{\operatorname{W}}
\newcommand{\gw}{\operatorname{GW}}
\newcommand{\coh}{\operatorname{H}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\cl}{\operatorname{Cl}}
\newcommand{\Vol}{\operatorname{Vol}}
\newcommand\tgg{\mathop{\rm tg}\nolimits}
\newcommand\ccup{\mathop{\cup}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\chr}{\operatorname{char}}
\newcommand{\rk}{\operatorname{rk}}
\DeclareMathOperator{\Coker}{Coker}
\DeclareMathOperator{\Ker}{Ker}
\newcommand{\im}{\operatorname{Im}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\re}{\operatorname{Re}}
\newcommand{\tr}{\operatorname{Tr}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\Stab}{\operatorname{Stab}}
\newcommand{\orb}{\operatorname{\mathcal O}}
\newcommand{\Fix}{\operatorname{Fix}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\End}{\operatorname{End}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\Out}{\operatorname{Out}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\renewcommand{\O}{\operatorname{O}}
\renewcommand{\U}{\operatorname{U}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\Adj}{\operatorname{Adj}}
\newcommand{\Disc}{\operatorname{Disc}}
\newcommand{\cnt}{\operatorname{cont}}
\newcommand{\Frob}{\operatorname{Frob}}
\newcommand{\Iso}{\operatorname{Iso}}
\newcommand{\Isom}{\operatorname{Isom}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\di}{\mathop{\,\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\ndi}{\mathop{\not\scalebox{0.85}{\raisebox{-1.2pt}[0.5\height]{\vdots}}\,}}
\newcommand{\nequiv}{\not \equiv}
\newcommand{\Nod}{\operatorname{\text{НОД}}}
\newcommand{\Nok}{\operatorname{\text{НОК}}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\Aff}{\operatorname{Aff}}
\newcommand{\AGL}{\operatorname{AGL}}
\newcommand{\PSL}{\operatorname{PSL}}
\newcommand{\Volume}{\operatorname{Volume}}

\def\llq{\textquotedblleft} 
\def\rrq{\textquotedblright} 
\def\exm{\noindent {\bf Примеры:}}


\def\Cb{\ovl{C}}
\def\ffi{\varphi}
\def\pa{\partial}
\def\V{\bf V}
\def\La{\Lambda}
\def\eps{\varepsilon}
\def\del{\delta}
\def\Del{\Delta}
\def\A{\EuScript{A}}
\def\lan{\left\langle }
\def\ran{\right\rangle}
\def\bar{\begin{array}}
\def\ear{\end{array}}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\thrm{\begin{thm}}
\def\ethrm{\end{thm}}
\def\dfn{\begin{defn}}
\def\edfn{\end{defn}}
\def\lm{\begin{lem}}
\def\elm{\end{lem}}
\def\zd{\begin{zad}}
\def\ezd{\end{zad}}
\def\prdl{\begin{predl}}
\def\eprdl{\end{predl}}
\def\crl{\begin{cor}}
\def\ecrl{\end{cor}}
\def\rm{\begin{rem}}
\def\erm{\end{rem}}
\def\fct{\begin{fact}}
\def\efct{\end{fact}}
\def\enm{\begin{enumerate}}
\def\eenm{\end{enumerate}}
\def\pmat{\begin{pmatrix}}
\def\epmat{\end{pmatrix}}
\def\utv{\begin{utvr}}
\def\eutv{\end{utvr}}
\def\upr{\begin{uprz}}
\def\eupr{\end{uprz}}
\def\nrml{\trianglelefteqslant}

\frenchspacing
\righthyphenmin=2
%\usepackage{floatflt}
\captiondelim{. }








\title{Актуальный конспект. СП. Алгебра 2020}
\author{Чепуркин К.М.}
\date{}

\begin{document}



% \maketitle

\tableofcontents

\setcounter{chapter}{5}
\chapter{Группы}

\section{Немного примеров}

Мы уже обсуждали примеры групп раньше. Вспомним некоторые из них  и посмотрим, что к ним добавилось.

\exm \\
1) Если $R$-кольцо, то группу $R$ относительно сложения будем обозначать просто как $R$\\
2) Если $R$-кольцо, то $R^*$ -- это группа обратимых элементов.\\
3) Группа биекций $S_X$ на множестве $X$. В частности, $S_n$ -- группа биекций на $\{1,\dots,n\}$. Каждая такая биекция $\sigma \in  S_n$ однозначно задаётся последовательностью $\sigma(1),\dots,\sigma(n)$. В этой последовательности перечислены все элементы множества $\{1,\dots,n\}$, но, возможно, в другом порядке. Мы будем обозначать такую перестановку $\sigma=[\sigma(1),\dots,\sigma(n)]$. Например, обозначение тождественной перестановки будет $[1,\dots,n]$. А вот перестановка $[2,1,3,4,\dots,n]$ меняет местами $1$ и $2$, но все остальные элементы оставляет на месте.\\ 
4) Рассмотрим группу $D_n$. Это подгруппа $\Isom_{\mb R^2}$ состоящая из самосовмещений правильного $n$-угольника на плоскости. Покажем, что в этой группе $2n$ элементов. Прежде всего предъявим их, а потом покажем, что других нет. Здесь стоит различать два случая: $n$ -- чётное, $n$ -- нечётное. Если $n$ -- нечётное, то есть повороты на углы $\ffi=\frac{2\pi k}{n}$, где $k\in \ovl{0,1}$ и $n$ симметрий относительно прямых проходящих через какую-то вершину и центр $n$-угольника. Если же $n$ -- чётное, то элементами $D_n$ так же являются повороты на  $\ffi=\frac{2\pi k}{n}$, но симметрии делятся на два типа -- $\frac{n}{2}$ симметрий относительно прямых, проходящих через пары противоположенных вершин и $\frac{n}{2}$ симметрий относительно прямых, проходящих через середины противоположенных сторон $n$-угольника.

Для того, чтобы показать, что в этой группе не более $2n$ элементов воспользуемся следующими фактами:

\fct Любая изометрия плоскости определяется своими значениями в трёх точках, не лежащих на одной прямой (определяется тем, куда переводит вершины невырожденного треугольника). 

Любое самосовмещение правильного $n$-угольника переводит его вершины в вершины и центр в центр.
\efct

Пусть $f\in  D_n$, $x_0$ -- некоторая вершина $n$-угольника, $x_1$ -- соседняя вершина с $x_0$. Заметим, что образом точки $x_0$ может служить любая вершина $n$-угольника $y_0$. Итого для образа $x_0$ есть $n$ вариантов. Пусть $x_0$ перешёл в $y_0$. Тогда для $x_1$ есть не более двух вариантов, а именно $x_1$ обязан перейти в соседа $y_0$, которых ровно 2. 

Заметим, что отображение $f$ задаётся образами точек $x_0$ и $x_1$. Действительно, ведь центр $n$-угольника переходит в центр и поэтому фактически мы знаем куда переходят 3 точки, не лежащие на одной прямой -- $x_0, x_1$ и центр. Но для значений $x_0,x_1$ не более $2n$ вариантов, как мы только что поняли. Значит, элементов из $D_n$ не более и, следовательно, ровно $2n$ штук.\\
5) Если $V$ -- векторное пространство над полем $K$, то $\GL(V)$ -- это группа всех обратимых линейных отображений из $V$ в $V$. В частности, если $V=K^n$, то элементы группы $\GL_n(V)$ соответствуют обратимым матрицам с операцией умножения. Группа таких матриц обозначается $\GL_n(K)$.\\

Группа $D_n$ является подгруппой группы $GL_2(\mb R)$. Действительно, любая изометрия переводит параллелограмм в параллелограмм, и, следовательно, сумму векторов в сумму векторов.\\
6) Определим группу аффинных преобразований $n$-мерного пространства как
$$\Aff_n(K)=\AGL_n(K)=\{ f\colon K^n\to K^n\,|\, f(x)=Ax+b, \text{ где } A\in\GL_n(K), \, b \in K^n\}.$$




\section{Подгруппа, порождённая множеством. Циклические группы}

Прежде всего сделаем техническое замечание:

\rm Пусть дан набор подгрупп $H_{\alpha}$, где элементы $\alpha$ пробегают множество индексов $I$. Тогда пересечение $$\bigcap_{\alpha \in I}H_{\alpha}$$
тоже подгруппа $G$.
\erm


\dfn
Пусть $G$ - группа, а $X$ некоторое подмножество $G$. Тогда подгруппой, порождённой $X$, называется наименьшая подгруппа $H\leq G$, содержащая $X$. Будем обозначать эту подгруппу за $\langle X\rangle$. Эта подгруппа всегда существует и совпадает с подгруппой, равной пересечению подгрупп содержащих $X$
$$\langle X\rangle=\bigcap\limits_{ X\subseteq L \leq G} L.$$
Если множество $X$ состоит из конечного числа элементов, $X=\{ x_1,\dots, x_n\}$,  то мы будем писать просто $\langle x_1 ,\dots, x_n\rangle$, а не $\lan\{x_1,\dots,x_n\}\ran$.
\edfn


\utv
Подгруппа $\langle X\rangle$ допускает альтернативное определение $$\langle X\rangle=\{x_1^{\eps_1}\cdots x_n^{\eps_n} \,|\, \text{ где $x_i\in X $, $\eps_i\in \{\pm 1\}$ }\}.$$
Если взять $n=0$, то положим такое произведение равным $1$.
\eutv
\proof Понятно, что элементы вида $x_1^{\eps_1}\cdots x_n^{\eps_n}$ обязаны лежать в подгруппе, содержащей $X$, а значит и в $\lan X\ran$. Это даёт включение $ \{x_1^{\eps_1}\cdots x_n^{\eps_n} \,|\, \text{ где $x_i\in X $, $\eps_i\in \{\pm 1\}$ }\} \subseteq \langle X\rangle.$ Осталось показать включение в другую сторону. Для этого покажем, что множество $\{x_1^{\eps_1}\cdots x_n^{\eps_n} \,|\, \text{ где $x_i\in X $, $\eps_i\in \{\pm 1\}$ }\}$ --- подгруппа в $G$. Так как это множество содержит $X$, то, являясь подгруппой, оно необходимо должно содержать $\lan X\ran$, как наименьшую подгруппу с этим свойством.

Для проверки того, что это подгруппа, надо всего лишь заметить, что произведение выражений $x_1^{\eps_1}\dots x_n^{\eps_n}$  и $y_1^{\gamma_1}\dots y_m^{\gamma_m}$ есть $x_1^{\eps_1}\dots x_n^{\eps_n}y_1^{\gamma_1}\dots y_m^{\gamma_m}$, то есть снова выражение такого вида. 
Так же обратный $(x_1^{\eps_1}\dots x_n^{\eps_n})^{-1}=x_n^{-\eps_n}\dots x_1^{-\eps_1}$ есть снова выражение такого вида. Единица, как мы отметили, соответствует случаю $n=0$.
\endproof

\dfn
Пусть $G$ группа, а $g\in G$ --- некоторый элемент. Подгруппа вида $$\langle g \rangle=\{g^n\,|\, n\in \mb Z\}$$
называется циклической подгруппой группы $G$, порождённой $g$.
\edfn

\dfn Будем говорить, что группа $G$ порождена множеством $X$, если $\lan X\ran =G$. Если множество $X$ конечно и состоит из элементов $x_1,\dots,x_n$, то будем писать $G=\lan x_1,\dots,x_n \ran$ и называть $x_1,\dots,x_n$ образующими $G$. Если такое конечное множество есть, то  будем говорить, что $G$ конечно порождена. 
\edfn

\dfn Группа $G$ называется циклической, если она порождена одним элементом. То есть, если существует такой $g\in G$, что $G=\lan g \ran$.
\edfn

\exm
\enm
\item Группа $\mb Z$ -- циклическая. В качестве её образующих можно взять элементы $1$ или $-1$.
\item Рассмотрим группу $\GL_n(K)$ -- какие у неё образующие? Ответ прост -- это матрицы элементарных преобразований. Простое упражнение состоит в том, что матрицы, соответствующие транспозиции можно исключить из системы образующих.
\item Группа $\mb Z/n$ -- тоже циклическая -- в качестве её образующих можно взять любой элемент $l$ взаимно простой с $n$.
\item Рассмотрим группу $D_n$. Может ли она быть порождена одним элементом? Ответ: нет. Самое простое объяснение заключается в том, что группа $D_n$ не абелева. Ведь поворот на угол $\ffi$ относительно точки $x_0$ и симметрия относительно прямой, проходящей через эту точку не коммутируют (если $\ffi \neq 0, \pi $). С другой стороны, элементы циклической группы $\lan g\ran$ -- степени одного элемента $g$ -- коммутируют между собой, ведь $g^ig^j=g^{i+j}=g^jg^i$.

Чем же можно породить группу $D_n$? Оказывается, двух элементов уже достаточно.

\utv Пусть $f_{\ffi}$ -- поворот на угол   $\ffi=\frac{2\pi}{n}$ и $f_l \in D_n$ -- симметрия относительно какой-нибудь прямой.
Тогда $\lan f_{\ffi},f_l\ran =D_n$.
\eutv
\proof
Совершенно понятно, как получить поворот на угол $\frac{2\pi k}n$ -- это есть $f_{\ffi}^k$. Но как получить произвольную симметрию относительно прямой? Это вопрос чуть сложнее и мы не станем на него отвечать а приведём косвенное доказательство того, что такое представление есть. А именно, рассмотрим набор $f_l^{\eps} f_{\ffi}^k$, где $\eps\in \ovl{0,1}$, а $k\in \ovl{0,n-1}$.  Мы покажем, что все $2n$ элементов этого набора различны и, следовательно, пробегают все элементы группы $D_n$. 

Пусть $f_l^{\eps_1}f_{\ffi}^{k_1}=f_l^{\eps_2}f_{\ffi}^{k_2}$. Значит, $f_l^{\eps_1-\eps_2}f_{\ffi}^{k_1-k_2}=\id$.
Если $\eps_1=\eps_2$, то имеем $f_{\ffi}^{k_1-k_2}=\id$, что возможно только если $k_1=k_2$. Если же $\eps_1\neq \eps_2$, то будем считать $\eps_1-\eps_2>0$. Тогда $f_{l}=f_{\ffi}^{k_2-k_1}$. То есть поворот равен симметрии относительно прямой. Но такого не бывает, так как множество точек, остающихся на месте при симметрии -- это прямая. А множество точек, остающихся на месте относительно поворота: это либо точка, относительно которой поворачивают, либо всё пространство (в случае поворота на угол $0$).
\endproof
\eenm

Как мы уже отметили, группы $\mb Z$ и $\mb Z/n$ --- циклические. Посмотрим ещё в этом направлении. Что происходит, когда мы берём подгруппу, порождённую произвольным элементом $g \in G$? 


\dfn Порядок элемента $g\in G$ --- это количество элементов в подгруппе $\lan g \ran$.
\edfn

\lm Пусть $g$ -- элемент $G$. Тогда если $\ord g$ конечен, то $\ord g=n$, где $n$ -- такое наименьшее натуральное число, что $g^n=e$. Если же порядок $g$ бесконечен, то не существует такого элемента $n\in \mb N$, что $g^n=e$. 
\elm
\proof Пусть $n$ --- такое, что $g^n=e$. Покажем, что $\ord g \leq n$. Рассмотрим элементы $e,g,g^2,\dots,g^n, \dots$. Заметим, что, начиная с $g^n$, все элементы этой последовательности будут повторяться. Точнее, если поделить с остатком $m=nq+r$, где $0\leq r<n$, то
$$g^m=g^{nq+r}=g^r.$$
Так как подгруппа $\lan g \ran$ в точности состоит из элементов вида $g^i$, то мы установили, что различных среди них не более $n$ штук и все они имеют вид $g^r$, где $0\leq r < n$. 

Теперь если  $\ord g= \infty$ и одновременно $g^n=e$ при $n \in \mb N$, то предыдущее рассуждение приводит нас к противоречию. Действительно, условие $g^n=e$ означает, что в $\lan g\ran$ не более чем $n$ различных элементов. В частности, она конечна, что противоречит  $\ord g= \infty$.

Пусть теперь  $m=\ord g < \infty$. Рассмотрим набор элементов $e,\dots,g^m$. Их $m+1$ штука и они лежат в $\lan g \ran$ где ровно $m$ элементов. Значит, среди них есть одинаковые. Пусть это $g^i=g^j$, где $0\leq j <i\leq m$. Домножим на $g^{-j}$ и получим, что $g^{i-j}=e$. Но тогда в $\lan g \ran $ не более $i-j$ элементов по предыдущему рассуждению. Такое бывает только если $i=m$ и $j=0$.  Отсюда видно, что $g^m=e$. К тому же это рассуждение показывает, что элементы $g^i$ при $0<i<m$ не могут быть равны $e$. Значит $m$ -- минимальное. 
\endproof

Здесь в теореме мы снова пользовались тем, что если $g^n=e$, то в подгруппе, порождённой $g$, не более чем $n$ элементов. Сформулируем более точную версию этого утверждения, которая понадобится нам:

\utv Пусть $g\in  G$ и $g^n=e$ для $n\in \mb N$. Тогда $n \di \ord g$.
\eutv
\proof Пусть $m=\ord g$. Тогда $g^m=e$. Поделим с остатком: $n=mq+r$, где $0\leq r < m$. Тогда $e=g^n=g^{mq+r}=g^r$. Но если $r\neq 0$, то это противоречит определению $m$. Значит, $r=0$, что и требовалось.
\endproof









\thrm Пусть $g\in G$ -- элемент порядка $n\in \mb N$. Тогда циклическая группа $\lan g \ran $ изоморфна группе $\mb Z/n$. Если же $\ord g = \infty$, то $\lan g \ran$ изоморфна $\mb Z$.
\ethrm
\proof[Доказательство теоремы] Разберём сначала второй случай. Для этого докажем лемму: 
\lm Пусть $G$ группа, $g\in G$. Тогда существует такой единственный гомоморфизм $f \colon \mb Z \to G$, что $f(1)=g$.
\elm
\proof Для доказательства единственности заметим, что $f(n)$ должно быть равно $f(1)^n=g^n$. 

Итак, осталось показать, что заданное этой формулой отображение --  гомоморфизм групп. Но это равносильно базовым свойствам возведения в степень, которые мы уже обсуждали.
\endproof
\proof[Продолжение доказательства теоремы]
Пусть $\ord g=\infty$. Тогда построим гомоморфизм $\mb Z \to \lan g \ran$, переводящий $1$ в~$g$. В этом случае образ элемента $n \in \mb Z$ будет равен $g^n$. Значит, указанный гомоморфизм сюръективен, так как группа $\lan g \ran$ состоит ровно из степеней $g$. Осталось проверить инъективность. Предположим, что ядро $\Ker f \neq \{0\}$. Тогда есть число $0\neq n \in\Ker f $. Можно считать, что $n$ -- натуральное. Тогда $g^n=e$. Но это означает, что $\lan g \ran$ --- конечная подгруппа в $G$. Что противоречит условию. Значит, ядро тривиально.


Для доказательства в первом случае можно было бы сформулировать аналогичную лемму, но я не буду этого делать, а докажу напрямую.

Пусть $n=\ord g$. Построим отображение $f \colon\mb Z/n \to \lan g\ran $ по правилу $f(\ovl{k})=g^k$ (я тут специально напомнил, что в $\mb Z/n$ живут классы эквивалентности целых чисел).  Прежде всего необходимо проверить корректность такого определения, ведь выбор представителя $k$ может повлиять, априори, на $g^k$. 

Пусть $k_1\equiv k_2 \mod n$. Это значит, что $k_1=k_2+ns$. Но тогда $g^{k_1}=g^{k_2+ns}=g^{k_2}$. Значит, отображение определено корректно. Так же как и в случае с целыми числами, из свойств степени следует, что это гомоморфизм групп.   Осталось проверить, что это биекция. Для этого заметим, что элемент $g^k$ есть образ класса $k$. Это показывает сюръективность. Инъективность теперь следует из принципа Дирихле.
\endproof

Прервёмся на секунду и обсудим примеры подгрупп, порождённых подмножеством: прежде всего разберёмся с "модельной" циклической группой $\mb Z/n$.

\lm Пусть $k\in \mb Z/n$. Тогда $\ord k = \frac{n}{(n,k)}$. 
\elm
\proof Перепишем всё на языке сравнений. Нам надо найти наименьшее натуральное $d$, что $dk\equiv 0 \mod n$. Все решения этого сравнения имеют вид $d=\frac{n}{(n,k)}t$. Наименьшее натуральное решение получается при $t=1$, что и ожидалось.
\endproof

Изоморфизм сохраняет все свойства, которые можно выразить через групповую операцию. В частности, элементы порядка $n$ он переводит в элементы порядка $n$. Если $g$ -- это элемент порядка $n$, то циклическая подгруппа, порождённая им, изоморфна $\mb Z/n$. Отсюда автоматически следует: 

\crl Пусть $G$ -- группа, а $g\in G$ имеет порядок $n$. Тогда элемент $g^k$ имеет порядок $\frac{n}{(n,k)}$ 
\ecrl

А что будет, если взять целые числа? Понятно, что любой элемент внутри $\mb Z$, кроме $0$, имеет бесконечный порядок. Можно спросить поглубже, а чем вообще порождены подгруппы $\mb Z$? Оказывается, ответ довольно прост.

Для элементов группы $D_n$ легко посчитать их порядок: если это поворот, то он лежит в циклической группе, порождённой поворотом на $\frac{2\pi}{n}$, а если это симметрия, то порядок равен 2. Но как проверить, что порядок элемента  $g\in G$ равен $n$ для группы, которая задана не так явно, например, для группы $(\mb Z/n)^*$? Теоретически, можно возводить элемент $g$ подряд во все степени и смотреть, когда же $g^k=1$. Если перебирать таким образом, то может случиться, что вам придётся перебирать $|G|=n$ элементов (например, когда $G$ циклическая группа, а $g$ её порождает). Если $n=2^{2048}$, то такой перебор может и не закончиться. Следующая теорема говорит нам, как можно сэкономить в вычислениях:

\lm Пусть $g \in G$ такой, что $g^n=e$ для числа  $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}$. Тогда если $g^{\frac{n}{p_i}}\neq e$ для всех $i\in\ovl{1,k}$, то $n=\ord g$. 
\elm
\proof Пусть $m=\ord g$. Из условия $g^n=e$ мы знаем, что $m | n$. Пусть $m<n$. Тогда существует такой простой делитель $p_i$ числа $n$, что $n\di p_i^{\alpha_i}$, но $m\ndi p_i^{\alpha_i}$. Заметим тогда, что $n/p_i \di m$. То есть $n/p_i=mk$. Но тогда $g^{n/p_i}=g^{mk}=e$, что противоречит условию теоремы. Значит, $n=m$, что и требовалось.
\endproof

Посмотрим простейшие примеры того, как это утверждение работает. Рассмотрим элемент $2\in (\mb Z/13)^*$. Прежде всего я утверждаю, что $2^{12}=1$. Это следует из теоремы Эйлера. Простые делители $12$ -- это $2$ и $3$. Значит, для того, чтобы показать, что порядок элемента $2$ в группе $(\mb Z/13)^*$ равен 12 нам осталось проверить, что $2^{6}\neq 1$ и $2^4\neq 1$ в $\mb Z/13$. Что довольно легко делается.

Заметим, что так как в группе $(\mb Z/13)^*$ итак 12 элементов, то подгруппа $\lan 2\ran =(\mb Z/13)^*$. То есть $(\mb Z/13)^*$ -- циклическая.










\subsection{Подгруппы циклических групп}



\thrm Пусть $H$ -- подгруппа в циклической группе $G$, тогда $H$ -- циклическая. Более того, если $|G|=n$, то для любого $d|n$ существует единственная подгруппа $H\leq \mb Z/n$, что $|H|=d$. 
\ethrm
\proof

Рассмотрим сначала случай $G=\mb Z$. Это докажет нашу теорему в случае произвольной бесконечной циклической группы.

\lm Пусть $H$ подгруппа в $\mb Z$. Тогда $H$ -- циклическая.
\elm
\proof Мы уже проделывали это доказательство, когда говорили про наибольший общий делитель. Нам надо понять, что $H=\lan n\ran =n\mb Z$. Если $H=\{0\}$, то $n=0$. Иначе рассмотрим в $H$ наименьший натуральный элемент $n$. Покажем, что $H=n\mb Z$. Так как $n \in H$, то и $kn \in H$. Следовательно, получаем включение $n\mb Z \subseteq H$. Покажем обратное включение. Пусть $m\in H$. Поделим с остатком: $m=nq+r$, где $0\leq r<n$. Заметим, что тогда элемент $r=m-nq$ тоже лежит в $H$. Но если $r$ натуральный, то $r<n$ и мы приходим к противоречию с определением $n$. Значит,  $r=0$. То есть $m=nq \in n \mb Z$, что и требовалось. 
\endproof

Если $G$ -- бесконечная циклическая группа, то $G \simeq \mb Z$ и предыдущее утверждение закрывает доказательство этого случая. Пусть теперь $|G|=n$. Тогда $G\simeq \mb Z/n$ и утверждение достаточно доказать для $\mb Z/n$. Вместо прямого доказательства, сведём указанное утверждение к уже известному аналогу для $\mb Z$. Для этого рассмотрим гомоморфизм $\pi\colon \mb Z \to \mb Z/n$ переводящий $x\to \ovl{x}$. Сведение теперь будет основано на лемме:

\lm Пусть $f\colon G_1\to G_2$ гомоморфизм групп и $H\leq G_2$. Тогда $f^{-1}(H)$ -- подгруппа в $G_1$.
\elm
\proof Проверим: $e \in f^{-1}(H)$, то есть, по определению, что $f(e)\in H$. Это так, потому что $f(e)=e\in H$. Аналогично, если $a=f^{-1}(x)$, где $x \in H$, то $f(a^{-1})=x^{-1}\in H$. Если же $f(a), f(b) \in H$, то $f(ab)=f(a)f(b)\in H$. 
\endproof

Итак, пусть $H\leq \mb Z/n$. Тогда, воспользовавшись леммой, получаем, что  $\pi^{-1}(H)$ ---  подгруппа в $\mb Z$. Но такая подгруппа циклическая, то есть $\pi^{-1}(H)=\lan k \ran$. У любого элемента $x\in H$ есть прообраз $a\in \pi^{-1}(H)=\lan k \ran$, так как $\pi$ сюръективно. Но тогда $x\in \lan \ovl{k} \ran$. То есть $H\subseteq \lan \ovl{k}\ran$. Осталось проследить, что верно и обратное включение. Откуда вытекает, что $H=\lan \ovl{k}\ran$ -- циклическая.

Покажем существование и единственность подгруппы порядка $d$ для всех $d|n$.

Вначале предъявим подгруппу порядка $d$. Как мы уже показали, такая подгруппа должна быть циклической. То есть нужно предъявить элемент порядка $d$. Проще всего взять элемент $\frac{n}{d}\in \mb Z/n$.

Покажем, что любая другая подгруппа $H$ порядка $d$ равна $\lan \frac{n}{d} \ran$. Пусть $H$ порождена элементом $x$ порядка $d$. Если отождествить $x$ с соответствующим целым числом, то условие на порядок записывается как  $d=\frac{n}{(n,x)}$, откуда $\frac{n}{d}=(n,x)$. Значит, $x$ кратен $\frac{n}{d}$. Но тогда $H \subseteq \lan\frac{n}{d}\ran$. Так как в обеих группах ровно $d$ элементов, то мы получаем равенство.
\endproof




\section{Классы смежности и теорема Лагранжа}

Из предыдущего раздела осталось несколько вопросов: откуда же взять изначальное условие $g^n=e$ для нахождения порядка элемента $g$? Почему в теореме про подгруппы циклической группы мы ограничились только теми подгруппами, порядок которых делит размер объемлющей группы? Оказывается это всё проявление некоторого общего принципа свойственного всем группам.


\dfn Пусть $H$ --- подгруппа $G$. Определим отношение эквивалентности $\sim_H$ на $G$ следующим образом: $g_1\sim_H g_2$ если $\exists h \in H$, что $g_1=g_2 h$.
\edfn

\utv Это отношение эквивалентности.
\eutv
\proof Действительно. Для того, чтобы показать, что $g\sim_H g$ возьмём $h=e$. 

Cимметричность: если $g_1 \sim_H g_2$, то $g_1=g_2h$ для некоторого $h\in H$. Тогда $g_2=g_1h^{-1}$. Осталось заметить, что $h^{-1}\in H$.

Покажем рефлексивность. Пусть $g_1\sim_H g_2 \sim_H g_3$.  То есть $g_1=g_2h_1$, $g_2=g_3h_2$. Но тогда подставив в первое равенство второе получим $g_1=g_3h_2h_1$. Значит, $g_1\sim_H g_3$. Что и требовалось. 
\endproof

Посмотрим как выглядят классы эквивалентности относительно $\sim_H$.

\dfn Пусть $G$ -- группа, $H$ -- подгруппа и задан некоторый элемент $g\in G$. Тогда множество $gH=\{ gh\,|\, h \in H\}$ является классом эквивалентности относительно $\sim_H$. Будем называть $gH$ левым смежным  классом элемента $g$ по подгруппе $H$. 
\edfn

\dfn Множество всех левых смежных классов будем обозначать $G/H$. Количество элементов в $G/H$ называется индексом $H$ в $G$  и обозначается $[G:H]$. 
\edfn

Благодаря тому, что отношение $\sim_H$ -- это отношение эквивалентности получаем, что:


\crl Группа $G$ разбивается в дизъюнктное объединение левых смежных классов $$G=\coprod_{ gH \in G/H} gH.$$
\ecrl




\rm Аналогично определяется правый смежный класс $Hg$ для элемента $g$. Группа $G$ так же разбивается в диъюнктное объединение правых смежных классов. Множество правых смежных классов обозначается как $H\setminus G$.
\erm 

Это не всё, что нам нужно от смежных классов:

\utv Пусть $H$ -- подгруппа $G$ и $g\in G$ -- некоторый элемент. Тогда отображение $H \to gH$, заданное по правилу $h \to gh$, является биекцией.
\eutv
\proof Построим обратное отображение. Оно берёт элемент $x=gh\in gH$ и отправляет его в $g^{-1}x=h \in H $. Несложно проверить, что это взаимно обратные отображения.
\endproof

\dfn Пусть $G$ -- группа. Тогда число элементов в $G$ называют порядком $G$.
\edfn

\thrm[Лагранжа]  Пусть $G$ -- группа, $H$ -- подгруппа. Пусть порядок  $H$ конечен и индекс $[G:H]$ конечен. Тогда $G$ -- конечная группа и 
 $$|G|=|H|[G:H].$$
\ethrm
\proof По следствию из того, что $\sim_H$ -- отношение эквивалентности, получаем, что группа $G$ разбивается в дизъюнктное объединение левых смежных классов $$G=\coprod_{gH \in G/H} gH.$$
Таких смежных классов по определению $[G:H]$ штук. В каждом смежном классе $gH$ элементов столько же, сколько в $H$, то есть $|H|$. Но тогда число элементов в $G$ конечно и равно $|H|[G:H]$.
\endproof

\crl Пусть $G$ -- конечная группа, а $H$ -- её подгруппа. Тогда $|G| \di |H|$.
\ecrl
\proof В теореме Лагранжа даже сказано откуда берётся дополнительный множитель.
\endproof

\crl Пусть $G$ -- конечная группа. Тогда порядок элемента $g\in G$ делит $|G|$.
\ecrl
\proof Порядок $\ord g$ равен $|\lan g\ran|$. Но $\lan g\ran$ -- это подгруппа в $G$. Применим предыдущее следствие.
\endproof

\crl Пусть $G$ -- конечная группа порядка $n$, а $g$ -- её элемент. Тогда $g^n=e$.
\ecrl  
\proof По предыдущему следствию $n= \ord g \cdot m$, для $m\in \mb N$. Тогда $g^n=(g^{\ord g})^m=e^m=e$. 
\endproof

\crl Пусть $G$ -- конечная группа порядка $p$. Тогда $G$ циклическая и $G \simeq \mb Z/p$. 
\ecrl

\crl Пусть $G$ -- группа порядка $4$. Тогда либо $G \simeq \mb Z/2 \times \mb Z/2$, либо $G \simeq \mb Z/4$.
\ecrl



Некоторые старые факты удобно воспринимать в контексте теоремы Лагранжа.


\crl[Теорема Эйлера] Пусть $n$ -- натуральное число и $a\in \mb Z/n^*$. Тогда $a^{\ffi(n)}=1$.
\ecrl
\proof
Применим следствие из теоремы Лагранжа к группе $\mb Z/n^*$ и элементу $a\in \mb Z/n^*$.
\endproof

\rm Теорема Лагранжа может упростить доказательство того, что $D_n$ порождена поворотом на угол $2\pi/n$ и любой симметрией. Действительно: подгруппа порождённая только поворотом состоит из $n$ элементов. Если мы добавим ещё и симметрию, то должна получиться подгруппа в которой больше элементов. Её порядок должен делиться на $n$, то  есть должен быть не меньше $2n$. Значит, все элементы из $D_n$ мы точно получим.
\erm 




\section{Строение мультипликативной группы поля}


Из предыдущей темы мы знаем, что кольцо $\mb Z/n$ для составного $n$ раскладывается в произведение множителей вида $\mb Z/p^{\alpha}$, и описывать группу обратимых элементов нам нужно только в этом случае. Начнём с самой простой ситуации: $\alpha=1$. В этом случае $\mb Z/p$ -- поле, а группа $\mb Z/p^*$ состоит из $p-1$ элемента. Утверждается, что эта группа циклическая. Для этого нам понадобятся несколько лемм. 

\lm Пусть $n$ --- натуральное число. Тогда $n = \sum_{d|n}\varphi(d)$.
\proof  Рассмотрим циклическую группу $\mb Z/n$. Тогда если $d$ делит $n$, то в этой группе есть единственная подгруппа из $d$ элементов. Все элементы порядка $d$ лежат в этой подгруппе. Эта подгруппа циклическая. Следовательно, их $\ffi(d)$ штук. Тогда, сгруппировав все элементы одинакового порядка, получаем
$$n= |\mb Z/n |= \sum_{d|n} |\{ x \in \mb Z/n \,| \text{ $x$ элемент порядка $d$  }\}| = \sum_{d|n}\varphi(d).$$
\endproof
\elm

\lm Пусть $H$ --- такая конечная группа, что число элементов, удовлетворяющих равенству $x^d= e$, не больше $d$. Тогда $H$ --- циклическая.
\elm
\proof Посчитаем число элементов в $H$. Обозначим его за $n$. Тогда
$$ n = \sum_{d|n} |\{ x \in H \,|\text{  $x$ элемент порядка $d$ в $H$} \}|.$$

Пусть  $x\in H$ порядка $d$. Тогда все элементы из подгруппы $\lan x \ran$ удовлетворяют тождеству $y^d=e$. Их $d$ штук. С другой стороны, по условию в $H$ не более $d$ элементов удовлетворяющих $y^d=e$. Рассмотрим $z \in H$ порядка $d$. Тогда он удовлетворяет $z^d=e$ и, значит, лежит в $\lan x \ran$. Но в $\lan x \ran$ как и в любой циклической группе элементов порядка $d$ ровно $\ffi(d)$. В частности, число элементов порядка ровно $d$ либо $0$, либо $\varphi(d)$, то есть всегда меньше или равно $\varphi(d)$. Тогда
$$n = \sum_{d|n} |\{x \in H \,| \text{  $x$ элемент порядка $d$ в $H$ } \}|\leq \sum_{d|n}\varphi(d) =n$$

Значит, неравенство обращается в равенство для каждого слагаемого. В частности, для того, которое соответствовало элементам порядка $n$. Значит, элементов в $H$ порядка ровно $n$ в точности $\varphi(n)$ штук. В частности, они
есть. Тогда группа $H$ порождена любым из них.\endproof


\thrm[Конечные подгруппы в мультипликативной группе поля] Пусть $H$ -- конечная подгруппа в $K^*$, где $K$ -- поле. Тогда $H$ циклическая.
\proof Решений уравнения $x^d-1 = 0$ в $K$ не более $d$ штук. Значит, их не более $d$ штук в подгруппе $H$. Применим предыдущую лемму.
\endproof
\ethrm

Применяя предыдущую теорему мы сразу же можем описать структуру мульпипликативной группы $\mb Z/ p^*$, где $p$ -- простое.

\crl Пусть $p\neq 2$ простое число. Тогда группа $\mb Z/p^*$ изоморфна циклической группе $\mb Z/(p-1)$.
\ecrl

\dfn Если $n\in \mb N$, то число $a$, такое что $\lan a\ran =\mb Z/n^*$ называется первообразным корнем по модулю $n$. Мы показали, что по модулю простого числа есть первообразные корни. Однако это редкость. Для большинства $n$ таких первообразных корней нет.  
\edfn


\section{Проблема дискретного логарифма и алгоритм Диффи-Хеллмана}

Обсудим, как наличие первообразного корня может быть  применено в криптографии. Основную задачу криптографии можно сформулировать так: передать сообщение от одного адресата другому, так, чтобы в случае доступа к каналу связи третьего человека, он не смог получить текст исходного сообщения. 

Точнее, третье лицо может получить только зашифрованное сообщение, по которому, по идее, не должно иметь возможность за разумное время восстановить исходное сообщение. С другой стороны, необходимо, чтобы получатель сообщения смог бы расшифровать полученное (не умерев от нетерпения).

Классические системы шифрования -- системы с закрытым ключом подразумевали, что участники заранее договариваются о закрытом ключе. При помощи этого ключа происходит шифрование и расшифровка. Однако часто у вас нет возможности договориться заранее. 

Решить эту проблему помогает алгоритм описанный в статье \href{https://www-ee.stanford.edu/~hellman/publications/24.pdf}{W. Diffie and M. E. Hellman, New Directions in Cryptography }. Этот алгоритм позволяет договориться об общем секретном ключе двум людям, используя только открытый канал связи.

\dfn
Пусть $G$ --- группа. Рассмотрим некоторый элемент $g\in G$. Тогда для любого $h\in \lan g\ran$ определено число $l$, что $g^l=h$ и $l\leq \ord g$. Такое число $l$ называется логарифмом $h$ по основанию $g$.
\edfn

Проблему дискретного логарифма можно поставить следующим образом. Пусть даны группа $G$, её элементы $g\in G$ и $h\in G$, про которые заведомо известно, что они лежат в $\lan g\ran$. Задача: найти логарифм $h$ по основанию $g$.


Рассмотрим некоторое конечное поле $K$. Нам известно, что группа $K^*$ является циклической. Пусть дан некоторый элемент $g\in K^*$, порождающий группу $K^*$. Тогда задача нахождения дискретного логарифма по основанию $g$ считается трудной. Это даёт возможность предложить  алгоритм для получения общего ключа.

Будем считать общеизвестными описание конечного поля $K$ и первообразный корень $g$ степени $m=|K|-1$. Например, $K=\mb Z/p$ и описание --- это просто задание простого числа $p$. Множество сообщений --- это множество чисел от 1 до $m$. Боб загадывает некоторое число $b\leq m$, а Алиса --- число $a\leq m$. Боб передаёт Алисе число $B=g^b$. Алиса передаёт Бобу $A=g^a$. Тогда оба они знают число 
$$A^b=g^{ab}=B^a,$$
которое и служит секретным ключом.
 

\section{Группа перестановок}

Попытаемся примерить все наши определения к группе перестановок. Прежде всего посчитаем порядок перестановки. Начнём со случая цикла


\dfn[Цикл] Пусть $a_1,\dots,a_k$ -- набор различных элементов из $\{1,\dots,n\}$. Тогда определим $c$ -- элемент из $S_n$, который мы будем называть циклом $(a_1,\dots,a_k)$, следующим образом:
$$c(x)=\begin{cases}
x,\, x \notin \{a_1,\dots,a_k\}\\
a_{i+1},\, x=a_i,\, 1\leq i < k\\
a_1,\, x=a_k
\end{cases}$$
\edfn

\utv Порядок цикла $(a_1,\dots,a_k)$ равен $k$.
\eutv
\proof Для того, чтобы цикл $c=(a_1,\dots,a_k)$ имел порядок $d$ нужно, чтобы $d$ было минимальным таким, что $c^d=\id$, то есть, чтобы после $d$ применений $c$ точки возвращались в себя. Точка $a_1$ возвращается в себя в первый раз после $k$ итераций. Как и все остальные точки из цикла. Точки отличные от $\{a_1,\dots,a_k\}$ и так переходят себя. Значит, $d=k$. 
\endproof

Сведём вычисление порядка для произвольной перестановки к вычислению для циклов. Для этого будут полезны следующие определения.

\dfn[Неподвижная точка] Если $\sigma \in S_n$, то тогда неподвижной точкой называется такой $x\in \{1,\dots,n\}$, что $\sigma(x)=x$. Множество всех неподвижных точек относительно $\sigma$ обозначим как $\Fix(\sigma)$ 
\edfn

\dfn[Носитель] Носителем перестановки $\sigma \in S_n$ называется множество $\supp \sigma$ = $\{1,\dots,n\}\setminus \Fix(\sigma)$
\edfn

\dfn[Независимость] Перестановки $\sigma_1,\sigma_2\in S_n$ называются независимыми, если $\supp \sigma_1 \cap \supp \sigma_2=\varnothing$.
\edfn

\rm Две независимые перестановки коммутируют, то есть $\sigma_1\sigma_2=\sigma_2\sigma_1$.
\erm

\thrm Пусть $\sigma \in S_n$. Тогда существует единственный с точностью до порядка набор независимых циклов $c_1,\dots,c_k$, что $c_i\neq \id$ и 
$$\sigma=c_1\dots c_k.$$
\ethrm
\proof Постараемся привести строгое доказательство. Пример применения алгоритма, стоящего за этим доказательством, смотри внизу. Рассмотрим отношение эквивалентности $\sim$ такое, что $x\sim y$, если существует $k\in \mb Z$, что $\sigma^k(x)=y$. Покажем транзитивность. Если $y=\sigma^k(x)$, а $z=\sigma^l(y)$, то $z=\sigma^l(\sigma^k (x))= \sigma^{l+k}(x)$.

Будем называть класс эквивалентности точки $x$ относительно этого отношения орбитой точки $x$ под действием $\sigma$. Она состоит из всех элементов вида $\sigma^k(x)$, $k \in \mb Z$.

Пусть $\Omega_1,\dots,\Omega_s$ -- это все различные орбиты. Определим перестановки $c_i$, где $i\in\ovl{1,s}$ следующим правилом:
$$c_i(x)=\begin{cases} \sigma(x), x\in \Omega_i\\
x, x\notin \Omega_i
\end{cases}.$$
Я утверждаю, что $c_i$ -- это независимые циклы. Прежде всего заметим, что носитель $c_i \subseteq \Omega_i$. Отсюда следует независимость.

Далее, если $\Omega_i$ состоит из $l$ элементов, то $c_i$ -- это цикл длины $l$ вида 
$$c_i=(x,\sigma(x),\dots,\sigma^{l-1}(x)), $$ где $x$ -- произвольный элемент из $\Omega_i$. Для этого необходимо показать, что это все элементы $\Omega_i$, что они все различны и что $\sigma^l(x)=x$.

Для этого заметим, что если $\sigma^i(x)=x$, для $i>0$, то в орбите $x$ не более чем $i$ элементов, так как, начиная с номера $i$, элементы $\sigma^k(x)$ начинают повторяться.

Предположим теперь, что $\sigma^i(x)=\sigma^j(x)$, где $0\leq j<i<l$. Но тогда, подействовав  $\sigma^{-j}$, получаем, что $\sigma^{i-j}(x)=x$. Но тогда по сделанному замечанию в $\Omega_i$ не более чем $i-j <l$ элементов. Значит, все элементы различные. 

Рассмотрим элемент $\sigma^l(x)$. Прежде всего заметим, что он совпадает с одним из элементов в наборе $x, \sigma(x),\dots,\sigma^{l-1}(x)$ по принципу Дирихле. Пусть с $\sigma^{i}(x)$. Если $i\neq 0$, то $\sigma^{l-i}(x)=x$ и в $\Omega_i$ меньше чем $l$ элементов. Противоречие. Значит, $i=0$ и $\sigma^l(x)=x$.

Итак, $c_i$ -- цикл. Покажем, что 
$$\sigma=c_1\dots c_s.$$
Пусть $x\in \Omega_i$. Заметим, что $\sigma(x)$ тоже лежит в $\Omega_i$. Посчитаем левую часть
$$c_1\dots c_s(x)=c_1\dots c_{i-1} (c_i(x))=c_1\dots c_{i-1} (\sigma(x))=\sigma(x),$$
так как циклы $c_j$ не переставляют элементы из $\Omega_i$ при $i\neq j$. Осталось выкинуть те циклы $c_i$, которые равны тождественной перестановке.

Единственность. Пусть $\sigma= c_1\dots c_k$ -- произведение независимых циклов. Тогда $\supp c_i$ совпадает с некоторой орбитой $\sigma$. Порядок следования элементов в $c_i$ определяется действием $\sigma$ на этой орбите, так как остальные $c_j$ оставляют точки этой орбиты на месте в силу независимости. 
\endproof



\exm \,Перестановка $\sigma=[2,3,1,4,6,5]$ может быть представлена следующим образом. Посмотрим куда переходит $1$ под действием $\sigma^k$. Получаем следующие переходы:
$$1\to 2\to 3 \to 1$$ 
$$4 \to 4 $$
$$ 5\to 6\to 5.$$
Итого: 
$$[2,3,1,4,6,5]=(123)(4)(56)=(123)(56).$$

\utv Пусть перестановка $\sigma \in S_n$ разложилась в виде произведения  независимых циклов $\sigma=c_1\dots c_k$. Обозначим длину цикла $c_i$  как $d_i$. Тогда $\ord \sigma= \Nok(d_1,\dots,d_k)$.
\eutv
\proof Благодаря тому, что независимые перестановки коммутируют получаем
$$\sigma^d=c_1^d\dots c_k^d$$

Заметим, что $c_i^d$ -- тоже независимые перестановки (хотя и не обязательно циклы). Поэтому $\sigma^d$  есть произведение независимых перестановок. Для того, чтобы $\sigma =\id$, необходимо и достаточно, чтобы $c_i^d=\id$. Но это происходит только если $d\di d_i$. Наименьшее такое $d$ -- это $\Nok(d_1,\dots,d_k)$.
\endproof

\utv[Обратный в цикловой записи] Пусть дан цикл $c=(a_1,\dots,a_k)$. Тогда $c^{-1}=(a_k,\dots,a_1)$. Если перестановка   $\sigma= c_1c_2\dots c_s$ представлена в виде произведения непересекающихся циклов $c_i$, то
$$\sigma^{-1}=c_1^{-1}c_2^{-1}\dots c_s^{-1}.$$
\eutv
\proof Утверждение для одного цикла понятно. Далее заметим, что циклы $c_i^{-1}$ так же независимы. Отсюда
$$\sigma^{-1}=c_s^{-1}\dots c_1^{-1}=c_1^{-1}\dots c_s^{-1}.$$
\endproof



Мы разобрались с циклическими подгруппами в $S_n$. Но сама группа $S_n$ не циклическая (при $n\geq 3$) так как не абелева. Хотелось бы что-то сказать про образующие $S_n$. На текущий момент мы знаем, что множество всех циклов порождает $S_n$. Хочется уменьшить набор образующих.

\dfn[Транспозиция] Цикл вида $(ij)$ где $i\neq j$ называется транспозицией.
\edfn

\utv Любая перестановка представляется в виде произведения транспозиций.
\eutv
\proof Для этого достаточно научиться представлять циклы $(a_1,\dots,a_k)$. Но это легко сделать
$$(a_1 a_k)\dots( a_1 a_2)=(a_1,\dots,a_k)=(a_1a_2)\dots (a_{k-1} a_k)$$
\endproof
Здесь мы показали, что один и тот же цикл может быть двумя разными способами представлен в виде произведения транспозиций. Есть ли что-то общее у двух таких представлений?

\dfn[Инверсия] Будем говорить, что пара $i<j$  образует инверсию для перестановки $\sigma$, если $\sigma(i)>\sigma(j)$.
\edfn

\dfn[Чётность и знак] Чётностью перестановки будем называть чётность числа инверсий $Inv(\sigma)$ в ней. Знаком перестановки $\sigma$ будем называть 
$$\sgn(\sigma)=(-1)^{Inv(\sigma)}.$$
\edfn

\rm Знак перестановки можно задать другим способом: 
$$\sgn(\sigma)= \prod_{i>j} \frac{\sigma(i)-\sigma(j)}{i-j}.$$
Действительно, заметим, что знак выражения в правой части равен $(-1)^{Inv (\sigma)}$, так как сомножитель $\frac{\sigma(i)-\sigma(j)}{i-j}$ отрицателен, только если пара $i,j$ задаёт инверсию в $\sigma$.

Осталось показать, что получившееся выражение по модулю равно 1. Для этого перепишем его в виде 
$$\prod_{i>j} \frac{\sigma(i)-\sigma(j)}{i-j}= \frac{\prod_{i>j} (\sigma(i)-\sigma(j))}{\prod_{i>j}(i-j)}
$$
Сделаем в верхнем произведении "замену переменной". Представим $i=\sigma^{-1}(k)$, $j=\sigma^{-1}(l)$. Тогда верхнее произведение превращается в произведение 
$$\prod_{\sigma^{-1}(k)>\sigma^{-1}(l)} (k-l)$$
Видно, что это тоже произведение всех попарных разностей, только не обязательно из большего элемента в паре вычитается меньший. Итого по модулю это произведение равно $\prod_{i>j}(i-j)$, что  и требовалось.
\erm

\rm Удобно представить это выражение, как произведение по двухэлементным подмножествам.
$$\prod_{i>j} \frac{\sigma(i)-\sigma(j)}{i-j}=\prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\sigma(i)-\sigma(j)}{i-j}$$
\erm
 

\lm Отображение $\sgn \colon S_n \to \{\pm 1\}$ является гомоморфизмом групп.
\elm
\proof Пусть $\sigma, \tau \in S_n$. Покажем, что $\sgn \sigma \tau = \sgn \sigma \sgn \tau$. Представим левую часть в виде произведения:
$$\sgn \sigma \sgn \tau= \prod_{\substack{\{k,l\}\\ k\neq l}} \frac{\sigma(k)-\sigma(l)}{k-l} \prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\tau(i)-\tau(j)}{i-j}.$$
Сделаем замену в первом произведении: представим $k=\tau(i),l=\tau(j)$ для единственных $i,j$. Тогда если $\{k,l\}$ пробегает  все двухэлементные подмножества в $\{1,\dots,n\}$, то $\{i,j\}$ тоже пробегает все двухэлементные подмножества. Итого
$$\sgn \sigma \sgn \tau=\prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\sigma(\tau(i))-\sigma(\tau(j))}{\tau(i)-\tau(j)} \prod_{\substack{\{i,j\}\\ i\neq j}} \frac{\tau(i)-\tau(j)}{i-j}$$
Сокращая знаменатели из первого произведения с числителями из второго, получаем как раз $\sgn \sigma \tau$.
\endproof

Перейдём к связи чётности и транспозиций.

\rm Заметим, что чётность транспозиции $(1,2)$ равна $-1$.
\erm

\lm Пусть $g\in S_n$. Тогда имеет место равенство перестановок. $g(1,2)g^{-1}=(g(1),g(2))$.
\elm
\proof Пусть $X=\{1,\dots,n\}$. Нарисуем диаграмму из отображений:
\begin{center}
\begin{tikzpicture}
\node (A) at (2, 0) {$X$};
\node (B) at (0, 0) {$X$};
\node (C) at (2, 1) {$X$};
\node (D) at (0, 1) {$X$};
\path[->,font=\scriptsize,>=angle 60]
(C) edge node[left]{$g$} (A)
(D) edge node[right]{$g$} (B)
(C) edge node[above]{$(1,2)$} (D)
(A) edge node[below]{$g(1,2)g^{-1}$} (B);
\end{tikzpicture}
\end{center}
Перестановка $g\colon X \to X$ сопоставляет новую нумерацию элементам из $X$. Итак, пусть есть некоторый элемент $k\in X$. Число $k$ -- это номер элемента $g^{-1}(k)$ в новой нумерации. Применяя далее к $g^{-1}(k)$ транспозицию $(1,2)$ мы получаем образ $g^{-1}(k)$ под действием $(1,2)$ в старой нумерации. Действуя $g$ переходим обратно в новую нумерацию. 
Таким образом, действует так же как и перестановка $(1,2)$ только с поменяной нумерацией. То есть как перестановка $(g(1),g(2))$.
\endproof

\rm Возможно проще вычислить в лоб, то есть посмостреть, куда по определению переходят $g(1)$ и $g(2)$ и все остальные элементы.
\erm

\crl Знак любой транспозиции равен $-1$.
\ecrl
\proof Пусть дана транспозиция $(i,j)$. Рассмотрим такую перестановку $g\in S_n$, что $g(1)=i$ и $g(2)=j$. Тогда $$\sgn((i,j))=\sgn(g(1,2)g^{-1})=\sgn g \sgn (1,2) (\sgn g)^{-1}=\sgn (1,2)=-1.$$ 
\endproof


\thrm Чётность перестановки $\sigma=\tau_1\dots \tau_k$, где $\tau_i$ -- транспозиции, равна $(-1)^{k}$.
\ethrm
\proof
$$\sgn(\tau_1\dots \tau_k)=\underbrace{(-1)\cdots (-1)}_{k \text{ раз }}=(-1)^k.$$

\endproof

\rm $\sgn \sigma = \sgn \sigma^{-1}$
\erm

Как посчитать знак (чётность) перестановки? Для этого удобно использовать представление перестановки в виде произведения непересекающихся циклов.

\utv Пусть $\sigma=c_1\dots c_k$, где $c_i$ независимые циклы. Тогда
$$\sgn \sigma = (-1)^{\text{число $c_i$ чётной длины}}.$$
\eutv
\proof Цикл длины $k$ раскладывается в виде произведения $k-1$ транспозиции. Значит, циклы нечётной длины дают сомножитель $1$, а циклы чётной длины сомножитель $(-1)$.
\endproof

Есть ещё один эквивалентный способ.
\utv Пусть $\sigma\in S_n$. Тогда
$$\sgn \sigma = (-1)^{n-k},$$
где $k$ -- это число орбит $\sigma$.
\eutv
\proof Множество точек $\{1,\dots,n\}$ разбивается в дизъюнктное объединение  $k$ орбит $\Omega_i$. Пусть $\Omega_i$ состоит из $k_i$ элементов. Получаем, что $\sum_{i=1}^{k} k_i=n$. Каждой орбите соответствует один цикл $c_i$ длины $k_i$ в разложении $\sigma$. Он раскладывается на $k_i-1$ транспозицию. Значит всего в разложении $\sigma$ необходимо взять $\sum_{i=1}(k_i-1)=n-k$ транспозиций. 
\endproof

В качестве примера, в котором важную роль играет понятие чётности перестановки посмотрим на игру в пятнадцать. Выглядит игра следующим образом: дан квадратик $4\times 4$ в пятнадцати клетках которого написаны все числа от 1 до 15, а оставшаяся пуста. Разрешается пустую клетку поменять с любой соседней местами.

$$\begin{array}{|c|c|c|c|}
\hline
1 & 2 & 3 & 4\\
\hline
5 & 6 & 7 & 8\\
\hline
9& 10& 11& 12\\
\hline
13& 14& 15&\\
\hline
\end{array}
$$

Обычно стартовая или финишная позиция для игры -- это указанная выше расстановка чисел. Будем называть её базовой расстановкой.
Далее в разных вариантах игр спрашивают разное. В первоначальной постановке мистер Ной Палмер Чепмэн, создатель головоломки, предлагал при помощи указанных правил из базовой расстановки получить магический квадрат, то есть такую расстановку, что сумма чисел в любой строке и любом столбце была бы одинакова. В таком виде игра в пятнадцать увидела свет в 1874 году в США и к 1880 году стала общеизвестной. 

Другая её вариация состояла в следующем: дана расстановка, которая отличается от базовой только тем, что числа 14 и 15 поменяны местами

$$\begin{array}{|c|c|c|c|}
\hline
1 & 2 & 3 & 4\\
\hline
5 & 6 & 7 & 8\\
\hline
9& 10& 11& 12\\
\hline
13& 15& 14&\\
\hline
\end{array}
$$

Требовалось предъявить такую последовательность ходов, которая переводит эту расстановку в базовую. За решение этой задачи было обещано вознаграждение.

Мы покажем, что решить эту задачу нельзя. Сопоставим каждой расстановке чисел перестановку из $S_{16}$. Прежде всего, вместо пустой клетки поставим число 16. Потом, выпишем числа из всех ячеек, в том порядке, который задаёт базовая перестановка, то есть перечисляя их слева направо, сверху вниз.

Теперь заметим, что если на данном шаге мы находимся в положении, соответствовавшем перестановке $\sigma$, то на следующем шаге, мы получим перестановку $\tau \sigma$, где $\tau$ -- транспозиция.

Пусть мы за $k$ шагов получили из расположения в котором поменяны 14 и 15 местами базовую расстановку. Это означает, что есть некоторые транспозиции $\tau_1,\dots, \tau_k$, что 
$$\id= \tau_1\dots \tau_k\,(14,15).$$
Сравнивая знак справа и слева получаем, что $1=(-1)^{k+1}$, то есть, что $k$ нечётно.

С другой стороны посмотрим на путешествие числа 16 (пустой клетки). В результате число 16 должно вернуться на место. Это означает, что перемещений вверх было столько же, сколько и перемещений вниз, а перемещений вправо столько же, сколько и перемещений влево. Но тогда в сумме число перемещений чётно. Противоречие!


Важным математическим объектом является само множество, а точнее группа чётных перестановок.

\dfn Знакопеременной группой $A_n$ называется
$$A_n=\{\sigma \in S_n\,|\, \sigma \text{ чётна } \} = \Ker \sgn \leq S_n.$$
\edfn

\rm Группа чётных перестановок состоит из $\frac{n!}{2}$ элементов.
\erm

\subsection{Образующие $S_n$ и $A_n$}

Наше рассмотрение понятия чётности началось с того, что мы пытались понять, какие перестановки порождают $S_n$. Попытаемся построить удобные порождающие системы для групп $S_n$ и $A_n$.

Прежде всего сформулируем общий факт про образующие, которым мы уже один раз неявно воспользовались

\utv Пусть $g_1,\dots,g_k $ -- образующие $G$. Для того, чтобы набор $h_1,\dots,h_l\in G$ порождал  $G$ необходимо и достаточно, чтобы все $g_i$ выражались через $h_1,\dots,h_k$. 
\eutv

\utv Группа $S_n$ порождена перестановками $(12), (13),\dots, (1n)$
\eutv
\proof Мы уже знаем, что группа $S_n$ порождена транспозициями. Значит, надо при помощи транспозиций вида $(1i)$ получить произвольные транспозиции.  Имеем
$$(ij)=(1i)(1j)(1i).$$
Что и завершает доказательство.
\endproof

В этой системе образующих $n-1$ перестановка. Можно ли обойтись меньшим числом? Да. Но прежде докажем обобщение той конструкции, которую мы применили для вычисления знака транспозиции.

\utv Пусть $g\in S_n$ и $c=(a_1,\dots,a_k)\in S_n$. Тогда 
$$gcg^{-1}=(g(a_1),\dots,g(a_k)).$$
\eutv
\proof Достаточно дословно повторить аргумент с изменением нумерации.
\endproof

\utv Пусть $\sigma=c_1\dots c_k$ разложена в произведение независимых циклов. Тогда для любого $g\in S_n$
$$g\sigma g^{-1}= gc_1 g^{-1}\dots gc_kg^{-1},$$
есть произведение независимых циклов той же длины, что и у $\sigma$.
\eutv
\proof Равенство для $g\sigma g^{-1}$ очевидно. Так же ясно, что длины циклов $gc_i g^{-1}$ такие же как и у $c_i$. Осталось заметить, что если множества $\{a_1,\dots,a_k\}$ и $\{b_1,\dots,b_l\}$ не пересекались, то $\{g(a_1),\dots,g(a_k)\}$ $\{g(b_1),\dots,g(b_l)\}$ тоже не пересекаются. Это показывает, что $gc_i g^{-1}$ независимые циклы.
\endproof

В качестве завершающего аккорда покажем, что разложение на циклы одинаковой длины необходимо для существования такого $g$. Но сначала пара определений.
\dfn Пусть $\sigma \in S_n$. Тогда её цикленным (или цикловым) типом называется набор упорядоченных пар $(1,k_1),\dots, (n,k_n)$, где $k_i$ -- это число орбит размера $i$ относительно $\sigma$. В этом определении фигурируют именно орбиты, чтобы избежать неоднозначности связанной с циклами длины один.
\edfn

\dfn Пусть $G$ -- некоторая группа. Если $g,h\in G$, то элемент $ghg^{-1}$ называется сопряжённым к $h$ при помощи $g$. Два элемента $h_1$ и $h_2$ называются сопряжёнными, если существует $g \in G$, что $gh_1g^{-1}=h_2$.
\edfn 

\thrm Две перестановки $\sigma_1,\sigma_2\in S_n$ сопряжены тогда и только тогда, когда у них одинаковые цикленные типы.
\ethrm
\proof Надо лишь показать, что одинаковы цикленные типы для перестановок означают, что они сопряжены. Предъявим алгоритм построения перестановки $g$. Для этого выпишем все циклы в $\sigma_1$ в порядке возрастания их длины, не исключая циклы длины 1. Аналогично сделаем для $\sigma_2$. 
$$\sigma_1=(a_1)\dots(a_s)(a_{s+1}a_{s+2})\dots (a_{n-t}\dots a_n)$$
$$\sigma_2=(b_1)\dots(b_s)(b_{s+1}b_{s+2})\dots (b_{n-t}\dots b_n)$$
Под циклом длины $k$ в $\sigma_1$ расположен цикл длины $k$ в $\sigma_2$. Положим $g(a_i)=b_i$. Так как наборы из $a_i$ и из $b_i$ содержат каждый элемент из $\{1,\dots,n\}$ по одному разу, то получилась перестановка. Очевидно $g\sigma g^{-1}=\sigma_2$.
\endproof

Вернёмся к образующим симметрической группы.

\utv Группа $S_n$ порождена перестановками $(12), (1 \dots n)$. 
\proof Выразим перестановки $(1i)$ через данные. Вначале умножим $(12)(12\dots n)=(2\dots n)=\gamma$. Теперь
$$\gamma^{i-2}(12)\gamma^{-(i-2)}=(1i).$$
\endproof
\eutv

Теперь получим набор образующих для $A_n$. Доказательство так же будем вести по индукции.


\thrm Группа $A_n$ порождена циклами $(123),\dots,(12n)$. 
\ethrm
\proof При $n=3$ утверждение очевидно. Возьмём теперь $\sigma\in A_n$ и пусть $\sigma(n)=i$. Заметим, что перестановка $(12n)^2(12i)$ переводит $i\to n$. Тогда $\sigma'=(12n)^2(12i) \,\sigma$ лежит в  $A_{n-1}$, так как она $n$ переводит в $n$ и является чётной, как произведение чётных перестановок. Но по индукционному предположению $\sigma'$ выражается через $(123),\dots,(12\,n-1)$. Значит исходная перестановка выражается через $(123),\dots,(12n)$.
\endproof


\utv Группа $A_n$ порождена циклами $(123),(12\dots n)$, если $n$ нечётно  и  $(123),(2\dots n)$, если $n$ чётно.
\proof В случае нечётного $n$ рассмотрим произведение $(123)^{-1}(12\dots n)=(3\dots n )= \gamma$. Теперь при помощи $\gamma$ получим 
$$\gamma^{i-3}(123)\gamma^{-(i-3)}=(12i).$$
Для чётных $n$:
$$\gamma_i= (2\dots n)^{i-2} (123)(2\dots n)^{-(i-2)}=(1, i,i+1).$$
Теперь 
$$\delta_i=\gamma_{i+1}\gamma_i \gamma_{i+1}^{-1}= (1, i+1,i+2)(1, i,i+1)(1, i+1,i+2)^{-1}=(i+1,i,i+2)=(i,i+2,i+1).$$
Теперь, если мы получили $(12i)$, то получим и $(1,2, i+1)$. Точнее
$$\delta_i^2 (12i)\delta_i^{-2}=(1,2,i+1).$$
При $i+1=n$ стоит воспользоваться $\delta_{i-1}$. Заметим, что это доказательство не работает для $A_4$. Здесь нужно повозиться руками.
\endproof
\eutv


\upr Придумайте доказательство, которое работает в чётном случае лучше (В частности, в случае $A_4$).
\eupr



\section{Разложение в произведение}
Представим себе, что вам дана группа, например как подгруппа в $S_n$ заданная некоторыми образующими. Можно ли как-то упростить её представление? Главный вопрос здесь, что значит упростить. Я предлагаю считать упрощением тот факт, что группа изоморфна прямому произведению. Действительно, пусть 
$$G\cong G_1 \times G_2.$$
Тогда, если  $G$ конечная, то $|G|=|G_1|\cdot |G_2|$. В частности, если обе группы $G_1$ и $G_2$ нетривиальны, то $|G_1|< |G|$ и $|G_2|<|G|$, что уже свидетельствует о некотором упрощении. Покажем ещё несколько результатов в этом направлении.

\utv Пусть $(g,h)\in G_1\times G_2$. Тогда $\ord (g,h)= \Nok(\ord g, \ord h)$.
\eutv
\proof По определению произведения $(g,h)^d=(g^d,h^d)$. Эта пара равна $(e,e)$ только если $g^d=e$ и $h^d=e$. Первое условие означает, что $d\di \ord g$, а второе, что $d\di \ord h$. То есть $d\di \Nok(\ord g, \ord h)$. Наименьшее такое $d$ и есть $\Nok(\ord g, \ord h)$.
\endproof

Произведение $G_1\times G_2$ содержит внутри себя подгруппы $G_1\times \{e\}$  и $\{e\}\times G_2$. Первая -- это фактически $G_1$, а вторая -- это $G_2$. Будем обозначать эти подгруппы просто как $G_1$ и $G_2$. Такое замечание позволяет построить образующие $G_1\times G_2$, если известны образующие для $G_1$ и $G_2$.

\thrm Пусть $g_1,\dots,g_k$ образующие $G_1$, а $h_1,\dots,h_l$ образующие $G_2$. Тогда $(g_1,e), \dots, (g_k,e), (e,h_1),\dots,(e,h_l)$ -- это образующие $G_1\times G_2$.
\ethrm
\proof Рассмотрим пару $(g,h)$. Она представлена в виде произведения двух пар $(g,h)=(g,e)(e,h)$. Представим элемент $g$ в виде $g=g_{i_1}^{\eps_1}\dots g_{i_s}^{\eps_s}$ -- это даст аналогичное разложение для $(g,e)$. Так же для $(e,h)$.
\endproof

Осталось только понять как извлечь из группы $G$ информацию про то, изоморфна ли она прямому произведению каких-то групп. Предположим, что так произошло и $G\cong G_1\times G_2$. Внутри $G_1\times G_2$ есть подгруппы $G_1\times \{e\}$ и $\{e\}\times G_2$, изоморфные $G_1$ и $G_2$ соответственно. Тогда внутри $G$ должны быть подгруппы изоморфные $G_1$ и $G_2$. Это сразу означает, что кандидаты на сомножители мы должны искать среди подгрупп в $G$. На самом деле, можно заметить, что так как в произведении произвольная пара $(g,h)=(g,e)(e,h)$ есть произведение элементов из $G_1\times \{e\}$ и $\{e\}\times G_2$, то аналогичное условие выполнено и для $G$. Это приводит к определению

\dfn Будем говорить, что группа $G$ раскладывается в виде произведения своих подгрупп $G_1$ и $G_2$, если отображение
$$f\colon G_1\times G_2 \to G$$
$$\quad (g,h)\to gh$$
является изоморфизмом. Будем записывать этот факт как $G=G_1\times G_2$.
\edfn

Какие свойства необходимо наложить на подгруппы $G_1$ и $G_2$, чтобы $G$ разложилось в их прямое произведение.Заметим, что свойства этих подгрупп должны быть такие же, как и у $G_1\times \{e\}$ и $\{e\}\times G_2$ в $G_1\times G_2$. Сформулируем их:\\
1) $G_1\times \{e\} \cap \{e\}\times G_2 =\{(e,e)\}$. Действительно, у элемента пересечения каждая компонента равна $e$.\\
2) Если $(g,e)\in G_1\times \{e\}$, а $(e,h)\in \{e\}\times G_2$, то $(g,e)(e,h)=(e,h)(g,e)$. Это верно, потому что оба произведения равны $(g,h)$.\\
3) Любой элемент из $G_1\times G_2$ есть произведение элемента из $G_1\times \{e\}$  и элемента из $\{e\}\times G_2$. Запишем это в ослабленном виде $\lan G_1\times \{e\},\{e\}\times G_2\ran = G_1\times G_2$.\\

Итак, если группа $G$ изоморфна произведению двух подгрупп, то в ней должны быть подгруппы с указанными свойствами. Оказывается, что этих свойств и достаточно.

\thrm Пусть даны две подгруппы $G_1,G_2 \leq G$. Тогда $G_1\times G_2=G$, тогда и только тогда, когда\\
1) $G_1\cap G_2 =\{1\}$.\\
2) Если $g\in G_1$, а $h\in G_2$, то $gh=hg$.\\
3) $\lan G_1,G_2\ran = G$.
\ethrm
\proof Проверим, что указанное отображение есть гомоморфизм. Пусть $g_1,g_2 \in G_1$, а $h_1,h_2\in G_2$. Тогда образ $$f((g_1,h_1)(g_2,h_2))=g_1g_2 h_1h_2.$$
С другой стороны перемножая образы произведения получаем 
$$f((g_1,h_1))f((g_2,h_2))=g_1h_1 g_2h_2.$$
Осталось заметить, что центральные элементы можно переставить благодаря условию 2).

Покажем, то что $f$ -- мономорфизм. Для этого заметим, что пара $(g,h)\in \Ker f$, если $gh=e$, то есть $g=h^{-1}$. Но правая часть лежит в $G_1$, а левая -- в $G_2$. Благодаря условию на пересечение мы знаем, что это единичный элемент. То есть $g=h=e$. Значит ядро  тривиально. 

Покажем сюръективность. Заметим, что образ при гомоморфизме  -- это подгруппа. В данном случае, это подгруппа в  $G$ и она содержит $G_1$ и $G_2$. Но тогда по третьему условию образ равен  $G$.
\endproof

\rm Условие 3) равносильно тому, что любой элемент из $G$ раскладывается в виде произведения $gh$, где $g\in G_1$, а $h\in G_2$ (если есть второе условие). Так же условия 1) и 3') можно заменить на условие, что разложение вида $gh$ единственно.  
\erm

\exm \enm
\item Пусть $H=\lan (123) (56), (124)(67)\ran \leq S_7$. Покажем, что $H$ раскладывается в произведение двух групп. Прежде всего поменяем набор образующих у $H$. Пусть $h=(123) (56)$, а $g=(124)(67)$. Тогда $h^3=(56)$ И, значит, циклы $(56)$, $(123)$ по отдельности тоже лежат в $H$. Аналогично
с $(124)$ и $(67)$. Значит $H$ порождена
$$H=\lan (123),(56), (124), (67)\ran.$$
Теперь легко заметить две подгруппы в $H$, в произведение которых она раскладывается: $H_1=\lan (123), (124)\ran$ и $H_2=\lan (56), (67)\ran$. Действительно, так как образующие из $H_1$ независимы с образующими $H_2$, то все возможные элементы из $H_1$ и $H_2$ коммутируют  между собой, а все элементы из пересечения обязаны совпадать с тождественной перестановкой. Понятно, что $H$ порождена $H_1$ и $H_2$.

Теперь, неплохо бы понять, чему изоморфны $H_1$ и $H_2$. Заметим, что $H_1$ это в точности $A_4$. Проще всего это понять, увидев в образующих $H_1$ образующие $A_4$. Но можно это сделать используя косвенные аргументы: заметим, что порядок $H_1$ делится на $3$ по теореме Лагранжа так как группа содержит элемент порядка $3$. С другой стороны, произведение $(123)(124)=(13)(24)$ и сопряжение $(123) (13)(24)(123)^{-1}=(12)(34)$ дают две образующих подгруппе $V_4=\{\id, (12)(34), (13)(24), (14)(23)\}$.  Это означает, что $|H_1|\di 4$ и отсюда $|H_1|\di 12$. Но $H_1 \leq A_4$, в которой итак 12 элементов. Значит имеет место равенство

В свою очередь $H_2\simeq S_3$, как группа перестановок на элементах $5,6,7$. Отсюда $H\cong A_4 \times S_3$.
\item Не только независимость приводит к тому, что перестановки в $S_n$ коммутируют. Пусть $$H=\lan (12)(34)(567), (13)(24)(5678)\ran=\lan (12)(34), (567), (13)(24)(5678)\ran ,$$
то перестановки $g=(12)(34)$ и $h=(13)(24)(5678)$ коммутируют, что проверяется непосредственно. Теперь несложно показать, что $$H\cong \lan (12)(34) \ran \times \lan (13)(24)(5678), (567) \ran.$$
Первая группа изоморфна циклической группе $\mb Z/2$. А про  вторую можно утверждать, что она изоморфна $S_4$. Для этого покажем, что элемент из $\sigma \in H_2$, действующий тождественно на $5,6,7,8$ должен быть тривиальным. Посмотрим на чётность. Если записать $\sigma$ как произведение образующих, то образующая $(13)(24)(5678)$ входит в это разложение чётное число раз. Но тогда на элементах $1,2,3,4$ перестановка $\sigma$ действует тривиально. Итак, перестановка из $H_2$ определяется своим действием на $5,6,7,8$. Но несложно понять, что любая перестановка этих элементов реализуется (так как что-то является образующими чего-то).
\eenm



\subsection{Структура $\mb Z/n^*$}

Если $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}$, то мы уже знаем, что 
$$(\mb Z/n)^* \cong \mb (Z/p_1^{\alpha_1})^{*}\times \dots \times \mb (Z/ p_k^{\alpha_k})^{*}.$$
Итого надо разобраться со степенью простого. Для этого нам понадобится техническая лемма.



\lm Пусть $p$ простое и при нечётном $p$ верно, что $s\geq 1$, а при $p=2$ -- что $s\geq 2$. Тогда, если $$x \equiv 1+cp^s \mod p^{s+1}, \text{ то } x^p\equiv 1+cp^{s+1} (\mod p^{s+2}).$$
\elm
\proof Мы знаем по предположению, что $x=1+p^s(c+rp)$. Тогда $$x^p=(1+p^s(c+rp))^p=1+pp^s(c+rp)+C_p^2p^{2s}(c+rp)^2+\dots+p^{ps}(c+rp)^{p}\equiv 1+cp^{s+1} \mod p^{s+2}.$$
\endproof



\utv Пусть  $p$ -- нечётное простое. Тогда ${\mb Z/p^{\alpha}}^*$ изоморфна циклической группе 
$$\mb Z/p^{\alpha-1} (p-1) \cong \mb Z/(p - 1) \times \mb Z/p^{\alpha-1}.$$
Если же $p=2$, то если $\alpha = 1$, то группа $\mb Z/p^{\alpha}$ тривиальна.\\
2) если $\alpha \geq 2$, то $\mb Z/^*p^{\alpha}$ изоморфна произведению $ \mb Z/2 \times \mb Z/2^{\alpha-2}$.
\eutv
\proof
Пусть $p$ -- нечётное. Рассмотрим подгруппу $H_1=\{ x\equiv 1 \mod p\}$. Её порядок $p^{\alpha -1}$. Я утверждаю, что она порождена $1+p$. Для этого проверим, что порядок $1+p$ не меньше $p^{\alpha-1}$. Для этого надо проверить, что $(1+p)^{p^{\alpha-2}} \not\equiv 1 \mod p^{\alpha}$. Но это так -- последовательно используя лемму получаем, что
$$(1+p)^{p^{\alpha-2}}\equiv 1+p^{\alpha-1}\not\equiv 1 \mod p^{\alpha}.$$
Значит $H$ -- циклическая. Посмотрим теперь, откуда берётся циклическая подгруппа из $p-1$ элемента. Для этого рассмотрим $g$ -- первообразный корень степени $p-1$ в поле $\mb Z/p$. В частности, $g$ удовлетворяет уравнению $g^{p-1}-1=0$. По лемме Гензеля у решения этого уравнения есть подъём до решения $\hat{g}$ по модулю $p^{\alpha}$. Это значит, что $\hat{g}^{p-1}-1=0$ в $\mb Z/p^{\alpha}$ и $\hat{g}\equiv g \mod p$. Первое условие говорит, что порядок $g$ есть делитель $p-1$, а второе -- что это ровно $p-1$.

Итак в $\mb Z/^*p^{\alpha}$ есть две подгруппы $H_1=\lan 1\ran$ и $H_2=\lan \hat{g} \ran$. Так как порядки этих подгрупп взаимно просты, то сами подгруппы пересекаются только по нейтральному. Порядок подгруппы, их содержащей  должен делиться на $p-1$ и $p^{\alpha-1}$, то есть на $(p-1)p^{\alpha-1}$, откуда следует, что $H_1$ и $H_2$ порождают всю группу. Коммутативность так же имеет место. Итого, группа разложилась в произведение.

С $\mb Z/2^{\alpha}$ поступим так же. Случай $\alpha=1,2$ понятны. Рассмотрим подгруппу $H_1=\{\pm 1\}$ в $(\mb Z/2^\alpha)^*$. Если $\alpha\geq 3$, то в $(\mb Z/2^\alpha)^*$ есть нетривиальная подгруппа 
$$H_2=\{ x\in \mb Z/2^\alpha\,|\, x \equiv 1 \mod 4\}.$$
Покажем, что $(\mb Z/2^\alpha)^* =H_1 \times H_2$. Действительно, так как $-1\not\equiv 1 \mod 4$, то пересечение состоит из единичного элемента. Все элементы коммутируют так как мы находимся внутри абелевой группы. Для того, чтобы проверить третье условие заметим, что если $x\in (\mb Z/2^\alpha)^*$, что $x\equiv 1\mod 4$, то он и так лежит в $H_2$, а если $x\equiv -1 \mod 4$, то $x$ лежит в $(-1)H_2$.

Осталось понять, что $H_2$ -- циклическая. Рассмотрим элемент $5=1+4\in H_2$. Используя лемму получим, что $\ord 5 = 2^{\alpha-2}$, что и требовалось. 

\endproof

Подведём итог: 

\crl[Ответ в зависимости от разложения] Пусть $n=2^kp_1^{\alpha_1}\dots p_s^{\alpha_s}$. Тогда\\
1) если $k = 0,1$, то  $\mb Z/n^*$ изоморфна $$\prod_{i=1}^s \mb Z/p_i^{\alpha_i-1}(p_i -1).$$
2) если $ k\geq 2$, то $\mb Z/n^*$ изоморфна $$\mb Z/2 \times \mb Z/2^{k-2} \times \prod_{i=1}^s \mb Z/p_i^{\alpha_i-1}(p_i -1).$$
\ecrl

\subsection{Доказательство теоремы Рабина}

\thrm[Рабин] Пусть $n$ нечётное составное число,  $n>9$. Тогда $S(n)\leq \frac{\varphi(n)}{4}$, где $S(n)$ -- множество свидетелей простоты в тесте Миллера-Рабина.
\ethrm
\proof Пусть $n=p_1^{\alpha_1}\dots p_k^{\alpha_k}$, а $n-1=2^sd$, где $d$ -- нечётно. Покажем, что $S(n)$ лежит в некоторой подгруппе внутри $\mb Z/n^*$ индекса по крайней мере $4$.

Для этого рассмотрим  такое наибольшее $l$, что для любого $p_i$ существует $b$, что $b^{2^l}=-1 \mod p_i^{\alpha_i}$ (понятно, что такое есть). Рассмотрим теперь $m=2^ld$ и подгруппы
$$H=\{ x\,|\, x^m\equiv \pm 1 \mod n\}, \quad
H_1=\{ x\,|\, x^m \equiv \pm 1 \mod p_i^{\alpha_i} \,\,\forall i\}.$$
Очевидно, $H \leq H_1$. Заметим, что $S(n) \subseteq H$. Действительно, если $a\in S(n)$, то $(a^d)^{2^r}= -1$ (либо $a^d=1$, что нас и так устраивает).  Но тогда, взяв $b=a^d$ получим, что $l\geq r$, так как $l$ -- наибольшее.

Посчитаем индекс $H \leq H_1$. Удобно посмотреть на $H_0= \{ x^m \equiv 1 \mod n$. Понятно, что $H_0 \leq H$ -- подгруппа индекса 2. Посчитаем индекс $H_0$ в $H_1$. Для этого заметим, что в $H_1$ реализуется любая из $2^k$ комбинаций знаков в сравнениях (благодаря определению числа $l$). Каждая такая комбинация соответствует смежному классу. Теперь получаем, что индекс $$[H_1:H]=[H_1:H_0]/[H:H_0]=2^{k-1}$$
Итого: при $k\geq 3$ теорема доказана.


Пусть $k=2$. Заметим, что любой элемент из $H_1$ удовлетворяет свойству $x^{n-1}=1$. Для этого надо показать, что $n-1 \di 2m=2^{l+1}d$. Заметим, что все простые $p_i$ имеют вид $p_i=1+c_i2^{l+1}$, потому что по модулю каждого простого есть элемент порядка $2^{l+1}$. Но тогда и $n=1+c2^{l+1}$. Тогда $n-1\di 2^{l+1}$ и $n-1\di d$. Отсюда получаем, что $n-1 \di 2m$. Теперь  $x=p_1^{\alpha_1}p_2^{\alpha_2}$. Рассмотрим случай, когда в этом разложении есть кратные множители, то есть, скажем, $\alpha_1\geq 2$. В этой ситуации в $\mb Z/n^*$ есть элемент порядка $p_1^{\alpha_1-1}$. Но элемента такого порядка нет в $H_1$, так как порядки элементов $H_1$ делят $2m$. Итого $H_1 \neq \mb Z/n^*$ и общий индекс $[\mb Z/n^*: H]=[\mb Z/n^*:H_1][H_1:H]\geq 4$.

Пусть теперь $n=p_1p_2$.  Заметим, что в $Z/n^*$ есть элемент порядка $p_2-1$. Покажем, что его нет в $H_1$ так как он не удовлетворяет условию $x^{n-1}=1 \mod n$. Посмотрим на $n-1 \mod p_2-1$. Имеем $$n-1=p_1p_2-1\equiv p_1-1 \mod p_2-1.$$
Значит $(n-1,p_2-1)=(p_1-1,p_2-1)<p_2-1$. То есть в $H_1$ нет элемента порядка $p_2-1$. Что приводит к оценке индекса $H_1$ в $\mb Z/n^*$.

Последний случай: $n=p^{\alpha}$. Тогда индекс $H_1$ в $\mb Z/n^*$  делит (на самом деле равен) $p^{\alpha-1}$. Действительно, порядки элементов из $H_1$ делят и $n-1$ и порядок группы $p^{\alpha-1}(p-1)$. Значит порядки этих элементов делят $p-1$. Но группа $(\mb Z/p^{\alpha})^*$ циклическая. Все элементы порядка делящего $p-1$ содержатся в подгруппе из $p-1$ элемента. То есть $p-1 \di |H_1|$.    Если $p\geq 5$, то это нам подходит. Как и в случае  $p=3$, так как по нашему условию из этого следует, что $\alpha\geq 3$.

\endproof


\upr $S(n)$ -- не подгруппа в $\mb Z/n^*$.
\eupr

\subsection{Дополнение: другое доказательство теоремы Рабина}

Вообще, коль скоро мы знаем строение $\mb Z/n^*$, то можно просто в лоб подсчитать число элементов в $S(n)$ и вывести отсюда теорему Рабина. Можно посмотреть, что из этого получается.

\begin{thmm}[Рабин] Пусть $n$ нечётное составное число,  $n>9$. Тогда $|S(n)|\leq \frac{\varphi(n)}{4}$.
\end{thmm}
\proof Пусть $n$ -- это $p_1^{\alpha_1}\dots p_k^{\alpha_k}$, а $n-1=2^rd$. Тогда группа $\mb Z/n^*$ изоморфна группе
$$ \prod_{i=1}^k \mb Z/p_i^{\alpha_i-1}(p_i -1).$$
Видно, что в задаче выделенную роль играет степень двойки. Пусть $p_i-1=2^{s_i}r_i$. Тогда группа $\mb Z/p_i^{\alpha_i-1}(p_i -1)$ изоморфна 
$$\mb Z/2^{s_i}\times \mb Z/p_i^{\alpha_i-1}r_i.$$
Итого имеем 
$$\mb Z/n^*\cong \prod \mb Z/2^{s_i} \times \mb Z/p_i^{\alpha_i-1}r_i.$$ 
Рассмотрим элемент $x\in \mb Z/n^*$ и посмотрим, что же означает условие, что $x^m=1$ или $x^{2^sm}=-1$ для его компонент. 
Для того, чтобы $y=-1$ необходимо и достаточно, чтобы все его компоненты в группах $\mb Z/2^{s_i}$ были единственным элементом порядка $2$, а все компоненты в группах $\mb Z/p_i^{\alpha_i-1}r_i$ были равны 0 (здесь используется аддитивная запись).

Для начала посмотрим на компоненты в группах $\mb Z/p_i^{\alpha_i-1}r_i$. Если $2^smb=0 (\mod p_i^{\alpha_i-1}r_i)$, то это означает, что порядок $b$ делит $2^{s_i}m$. Множество таких элементов образует подгруппу порядка $\Nod(2^{s_i}m,r_ip_i^{\alpha_i-1})=\Nod(m,r_i)$, так как $(p_i,2^{s_i}m)=1$ и $(r_i,2^{s_i})=1$.

Теперь рассмотрим произведение $\prod \mb Z/2^{s_i}$. То, что элемент $2^{s}mx$ имеет порядок 2 $(\mod \mb Z/2^{s_i})$ означает, что $x$ имеет порядок $2^{s+1}$. Таким образом $T$ принадлежат только те элементы чьи компоненты в $\mb Z/2^{s_i}$ имеют одинаковый порядок для всех $i$.

Сколько таких элементов в $\prod \mb Z/2^{s_i}$? Рассмотрим $l= \min_{i=1}^k s_i$. Тогда порядок каждой компоненты элемента не может превосходить $2^l$ -- он не может получиться одинаковым по всем компонентам. Сколько элементов, которые имеют порядок $2^l$ по каждой компоненте? Их $2^{(l-1)k}$ штук. Аналогично элементов, чьи компоненты порядка  $2^i$ ровно $2^{(i-1)k}$ штук. Итого
$$\sum_{i=1}^l 2^{(i-1)k}= \frac{2^{lk}-1}{2^k-1}.$$ Заметим, что ещё есть первое условие которое подходит только для нейтрального элемента. Итого из сомножителя $\prod \mb Z/2^{s_i}$  подходящих нам вариантов ровно $$\frac{2^{lk}-1}{2^k-1}+1\leq 2\cdot 2^{k(l-1)}.$$
В случае $k,l\neq 1$ можно поставить строгое неравенство. Объединяя, получаем, что всего элементов в $T$ не более
$$2\cdot 2^{k(l-1)}\prod_i\Nod(r_i,m).$$
Покажем, что 
$$\frac{|S(n)|}{\ffi(n)}\leq 2\frac{ 2^{k(l-1)}\prod_i\Nod(r_i,m)}{(p_i-1)p_i^{\alpha_i-1}}\leq \frac{1}{4}.$$
Прежде всего заметим, что выражение $2^{l-1}\Nod(r_i,m)$ делит $\frac{p_i-1}{2}$.
Тогда можно написать оценку
$$2\frac{ 2^{k(l-1)}\prod_i\Nod(r_i,m)}{(p_i-1)p_i^{\alpha_i-1}}\leq \frac{2}{2^k}\prod \frac{1}{p_i^{\alpha_i-1}}.$$
Получаем, что если простых сомножителей не меньше 3-ёх, то неравенство выполнено. Действительно -- в знаменателе возникает множитель $8$, который может сократиться только с одной двойкой сверху. 

Рассмотрим случай, когда различных простых ровно 2. Заметим, что, если есть кратные множители то в последнем множителе есть не единичный элемент в знаменателе. 

Итак остался случай, когда есть два  не кратных множителя $n=pq$ или один кратный $n=p^{\alpha}$. При $n>9$  последний случай очевидно подходит. Теперь $n=pq$, $p>q\geq 3$. Вернёмся к изначальной оценке и посмотрим на $\Nod (m,p-1)=\Nod(m,r)$. Заметим, что $2^t \Nod (m,p-1)= \Nod(n-1,p-1)$. Оценим последний $\Nod$. Заметим, что $n-1=pq-1\equiv q-1 (\mod p-1)$.
Тогда $\Nod(n-1,p-1)\leq q-1 < p-1$ и, так как является делителем $p-1$, то меньше, чем $\frac{p-1}{2}$. 
Итого $$\Nod (m,p-1)\leq \frac{1}{2}\Nod (n-1,p-1)\leq\frac{1}{4}(p-1),$$
что и даёт нужное неравенство.
\endproof


\section{Нормальные подгруппы и факторгруппа}

Однако далеко не всегда группа раскладывается в прямое произведение. Так, например, $\mb Z/p^2$, где $p$ простое не раскладывается нетривиальным образом в произведение двух подгрупп. Действительно, в ней всего одна нетривиальная подгруппа $\lan p \ran$. Найти непересекающуюся с ней нетривиальную подгруппу не представляется возможным. 

Однако мы довольно хорошо умеем упрощать работу по модулю $p^2$ переходя к сравнениям по модулю $p$. С точки зрения теории групп это означает, что мы берём образ всех элементов относительно сюръективного гомоморфизма 
$$\mb Z/p^2 \to \mb Z/p.$$
Возможность проконтролировать ситуацию так же гарантируется тем, что мы знаем, что любые два прообраза у элемента $\mb Z/p$ отличаются на элемент ядра.
Рассмотрим подробнее такую ситуацию. Пусть дан сюръективный гомоморфизм $f \colon G \to H$. Какую информацию про группу $G$ мы можем получить зная информацию про $H$ и $\Ker f$?

\utv Пусть дан сюръективный гомоморфизм $f \colon G \to H$. Пусть так же $g_1,\dots,g_k$ образующие $\Ker f$, а $h_1,\dots, h_l$ образующие $H$. Если взять $h_i'\in G$, такие, что $f(h_i')=h_i$, то группа $G$ будет порождена $h_1',\dots,h_l', g_1,\dots,g_k$.
\eutv
\proof  Воспользуемся простенькой леммой
\lm Пусть $f\colon G \to H$ -- гомоморфизм групп. Тогда $f(g_1)=f(g_2)$ тогда и только тогда, когда $g_1\in g_2 \Ker f$.
\elm
Пусть $g\in G$. Тогда $f(g)=h_{i_1}^{\eps_1}\dots h_{i_s}^{\eps_s}$. Тогда $g'={h_{i_1}'}^{\eps_1}\dots {h'_{i_s}}^{\eps_s}$ обладает свойством $f(g')=f(g)$. Тогда по лемме $g\in g'\Ker f$, то есть выражается через нужные образующие.
\endproof

\utv  Если $G$ конечна  и  $f\colon G \to H$ сюръективный гомоморфизм групп, то $|G|=|\Ker f| |H|$.
\eutv
\proof Каждый элемент $H$ соответствует ровно одному смежному классу $G/\Ker f$ по лемме.
\endproof

Здесь в рассмотрение попала подгруппа $\Ker f$ в $G$. Именно она в паре с $H$ позволила восстановить некоторую информацию про $G$. Таким образом, если мы ищем гомоморфизм $G$ в некоторую группу $H$, то сначала логично найти кандидата на ядро этого гомоморфизма, то есть подгруппу в $G$. Любая ли подгруппа может быть ядром гомоморфизма? 

Нет. Действительно, если $f \colon G \to G_1$, и есть $g \in G$ и $x\in \Ker f$, то  сопряжённый $gxg^{-1}$ тоже лежит в $\Ker f$. 
$$f(gxg^{-1})=f(g)f(x)f(g)^{-1}=f(g)f(g)^{-1}=e.$$
Это приводит нас к следующему определению.

\dfn[Нормальная подгруппа] Подгруппа $H\leq G$ называется нормальной, если для любого $g\in G$ и любого $h \in H$ выполнено, что $ghg^{-1}\in H$. То, что $H$ является нормальной подгруппой будем обозначать как $H \nrml G$
\edfn

\exm \enm
\item Пусть $G$ -- абелева группа. Тогда любая подгруппа в $G$ нормальна.
\item Пусть $f\colon G \to H$ -- гомомморфизм групп. Тогда $\Ker f$ -- нормальная подгруппа в $G$.
\item Подгруппа $V_4=\{\id, (12)(34), (13)(24), (14)(23)\}$ нормальна в $S_4$.
\item Подгруппа, порождённая поворотом на $\pi$ нормальна в $D_n$.
\item Если $G=G_1\times G_2$, то $G_1,G_2\nrml G$.
\eenm


Сформулируем некоторые переформулировки этого свойства.

\utv Пусть $H \leq G$. Тогда следующие утверждения эквивалентны:\\
1) $H$ -- нормальная подгруппа в $G$.\\
2) для любого $g\in G$ $gHg^{-1}\subseteq H$.\\
3) для любого $g\in G$ $gHg^{-1}= H$.\\
4) для любого $g\in G$ $gH=Hg$.\\
5) для любого $g\in G$ $gH\subseteq Hg$.
\eutv

Посмотрим, что нормальные подгруппы раньше попадались нам.\\


\exm \enm
\item Пусть $A$ -- абелева группа. Тогда любая подгруппа $H$ в $A$ нормальна. Действительно если взять $h\in H$, то в силу коммутативности $aha^{-1} =h \in H$.
\item В частности, $n \mb Z \nrml \mb Z$. 
\item Ядро любого гомоморфизма $f\colon G \to H$ является нормальной подгруппой в $G$.
\item В частности, $A_n \nrml S_n$.
\item В качестве примера, не подходящего под общую конву, рассмотрим группу $$V_4=\{\id, (12)(34),(13)(24),(14)(23)\} \nrml S_4.$$ 
\eenm

Пусть теперь нам дана нормальная подгруппа $H\nrml G$. Как же построить гомоморфизм из $G$ в некоторую группу $G_1$, так, чтобы ядром было в точности $H$. Предположим, что $f\colon G \to G_1$ имеет ядром $H$. Тогда заметим, что все элементы вида $xH$ переходят туда же, куда и $x$. Заметим и обратное, если $f(y)=f(x)$, то $y=xh$, где $h\in \Ker f =H$. Таким образом, элементам $G_1$ должны однозначно соответствовать смежные классы $G/H$. Это соображение и положим в основу определения.

\dfn[Факторгруппа] Пусть $H\nrml G$. Определим на множестве смежных классов $G/H$ структуру группы положив 
$$g_1 H g_2 H= g_1g_2 H.$$
\edfn

\utv Приведённая конструкция действительно задаёт группу. Более того отображение $G \to G/H$ переводящее $g \to gH$ является сюръективным гомоморфизмом групп с ядром $H$. 
\eutv 
\proof Прежде всего проверим корректность. Пусть $g_1,g_2\in G$ и $g_1h_1$ и $g_2h_2$ эквивалентны им. Тогда $$g_1h_1g_2h_2=g_1g_2(g_2^{-1}h_1 g_2) h_2 \in g_1g_2H,$$
Что и требовалось. Здесь один раз пришлось воспользоваться нормальностью. Проверка ассоциативности происходит точно так же как и для $\mb Z/n$. А именно, если даны три класса $g_1H, g_2H, g_3H$, то
$$(g_1H g_2H) g_3H=g_1g_2H g_3H=(g_1 g_2) g_3H=g_1 (g_2 g_3)H=g_1H (g_2 g_3H)=g_1H (g_2H g_3H).$$
Нейтральным элементом является класс $1\cdot H=H$, обратным для класса $gH$ является класс $g^{-1}H$.
\endproof

\thrm[Универсальное свойство] Пусть $G,G_1$ -- группы, $H$ -- подгруппа в $G$. Тогда для любого гомоморфизма $f \colon G\to G_1$, такого что $ H \leq \Ker f$, существует единственный гомоморфизм $\ffi\colon G/H \to G_1$ такой, что <<треугольник коммутативен>>:
\begin{center}
\begin{tikzpicture}
\node (A) at (0, 0) {$G$};
\node (B) at (2.5, 0) {$G_1$};
\node (C) at (0, -1) {$G/H$};
\path[->,font=\scriptsize,>=angle 60]
(A) edge node[above]{$f$} (B)
(A) edge node[right]{$\pi$} (C);
\path[dashed,->,font=\scriptsize,>=angle 60]
(C) edge node[below]{$\exists !\, \ffi$} (B);
\end{tikzpicture}
\end{center}
\proof Заметим, что необходимо, чтобы $\ffi(gH)=f(g)$. Это показывает единственность. Покажем, что отображение $\ffi$, заданное этой формулой, корректно определено и является гомоморфизмом. Если взять другого представителя класса $g_1\in gH\subseteq g \Ker f $, то образ его, будет равен образу $g$.

Проверим, что это гомоморфизм: $\ffi(g_1g_2H)=f(g_1g_2)=f(g_1)f(g_2)=\ffi(g_1H)\ffi(g_2H).$
\endproof
\ethrm

\thrm Пусть $f\colon G \to G_1$ гомоморфизм групп. Тогда имеет место изоморфизм 
$$G/\Ker f \simeq \Im f.$$
Этот изоморфизм переводит класс $g \Ker f \to f(g)$. 
\ethrm
\proof Без ограничения общности можно считать, что $G_1=\Im f$. По универсальному свойству существует отображение $\ffi : G/H \to G_1$, заданное как $\ffi(gH)=f(g)$.

Покажем его инъективность. Пусть $\ffi(gH)=e$. Значит $f(g)=e$. Значит $g\in \Ker f=H$. Но тогда $gH=H$. То есть в ядре $\ffi$ лежит только тривиальный элемент факторгруппы, что  и требовалось.

Покажем сюръективность. Пусть $a\in \Im f$. Тогда существует $g\in G$, что $f(g)=a$. Тогда $a=\ffi(gH)$. Значит, отображение $\ffi$ сюръективно.
\endproof

В частности, это означает, что все сюръективные гомоморфизмы устроены как гомоморфизм факторизации. Обычно, однако, эту теорему используют для описания факторгруппы $G/H$, c помощью построения сюръективного гомоморфизма $G \to G_1$, ядро которого есть $H$. 



\exm \\
1) $S_n/A_n\simeq \{\pm 1\}$ так $A_n$  -- это ядро отображения знака.\\
2) $D_n/C_n \simeq \mb Z/2$ так как повороты -- это ядро отображения <<ориентации>>.\\
3) $\AGL_n(K)/\{f(x)=x+b\}\simeq \GL_n(K)$. Сдвиги -- это ядро гомоморфизма, сопоставляющего преобразованию вида $f(x)=Ax+b$ матрицу $A$.\\
4) В теореме Рабина мы встретили несколько подгрупп в группе $\mb Z/n^*$ 
$$H_2=\{ x\,|\, x^m\equiv 1 \mod n\} \leq H=\{ x\,|\, x^m\equiv \pm 1 \mod n\} \leq H_1=\{ x\,|\, x^m \equiv \pm 1 \mod p_i^{\alpha_i} \,\,\forall i\}.$$
Для того, чтобы посчитать индекс $[H_1: H_2]$ заметим, что есть гомоморфизм $H_1 \to \{\pm 1\}\times \dots \times \{\pm 1\} $ ($k$ раз, где $k$ -- число простых в разложении $n$)  заданный формулой $x \to x^m \mod p_i^{\alpha_i}$. По выбору $m$ этот гомоморфизм сюръективен. $H_2$ -- его ядро. Значит индекс $[H_1:H_2]$ есть число элементов в группе $\{\pm 1\}\times \dots \times \{\pm 1\} $, то есть $2^k$. Аналогично считается индекс $[H:H_2]$.


Если в группе $G$ есть нетривиальная нормальная подгруппа $H$, то это позволяет <<упростить>> вашу группу при рассмотрении многих вопросов, перейдя к паре групп $H$ и $G/H$. Если группа $G$ конечная, то рано или поздно этот процесс сойдётся к группам, в которых нет нетривиальных нормальных подгрупп.


\dfn Группа $G$ называется простой, если в $G$ нет нормальных подгрупп отличных от $G$ и $\{e\}$.
\edfn

Таким образом, простые конечные группы -- это такие кирпичики из которых <<собраны>> все другие конечные группы. Большим достижением 20 века стала классификация всех конечных простых групп. Некоторые примеры таких групп мы уже определили, к другим -- приблизились:\\
1) Группа $\mb Z/p$, где $p$ -- простое, является простой группой. Других конечных простых абелевых групп не бывает.\\
2) Группа $A_n$ простая при $n\geq 5$.\\
3) Определим группу $$\PSL_n(K)=\{A \in \GL_n(K)\,|\, \det A=1\}/ \{ \lambda E_n\,|\, \lambda^n=1, \lambda\in K \}.$$
Это простая группа за исключением случаев, когда $K=\mb Z/2$, $n=2$ и  $K=\mb Z/3$, $n=2$.

Дальнейшее смотри в \cite{}


\section{Действие группы на множестве}


В этой лекции мы поговорим о геометрическом взгляде на группы. На группы, как группы симметрий некоторых множеств. Ещё до того как дать определение посмотрим на примеры такого появления групп. Пусть $X$ -- множество. Тогда все элементы в этом множестве равноправны, все их переставляет между собой группа биекций $S_X$. Это группа симметрий множества $X$, когда на нём не задано никакой дополнительной структуры. 

Рассмотрим граф $G$ -- множество вершин которого обозначим за $V$, а множество рёбер за $E$. Тогда группой автоморфизмов графа  $\Aut(G)$ называется подгруппа в группе перестановок вершин $S_V$, элементы которой переводят рёбра в рёбра. Это группа -- группа симметрий множества $V$ с дополнительной структурой -- структурой графа. Однако так же эта группа переставляет между собой рёбра графа, клики внутри графа и т.д.

Рассмотрим другой пример: возьмём обычную плоскость $\mb R^2$ и рассмотрим все биекции $f\colon \mb R^2 \to \mb R^2$ сохраняющие расстояние. То есть, должно быть выполнено $|x-y|=|f(x)-f(y)|$. Множество таких биекций образует подгруппу в $S_{\mb R^2}$, которая обозначается $\Isom(\mb R^2)$ и называется группой изометрий (движений, самосовмещений) плоскости. 

Рассмотрим правильный $n$-угольник c  центром в нуле. Тогда можно рассмотреть подгруппу в группе $\Iso (\mb R^2)$ состоящую из тех преобразований, которые оставляют этот $n$-угольник на месте. Это группа $D_n$.

Попытаемся сформулировать характерные для всех этих примеров черты.


\begin{defn}
Действием группы $G$ на множестве $X$ называется отображение $\cdot \colon G\times X\to X$, удовлетворяющее аксиомам\\
1) $\forall x \in X$  выполнено, что $e\cdot x=x$.\\
2) $\forall x \in X$, $\forall g,h\in G$ выполнено, что $(gh)\cdot x= g\cdot (h\cdot x)$.\\
Если задано действие $G$ на $X$, то $X$ будем называть $G$-множеством. Тот факт, что группа $G$ действует на $X$ будем обозначать как $G \curvearrowright X$. 
\end{defn}


\exm \\
1) $S_n$ действует на множестве $\{1,\dots,n\}$.\\
2) Группа $G$ действует сама на себе домножениями слева.\\
3) Группа вращений пространства относительно нуля действует на точки пространства.\\
4) Группа $S_n$ действует на парах $\{1,\dots,n\}\times \{1,\dots,n\}$ (и на тройках тоже).\\
5) Группа $D_n$ самосовмещений правильного $n$-угольника действует на вершинах, диагоналях, парах диагоналей и т.д. в этом правильном $n$-угольнике.\\
6) И вообще, если группа $G\curvearrowright X$, то $G \curvearrowright X\times X\times \dots \times X$ , а так же $G \curvearrowright Y^X$, где $Y$ -- произвольное множество. Последнее действие задаётся формулой $g,f \to f(g^{-1}x)$.\\
7) Если $H\leq G$ и $G$ действует на $X$, то и $H$ действует на $X$. Более общо, если задан гомоморфизм $f\colon H \to G$ и $G$ действует на $X$, то и $H$ действует на $X$. Зададим это действие формулой $h,x \to f(h)\cdot x$.\\

Оказывается, что при помощи последней конструкции любое действие любой группы на любом множестве можно свести к действию перестановок на этом множестве.


\thrm
Пусть заданы группа $G$ и множество $X$. Тогда имеет место взаимооднозначное соответствие между различными действиями группы $G$ на $X$ и  гомоморфизмами $G \to S_X$. А именно, для каждого действия $G \curvearrowright X$ построим гомоморфизм, переводящий $g \to T_g$, где $T_g(x)=gx$ -- биекция заданная домножением на $g$. 

Обратно, по гомоморфизму $\psi \colon G \to S_X$ определим действие, такое что $gx=\psi(g)(x)$.
\ethrm
\proof
Пусть дано действие $G$ на $X$. Покажем, что отображение $g \to T_g$ есть гомоморфизм групп $G \to S_X$.  Прежде всего заметим, что $T_g$ биекция, так как у этого отображения есть обратное -- $T_{g^{-1}}$. 
Покажем, что $T_{g_1g_2}=T_{g_1}T_{g_2}$. Применим  левую часть к конкретному элементу $x$. Получаем $$T_{g_1g_2}(x)=g_1g_2x=T_{g_1}(g_2x)=T_{g_1}(T_{g_2}(x)),$$
что и требовалось. Мы уже отмечали, что $\psi$ -- гомоморфизм. Осталось показать, что два эти соответствия между действиями группы $G$ на $X$ и гомоморфизмами $G \to S_X$ взаимно однозначны. 

Построим по действию $G \curvearrowright X$ гомоморфизм $G \to S_X$ и потом обратно действие. Покажем, что получилось исходное действие. посмотрим куда переходит пара $(g,x)$
$$(g,x) \to T_g(x)=gx.$$
Что и требовалось. В другую сторону. Если дан некоторый гомоморфизм $\psi$, проверим, что для всякого элемента $g\in G$ результат гомоморфизма построенного по действию, определённому через $\psi$ такой же, как и у $\psi$. Возьмём $x\in X$ и проверим, что построенные по $g$ перестановки действуют на нём одинаково. 
$$T_g(x)=gx=\psi(g)(x).$$
\endproof

Раньше все группы рассматривались как подгруппы группы перестановок. Покажем, что так всегда можно думать:

\crl[Теорема Кэли] Любая  группа $G$ вкладывается в $S_G$. В частности, если $G$ конечная порядка $|G|=n$, то есть подгруппа  $H \leq S_n$ изоморфная $G$.
\ecrl
\proof Рассмотрим действие $G \curvearrowright G$ сдвигами слева. Это действие задаёт гомоморфизм $G \to S_G$ по правилу $g\to (h \to gh)$. Покажем, что ядро этого гомоморфизма тривиально. Рассмотрим $g\neq e$ и $h=e$. Тогда $e \to g\cdot e=g \neq e$. То есть перестановка, заданная $g$, не тождественная. 
\endproof

Для следующего примера сформулируем простейший геометрический факт.

\fct Изометрия трёхмерного пространства однозначно определяется образами 4-ёх точек не лежащих в одной плоскости.
\efct

Посмотрим, что благодаря нашей теореме можно сказать про группу $G$ всех изометрий, сохраняющих тетраэдр. Для краткости будем называть эту группу группой самосовмещений тетраэдра. 

Заметим, что эта группа действует на вершинах тетраэдра. Это даёт изоморфизм c $S_4$. 


\noindent С понятием действия группы на множестве связаны несколько определений:

\dfn Пусть $G\curvearrowright X$. Подмножество $Y\subseteq X$ называется инвариантным подмножеством относительно этого действия, если для всех $g\in G$ выполнено $g(Y)\subseteq Y$.
\edfn

Если группа действует на множестве $X$ и $Y$ инвариантно, то действие можно ограничить на $Y$. Приведём конструкцию самых маленьких инвариантных множеств


\dfn
Орбита элемента $x$ -- это множество элементов в которые можно попасть из $x$ при помощи действия группы $G$, а именно
$$O_x=G\cdot x:=\{y\in X \,|\, \exists g\in G, \,\, g\cdot x=y\}.$$
\edfn

Орбита является наименьшим возможным инвариантным подмножеством, содержащим точку $x$. Есть ещё одно подмножество, связанное с точкой из $X$. На этот раз в группе $G$.

\rm Отношение <<лежать в одной орбите>> есть отношение эквивалентности. Множество $X$ разбивается в дизъюнктное объединение орбит.
\erm

\exm\\
1) Пусть $G$ это группа поворотов плоскости, оставляющих на месте ноль, а $X$ -- сама плоскость на которой $G$ действует стандартным образом. Тогда орбиты при таком действии есть окружности с центром в нуле и, как исключение, точка $0$.\\
2) Пусть $H$ -- подгруппа $G$. Определим действие $H$ на $G$ по формуле $(h,g) \to gh^{-1}$. Орбиты относительно этого действия  -- это классы смежности относительно $H$.\\
3) Группа $G$ действует на себе сопряжениями $(g,h) \to ghg^{-1}$. Орбиты относительно этого действия есть классы сопряжённости.\\

\dfn
Стабилизатором точки $x$ называется множество элементов группы $G$, оставляющих её на месте, то есть
$$\Stab_x=G_x:= \{g\in G \,|\, g\cdot x=x\}.$$
\edfn

\lm Пусть $G$ действует на множестве $X$. Тогда для любой точки $x\in X$ множество $\Stab_x$ является подгруппой в $G$.
\elm

\rm
Для стабилизатора и орбиты приведены два обозначения. Здесь будут использоваться первые.
\erm








Прежде всего мы установим связь между геометрией  и внутренним строением группы, показав, что  левые смежные классы группы $G$ по стабилизатору точки $x$ соответствуют точкам орбиты $O_x$.

\thrm[О связи орбиты и стабилизатора] Пусть группа $G$ действует на множестве $X$ и задана точка $x \in X$. Тогда для любой точки $y \in O_x$ множество $\{h\in G\,|\, hx=y\}$ является левым смежным классом группы $G$ по стабилизатору $\Stab_x$. Обратно, для любого элемента $h\in g\Stab_x$ верно, что $hx=gx$. В частности, корректно определены и определяют взаимооднозначное соответствие $O_x \leftrightarrow G/\Stab_x$ отображения, заданные как $$y\in O_x \to \{h\in G\,|\, hx=y\}\, \text{ и }\, g\Stab_x \to gx \in O_x.$$
\ethrm
\proof Прежде всего покажем корректность указанных соответствий. Пусть $y\in O_x$ и $g$ такой, что $y=gx$. Покажем, что множество $\{h\in G\,|\, hx=y\}$ элементов группы, переводящих $x \to y$ есть левый смежный класс $g\Stab_x$. 

Действительно, если элемент $h$ лежит в $g\Stab_x$, то $h=gp$, где $p \in\Stab_x$ и, следовательно $hx=gpx=gx=y$. Обратно, если есть элемент $h$, что $hx=y$, то тогда $g^{-1}hx=g^{-1}y=x$. Отсюда $g^{-1}h \in\Stab_x$ и следовательно $h=g(g^{-1}h) \in g\Stab_x$. Это и доказывает равенство $\{h\in G\,|\, hx=y\}=g\Stab_x$.


Перейдём к соответствию делающему из смежного класса элемент орбиты. Пусть класс имеет вид $g\Stab_x$. Тогда сопоставим ему $y=gx \in O_x$. Здесь тоже есть проблема с корректностью. Дело в том, что в качестве элемента $g$ может быть взять какой-то другой элемент смежного класса и получиться, априори, другой элемент орбиты. Выберем другой элемент $h=gp\in g\Stab_x$ и  покажем, что $hx=gx$. Действительно $hx=gpx=gx$ так как $p\in\Stab_x$. Итого, обратное соответствие корректно задано

Теперь должна быть проведена небольшая, но необходимая проверка, что оба соответствия действительно взаимно обратны. 
Пусть мы стартовали с элемента $y\in O_x$, ему соответствует класс $g\Stab_x$, где $y=gx$. В свою очередь этому классу соответствует элемент $gx=y$. Что и требовалось. 

В другую сторону. Пусть мы взяли смежный класс $g\Stab_x$. Ему соответствует элемент $y=gx$. Этому элементу $y$ соответствует класс $\{h\in G\,|\, hx=y\}$. Заметим, что $g$ лежит в этом классе. Из свойства, что если классы пересекаются, то они совпадают, следует, что это и есть $g\Stab_x$.
\endproof

\crl
Пусть $G$ -- конечная группа, действующая на множестве $X$. Если задан элемент $x\in X$, то $$|G|=|O_x||\Stab_x|.$$
\ecrl


Пусть группа $G$ действует на множестве $X$ и вы знаете, что $G$ порождена элементами $g_1,\dots,g_n$. Как найти орбиту элемента $x \in X$ в такой ситуации?

Ответ ожидаемый: надо подействовать на $x$ образующими $g_1\dots,g_n$, то есть найти элементы $y_1=g_1x,\dots, y_n=g_n x$, потом применить образующие к новым получившимся элементам $y_1,\dots, y_n$, и так далее, пока на некотором шаге  новые элементы перестанут появляться. Все элементы $X$, что появились к этому моменту и составляют  $O_x$.

Посмотрим, как это работает на простеньком примере:

\upr Пусть $H$ подгруппа $S_4$, порождённая перестановками $(123)$ и $(13)(24)$. Тогда $H=A_4$.
\eupr
\proof[Решение]
Действительно, порядок $H$ должен делиться на порядок её элемента $(123)$, который равен 3. Осталось показать, что $|H|\di 4$. Для этого посчитаем орбиту элемента 1. Если мы применим перестановку $(123)$ несколько раз, то получим, что в орбите 1 лежат 2 и 3. Но при действии перестановкой $(13)(24)$ из $2$ получается $4$-ка, которая по транзитивности тоже лежит в орбите $1$. Так как больше элементов в множестве $\{1,2,3,4\}$ нет, то получаем, что $O_1=\{1,2,3,4\}$. Так как порядок орбиты делит порядок группы получаем, что число элементов в $H$ делится на 4. Итого, порядок $H$ делится на 12. Но $H$ и так лежит в группе из $12$ элементов -- в $A_4$. Значит $H=A_4$.
\endproof

Так же получаем ещё одно тривиальное следствие:

\crl Пусть $G$ -- конечная группа. Тогда количество элементов в любом классе сопряжённости делит $|G|$. 
\ecrl



\subsection{Теорема Коши}

Разберёмся со старым долгом -- частичным обращением теоремы Лагранжа:

\thrm[Коши] Пусть $G$ конечная группа порядок которой делится на простое число $p$. Тогда в группе $G$ есть элемент порядка $p$.
\ethrm
\proof Рассмотрим декартово произведение $G^p$. На этом декартовом произведении действует группа $\mb Z/p$ отображающая упорядоченную $p$-шку в её циклический сдвиг 
$$k,(g_0,\dots,g_{p-1})\to (g_k,g_{k+1},\dots,g_{p-1},g_0,\dots,g_{k-1}).$$
Заметим, что множество $$Y=\{(g_0,\dots,g_{p-1})\,|\, g_0\dots g_{p-1}=e\}$$
является инвариантным относительно действия $\mb Z/p$. Действительно, если $g_0\dots g_{p-1}=e$, то $g_1\dots g_{p-1}=g_0^{-1}$ и, следовательно $g_1\dots g_{p-1} g_0=e$. Это значит, что сдвиг на 1 переводит $Y$ в себя, а значит так поступают и все остальные сдвиги.

Если $(g_1,\dots,g_p)\in Y$ неподвижная точка относительно этого действия, то $g_1=\dots=g_p=g$, где $g^p=e$. То есть либо $g$ элемент порядка $p$ либо $g=e$. Осталось показать, что есть неподвижная точка не соответствующая $g=e$.

Действительно, заметим, что все орбиты относительно $\mb Z/p$ действия состоят либо из $1$ элемента, либо из $p$ элементов. Всего в $Y$ ровно $|G|^{p-1}\di p$ элементов. Значит число неподвижных точек делится на $p$. В частности, их либо нет, либо их больше $2$. Но уже есть одна неподвижная точка при $g=e$. Значит есть ещё одна. Значит есть  элемент порядка $p$.
\endproof


\section{Алгоритм Шрайера-Симса}

Где может быть использовано вычисление орбиты? Представим себе, что наша подгруппа $H \leq S_n$ задана образующими $H=\lan g_1,\dots,g_k\ran$. Хочется вычислить, например порядок $H$. Мы знаем, что порядок $|H|=|\Stab_1|\,|O_1|$. Как мы отметили, найти орбиту легко. Осталось найти порядок стабилизатора $1$ относительно действия $H$. $\Stab_1$ -- это подгруппа в $S_{n-1}$ так как она никуда не перемещает $1$. Тут бы и сказать, что можно воспользоваться индукционными соображениями, но есть одна загвоздка -- мы не знаем образующие стабилизатора.

Нахождение образующих стабилизатора не совсем простая вещь: мы знаем, что если $H=\lan (12),\dots,(1n)\ran$, то $H=S_n$ и $\Stab_1=S_{n-1}$ (действующая на точках от $2$ до $n$). Но! Ни одна из образующих $H$ не лежит в стабилизаторе! Однако все эти сложности можно преодолеть. Сформулируем соответствующий результат  в общем контексте действия группы на множестве.

\thrm[Лемма Шрайера] Пусть группа $G=\lan S\ran$ действует на множестве $X$.  Пусть дан $x\in X$ и для каждого элемента $y\in O_x$ задан $h_y \in G$, что $h_y x=y$. Также потребуем, чтобы для $y=x$ было $h_x=e$. Тогда 
$$\Stab_x =\lan h_{(sy)}^{-1}s h_y\ran \text{ по  всем $y\in O_x$ и $s\in S$.} $$
\ethrm
\proof Прежде всего отметим, что $h_{(sy)}^{-1}s h_y$ лежат в стабилизаторе $x$. Далее, в указанных предположениях любой элемент $g\in G$ есть произведение образующих $g=s_k^{\eps_k}\dots s_1^{\eps_1}$. Пусть $g\in \Stab_x$, то есть $gx=x$. Предположим, что все $\eps_i =1$ (так можно считать, если группа конечная, потом мы разберёмся с этим случаем). Посмотрим куда переходит элемент $x$ под действием $s_1$ и далее. Нарисуем диаграмму

\begin{center}
\begin{tikzpicture}
\node (a1) at (-2,0) {$x$};
\node (a2) at (1,0) {$x_1$};
\node (a3) at (4,0) {$x_2$};
\node (a4) at (7,0) {$x_{k-1}$};
\node (a5) at (10,0) {$x=x_k$};


\node (b2) at (1,-2) {$x$};
\node (b3) at (4,-2) {$x$};
\node (b4) at (7,-2) {$x$};


\draw[->, cyan ] (a1) -- node[above]{\color{black} $s_1$} (a2);
\draw[->, cyan ] (a2) -- node[above]{\color{black} $s_2$} (a3);
\draw[->, cyan ] (a4) -- node[above]{\color{black} $s_k$} (a5);
\draw[->, cyan ] (a3) -- node[above]{\color{black} $\dots$} (a4);

\draw[->, cyan] (a2) to[bend right] (b2);
\node (c1) at ($0.5*(a2)+0.5*(b2)-(0.7,0)$) {\color{black} $h^{-1}_{x_1}$};

\draw[->, cyan] (a3) to[bend right] (b3);
\node (c1) at ($0.5*(a3)+0.5*(b3)-(0.7,0)$) {\color{black} $h^{-1}_{x_2}$};

\draw[->, cyan] (a4) to[bend right] (b4);
\node (c1) at ($0.5*(a4)+0.5*(b4)-(0.8,0)$) {\color{black} $h^{-1}_{x_{k-1}}$};



\draw[<-, cyan] (a2) to[bend left] (b2);
\node (c1) at ($0.5*(a2)+0.5*(b2)+(0.7,0)$) {\color{black} $h_{x_1}$};

\draw[<-, cyan] (a3) to[bend left] (b3);
\node (c1) at ($0.5*(a3)+0.5*(b3)+(0.7,0)$) {\color{black} $h_{x_2}$};

\draw[<-, cyan] (a4) to[bend left] (b4);
\node (c1) at ($0.5*(a4)+0.5*(b4)+(0.8,0)$) {\color{black} $h_{x_{k-1}}$};

\draw[->,cyan] (a1) to[bend left] (a5);
\node (dd) at ($0.5*(a1)+0.5*(a5)+(0,1)$) {$g$} ;


\draw[->,cyan] (b2) to[bend left] (b3);
\node (dd1) at ($0.5*(b2)+0.5*(b3)$) {$h_{x_2}^{-1}s_2 h_{x_1}$};


\draw[->,cyan] (b3) to[bend left] (b4);

\end{tikzpicture}
\end{center}
Здесь $x_i=s_i x_{i-1}$. Произведение $h_{x_i}^{-1}s_{i-1}h_{x_{i-1}}$ является одной из образующих (если взять $y=x_{i-1}$). Видно, что $$g=s_kh_{x_{k-1}}h_{x_{k-1}}^{-1}\dots h_{x_2} (h_{x_2}^{-1}s_2 h_{x_1})h_{x_1}^{-1}s_1.$$
Осталось заметить, что $h_{x_1}^{-1}s_1=h_{x_1}^{-1}s_1h_x$ и $s_k h_{k-1}=h_x^{-1}s_k h_{x_{k-1}}$ благодаря тому, что $h_x=e$. Значит все элементы из $\Stab_x$ разложились в виде произведения потенциальных образующих. 

Если же где в последовательности встретился $\eps^{i}=-1$, то в качестве указанного тройного произведения возникнет $h^{-1}_{s_i^{-1}y} s_i^{-1} h_y$. Осталось заметить, что этот элемент обратный к образующей $h^{-1}_{s_iz}s_i h_z$, где $z=s_i^{-1}y$.

\endproof



Лемма Шрайера даёт возможность посчитать практически всё про группу $H \leq S_n$, начиная с порядка. Однако напрямую применять её не стоит. Дело в том, что если изначально в группе было $k$ образующих, то лемма выдаст в качестве образующих стабилизатора вплоть до $nk$ элементов. Если ничего не менять, то продолжая пользоваться леммой дальше,  получим, что множество образующих может очень разрастись. 

Покажем, однако, что в группе $G\leq S_n$ всегда порождена небольшим множеством.


\thrm Пусть $G\leq S_n$. Тогда существует существует набор образующих из не более чем $\frac{n(n-1)}{2}$ элементов.
\proof  При $n=2$ это очевидно. Переход. Посмотрим на $\Stab_{1}^G \leq S_{n-1}$. По индукционному предположению у этой подгруппы есть множество из $(n-1)(n-2)/2$ образующих. Достроим его до образующих $g$. Для этого для каждого $y\in O_{1},\, y\neq 1$ рассмотрим элемент $h_y\in G$, что $h_y(1)=y$. Таких $h_y$ не более $n-1$ штук.

Пусть теперь $\sigma \in G$. Если $\sigma \in \Stab_{1}$, то он выражается через образующие стабилизатора. Иначе рассмотрим произведение $h_{\sigma(1)}^{-1}\sigma$. Оно лежит в $\Stab_{1}$. Но тогда $\sigma$ выражается через образующие $\Stab_{1}$ и элементы вида $h_y$. В сумме их не более чем 
$$n-1+\frac{(n-1)(n-2)}{2}=\frac{n(n-1)}{2}.$$
\endproof
\ethrm

В указанной конструкции, мы получили набор образующих, среди которых есть образующие некоторого элемента (1 в данной конструкции), пары элементов и т.д. Однако, напрямую воспользоваться соображениями из теоремы нельзя, так как мы не знаем, как задать стабилизатор. Идея состоит в том, чтобы скрестить оба подходы -- беря постепенно образующие из леммы Шрайера для стабилизатора одной точки мы будем постепенно строить образующие для стабилизаторов некоторого набора точек, что  в итоге даст нам небольшой и удобный набор образующих.


\dfn[База] Пусть группа $G$ действует на множестве $X$. Назовём набор элементов $(b_1,\dots,b_k)$ из $X$ базой, если для любого $ g\in G$, если $gb_i=b_i$ для всех $i$, то $g=e$.
\edfn

Таким образом, действие элемента группы $G$ определяется его действием на базе. Не для любых действий группы на множестве вообще может существовать база: дело в том, что вообще говоря, элемент $g\in G$ не обязан определяться тем, как он действует на $X$, то есть отображением $T_g\in S_X$. Иными словами, гомоморфизм $G\to S_X$ в этом случае не инъективен. 

Всюду далее мы будем предпологать, что база есть. В этом случае указанный гомоморфизм инъективен, и можно считать, что $G$ подгруппа в $S_X$. То есть, если $X$ конечна, то $G$ можно считать подгруппой в $S_n$. 

Следующая конструкция позволяет удобно хранить хранить необходимую информацию об орбите конкретного элемента относительно действия данной группы и получать элементы $h_y$ из леммы Шрайера


\dfn[Дерево Шрайера] Пусть $G$ группа с конечным множеством образующих $S$ действует на множестве $X$. Деревом Шрайера для элемента $x\in X$ относительно множества образующих $S$ называется дерево (с корнем, направление рёбер -- к корню), вершины которого соответствуют элементам орбиты $O_x$ (сам $x$ -- корень), а на рёбрах стоят пометки из элементов $S$ таким образом, что есть ребро из $u$ в $v$ с меткой $s$, если $su=v$.
\edfn

Если дана система образующих $S$ для подгруппы $H$ в $S_n$ и некоторый элемент $x\in \{1,\dots,n\}$, то легко посчитать соответствующее дерево Шрайера для $x$ относительно $S$. Для этого надо, стартуя с элемента $x$, последовательно применять образующие из $S$. На каждом шаге будут появляться элементы орбиты $x$. продолжать это нужно до тех пор, пока новые подстановки не приведут к  полученным  ранее элементам орбиты $x$ и перебраны все ранее полученные элементы орбиты и к ним применены все образующие.

\dfn[Полная цепочка стабилизаторов] Пусть $G$ действует на множестве $X$, дана база $B=(b_1,\dots,b_k)$. Полной цепочкой стабилизаторов в $G$ относительно базы $B$ будем называть цепочку подгрупп 
$$G=G_0\geq G_1 \geq \dots \geq G_k=\{e\},$$
обладающую следующими свойствами:
\enm
\item имеет место равенство $G_{i+1}=\Stab_{b_{i+1}}^{G_i}$, при $0\leq i\leq k-1$.
\item группы $G_i$ заданы при помощи множества образующих $S_i$.
\item Для каждого $i\geq 0$ задано $T_i$ -- дерево Шрайера для $b_{i+1}$ относительно $S_i$.
\item Мы будем дополнительно предполагать невырожденность, то есть что $G_i\neq G_{i+1}$.
\eenm
\edfn

Группы $G_i$ в определении полной цепочки стабилизаторов можно определить как
$$G_{i+1} = \Stab_{(b_1,\dots,b_i)}^G.$$
Следуя доказательству леммы про небольшое число образующих, можно заметить, что данные, записанные в полной цепочке стабилизаторов дают возможность построить небольшую систему образующих для $G$.

\rm Для каждого $i$ возьмём подмножество тех элементов из $S_i$, которые встречаются в $T_i$. Объединим все эти множества. Получится множество образующих $G$. В нём не более чем $n(n-1)/2$ элементов (так как рёбер во всех деревьях $T_i$ ровно столько). 
\erm

Это множество образующих обладает ещё одним замечательным свойством:

\dfn[Сильное порождающее множество] Пусть группа $G$ действует на множестве $X$ и задана база $B=(b_1,\dots,b_k)$. Тогда множество $S$ называется сильным порождающим множеством относительно $B$, если 
$$S\cap G_i \text{ -- даёт образующие } G_i.$$ 
\edfn

Полная цепочка стабилизаторов (или просто сильное порождающее множество) позволяют легко решить задачу принадлежности элемента группы. Действительно -- пусть $G$ подгруппа $S_n$ и задана перестановка $\sigma$. Опишем процедуру проверки принадлежности $\sigma$ группе $G$, если задана полная система стабилизаторов для $G$.  Для индукционных рассуждений заметим, что из полной системы стабилизаторов для $G$ можно получить полную систему для всех $G_i$ просто отбросив начальный кусок. Будем сводить проверку $\sigma \in G_i$ к проверке $\sigma'\in G_{i+1}$ для некоторого элемента $\sigma_{new}$:
\enm
\item Рассмотрим натуральное число $i$ -- номер шага. Положим вначале $i=0$. Будем предполагать, что на $i$ шаге $\sigma$ оставляет на месте элементы $b_1,\dots,b_{i-1}$. Значит $\sigma \in G$ тогда и только тогда, когда $\sigma \in G_i$.
\item Рассмотрим $u=\sigma(b_i)$. Если $u\notin O_{b_i}^{G_i}$, то $\sigma \notin G_i$ и значит не из $G$.
\item Иначе, если $u\in O_{b_i}^{G_i}$, то по дереву Шрайера можно найти элемент $h_u\in G$, что $h_u u=b_i$. Тогда $\sigma \in G_i$ в том и только том случае, когда $\sigma'=h_u\sigma \in G_{i+1}$.
\item При $i=k$ проверка тривиальна.
\eenm

Посмотрим теперь, какие данные нам нужно будет поддерживать в процессе работы алгоритма по нахождению полной цепочки стабилизаторов:

\dfn[Частичная цепочка стабилизаторов]
Пусть $G$ действует на множестве $X$, набор точек $B=(b_1,\dots,b_k)$. Частичной цепочкой стабилизаторов в $G$ относительно набора точек $B$ будем называть цепочку подгрупп 
$$G=G_0\geq G_1 \geq \dots \geq G_k=\{e\},$$
обладающую следующими свойствами:
\enm
\item имеет место включение $G_{i+1}\leq \Stab_{b_{i+1}}^{G_i}$, при $0\leq i\leq k-1$.
\item группы $G_i$ заданы при помощи множества образующих $S_i$.
\item для каждого $i\geq 0$ задано $T_i$ -- дерево Шрайера для $b_{i+1}$ относительно $S_i$.
\eenm
\edfn

Как проверить, что частичная цепочка стабилизаторов -- полная? Для этого при каждом $0\leq i\leq k-1$ надо проверить, что образующие Шрайера для $G_{i+1}$ относительно $S_i$ лежат в $G_i$. Если цепочка полная, то все эти проверки выполнимы. На этом основана работа алгоритма Шрайера-Симса: на каждом этапе мы стараемся проверить, что некоторая образующая Шрайера лежит в соответствующей подгруппе. Мы либо выполняем эту проверку и переходим к следующей, либо добавляем недостающий элемент в базу, либо уменьшаем индекс $G_{i+1}$ в $\Stab_{b_{i+1}}^{G_i}$ для некоторого $i$. 

Итак, пусть $G$ задана при помощи семейства образующих $S$. Будем считать, что $X=\{1,\dots,n\}$. На самом деле нам нужен только порядок на $X$. Будем говорить, что частичная цепочка стабилизаторов корректна после уровня $i$, если $G_{t+1}=\Stab_{b_{t+1}}^{G_t}$ при $t> i$. Посмотрим на алгоритм:

\enm
\item стартовые данные: $G=G_0 \geq G_1=\{e\}$, $b_1=1$, $B=(b_1)$, $G_0=\lan S\ran$, дерево $T_1$ можно вычислить по $S$  и $b_1$. Цепочка корректна после уровня $i=1$. Сгенерируем по дереву $T_1$ образующие Шрайера для $\Stab_{b_1}$. Возьмём первую образующую.
\item Пусть дана частичная цепочка стабилизаторов $G_0\geq \dots\geq G_k$, для  $B=(b_1,\dots,b_k)$ корректная после уровня $i$ и дана некоторая образующая Шрайера $h\in G_i$ для $\Stab_{b_{i+1}}^{G_i}$. Запустим проверку принадлежности $G_{i+1}$ для $h$. 
\item Если $h\in G_{i+1}$, то его можно не включать в систему образующих для $G_{i+1}$ и надо перейти к следующей образующей Шрайера. Если следующей образующей нет, то надо сказать, что цепочка корректна после уровня $i-1$ и взять образующую Шрайера с уровня $i-1$ (или с большего уровня, если выкинет выше).
\item Если $h\notin G_{i+1}$, то алгоритм проверки может выкинуть какой-то номер $t$, что некий элемент $h'$ из стабилизатора $\Stab_{b_1,\dots,b_{t-1}}^G$, полученный по $h$ и уже известным образующим переводит элемент $b_t$ в элемент не из дерева $T_{t-1}$. В этом случае надо пересчитать дерево $T_{t-1}$, добавить $h'$ к образующим $G_{t-1},\dots,G_{i+1}$, сказать, что цепочка корректна после уровня $t-1$ и начать проверку образующей Шрайера для $\Stab_{b_t}^{G_{t-1}}$. 
\item Но бывает так, что $u=h'(b_k)\neq b_k$. Тогда надо взять новый элемент базы $b_{k+1}=u$, добавить $h'$ к образующим $G_k,\dots,G_{i+1}$, пересчитать дерево $T_k$, и взять $G_{k+1}=\{e\}$. Цепочка будет корректна после уровня $k$.
\eenm


Подробнее смотри в \cite{PGA}. Кроме проверки элемента полная цепочка стабилизаторов позволяет решить задачу о нахождении порядка группы $G$:


\crl[Вычисление порядка] Пусть дана полная цепочка стабилизаторов: $G=G_0\geq G_1\geq \dots \geq G_k=\{e\}$. Тогда $|G|=\prod_{i=0}^{k-1} |T_i|$. 
\ecrl




\section{Лемма Бернсайда}

Довольно часто в том или ином контексте возникает задача о подсчёте комбинаторных объектов. Базовый пример здесь число $C_{n}^k$, то есть число $k$ элементных подмножеств. Для того, чтобы задать $k$ элементное множество нужно выписать подряд $k$ различных элементов. Однако несколько таких записей часто задают один и тот же объект. Например, разные упорядоченные наборы $(1,2,3)=(2,1,3)=\dots$ задают одно и тоже множество. Какие записи мы отождествляем? Те, в которых элементы отличаются перестановкой, то есть лежат в орбите действия симметрической группы $S_k$. Всего упорядоченных $k$ элементных наборов $n(n-1)\dots(n-k+1)$. Орбита каждого набора состоит из $k!$ наборов -- при каждой перестановке получается новый набор. Итого число подмножеств есть отношение $C_n^k=\frac{n(n-1)\dots(n-k+1)}{k!}$, что вам хорошо известно. 



Приведём ещё одну похожую постановку задачи. Представим себе разноцветные бусы из $k$ бусин, которые могут быть раскрашены в $n$ возможных цветов. Совершенно ясно, что бусы задаются последовательностью бусин. Но две разные последовательности могут соответствовать одним бусам. Это можно понять следующим образом -- расставим бусины в вершины правильного $k$-угольника. Тогда любое самосовмещение правильного $k$-угольника меняет расположение бусин, но при этом сами бусы не меняются. Итого,  бусы однозначно соответствуют раскраскам вершин правильного $k$-угольника с точностью до самосовмещений этого $k$-угольника, то есть орбитам действия группы $D_k$ на множестве раскрасок вершин этого $k$-угольника в  $n$ цветов.

Эта задача сложнее предыдущей -- в задаче про $k$-элементные подмножества в каждой орбите было одинаковое число элементов и это мгновенно приводило к ответу. 

Здесь же более симметричные раскраски имеют меньшую орбиту. Попробуем проконтролировать это. Прежде всего введём обозначения.

\dfn Пусть группа $G$ действует на множестве $X$. Множество всех орбит относительно этого действия будем обозначать $X/G$.
\edfn 

\dfn Пусть так же задан элемент $g\in G$. Тогда обозначим за $\Fix(g)$ -- множество неподвижных точек элемента $g$ то есть 
$$\Fix(g)=\{x \in X\,|\, gx=x\}.$$
\edfn

Теперь мы можем разобрать основной технологический момент, который упрощает нахождение числа орбит:

\thrm[Лемма Бернсайда]
Пусть конечная группа $G$ действует на конечном множестве $X$. Тогда справедливо равенство
$$|X/G|=\frac{1}{|G|}\sum_{g\in G}|\Fix(g)|.$$
\ethrm
\proof Докажем нужное равенство подсчитав двумя способами число элементов в множестве $$\{(x,g) \,|\, x\in X,\, g\in G \text{ и } gx=x\}.$$
При каждом конкретном $g\in G$, число элементов $x$, образующих с ним пару равно $|\Fix(g)|$. C другой стороны имеем, что для каждого $x\in X$ число элементов в паре с ним равно $|\Stab_x|$. Итого $$\sum_{g\in G}|\Fix(g)|= \sum_{x\in X} |\Stab_x|.$$
Теперь вспомним, что $|\Stab_x|=|G|/|O_x|$. После этого замечания разобьём сумму по отдельным орбитам. В каждой орбите величина $|O_x|$ одинакова. Итого 
$$\sum_{g\in G}|\Fix(g)|= \sum_{x\in X} |\Stab_x|= |G|\sum_{O\in X/G}\,\, \sum_{x\in O} \frac{1}{|O|}=|G|\sum_{O\in X/G} 1=|G||X/G|.$$
Здесь мы заметили, что сумма $|O|$ раз величины $\frac{1}{|O|}$ равна единице. Для завершения доказательства осталось только поделить на $|G|$.
\endproof

Применим лемму Бернсайда для подсчёта числа различных бус. Для простоты ограничимся четырьмя бусинами, но не будем ограничивать число цветов. 

Напомню, что группа $D_n$ состоит из $2n$ элементов. $n$ из них -- это повороты на угол кратный $\frac{2\pi}{n}$. Ещё есть $n$ различных симметрий относительно прямых. В случае чётного $n$ половина из этих прямых проходит  через пары противоположенных вершин, а половина проходит через середины противоположенных рёбер. 

Итого в нашем случае имеем $|D_4|=8$. Переберём все элементы $D_4$. Для начала заметим, что все раскраски неподвижны относительно тождественного преобразования. Итого $|\Fix(\id)|=n^4$. Рассмотрим симметрию, проходящую через середины противоположенных сторон. Пары вершин на каждой из этих сторон должны быть раскрашены одинаково. Итого $n^2$ инвариантных раскрасок. Если же симметрии проходят через противоположенные вершины, то инвариантных раскрасок $n^3$. 

Теперь рассмотрим повороты. Инвариантность относительно поворота на 90  и 270 градусов означают, что все вершины должны быть покрашены одинаково. Итого на каждый по $n$ раскрасок. Поворот на 180 градусов даёт $n^2$ раскрасок. Итого
$$\frac{1}{8}(n^4+2n^2+2n^3+2n+n^2)=\frac{1}{8}(n^4+2n^3+3n^2+2n)$$
различных бус из 4 бусин для $n$ возможных цветов.


\chapter{Линейные операторы}



\section{Определитель}

Есть ли какая-нибудь численная характеристика, которая позволяет сказать, что матрица $A\in M_n(K)$ обратима? Для этого вспомним, что обратимость матрицы $A$ равносильна линейной независимости её столбцов. Если эти столбцы представить себе как вектора в пространстве $\mb R^n$, например, при $n=3$ в обычном трёхмерном пространстве, то видно, что вектора линейно независимы тогда и только тогда, когда объём параллелепипеда на них натянутого отличен от нуля. Конечно, в случае пространства, размерности больше трёх понятие параллелепипеда нужно уточнить:

\dfn Пусть $V$ -- векторное пространство размерности $n$ над $\mb R$, тогда для набора  $v_1,\dots,v_n \in V$ определим параллелепипед
$$D(v_1,\dots,v_n)=\left\{\sum_{i=1}^n \lambda_i v_i\,|\, \text{ где } \lambda_i\in [0,1]\right\}.$$
\edfn


Обозначим для краткости $\Vol(v_1,\dots,v_n)= \Vol (D(v_1,\dots,v_n))$. 

Попробуем понять есть ли возможность как-то аксиоматизировать понятие объёма параллелепипеда на пространстве $V=\mb R^n$, так, чтобы его можно было перенести на произвольное пространство над каким угодно полем. Мне будет удобно набор из $n$  столбцов из $\mb R^n$ объединять в матрицу. Итак, отображение объёма $\Vol M_n(\mb R) \to \mb R$ должно удовлетворять следующим свойствам:
\enm 
\setcounter{enumi}{-1}
\item Объём единичного кубика, единичен. То есть $\Vol(E_n)=1$.
\item При растяжении одного вектора объём меняется пропорционально $\Vol(\dots,\lambda v,\dots)=|\lambda|\Vol(\dots,v,\dots)$.
\item Исходя из принципа Кавальери $\Vol(\dots,v,\dots,u,\dots)=\Vol(\dots,v,\dots,u+\lambda v,\dots)$.
\item Если в наборе есть два одинаковых вектора, то $\Vol(\dots,v, \dots, v,\dots)=0$.
\eenm

Перейдём теперь к ситуации над произвольным полем $K$.
Для общего пространства $V$ условие типа 0) не имеет смысла, так как нет возможности выбрать какой-то канонический базис в $V$. Безусловно, модуль числа в свойстве 1) нет возможности определить над произвольным полем так как  возникают сложности с понятием положительности (особенно в конечных полях). Таким образом, заменой свойства 1) над произвольным полем стоит считать
\enm
\item[1')] $\omega(\dots,\lambda v,\dots)=\lambda \omega(\dots,v,\dots)$.
\eenm
Заметим, что если мы находимся над $\mb R$ и  для отображения $\omega \colon V \times \dots \times V \to \mb R$ выполнено свойство  1'), то для отображения $|\omega|$ выполнены свойства 1). 

Наконец, с алгебраической точки зрения свойство 2) означает независимость относительно элементарных преобразований столбцов матрицы. Это не самое удобное условие. Вместо него мы рассмотрим, как кажется, более сильное свойство
\enm \item[2')] $\omega(\dots,u+v,\dots)=\omega(\dots,u,\dots)+\omega(\dots,v,\dots)$.
\eenm
Действительно из свойства 1'), 2') и 3) следует свойство 2): $$\Vol(\dots,v,\dots,u+\lambda v,\dots)= \Vol(\dots,v,\dots,u,\dots)+\lambda \Vol(\dots,v,\dots,v,\dots)=\Vol(\dots,v,\dots,u,\dots).$$
Свойство 3) имеет смысл всегда и менять его, видимо не следует. Давайте немного покрутимся около этих свойств и посмотрим, что нам даёт их наличие.


\dfn[Общее определение полилинейности] Пусть $U_1,\dots,U_l, V$ -- векторные пространства над полем $K$. Отображение $\omega \colon  U_1\times \dots \times U_l\to V $ называется полилинейным, если
$$\omega(v_1,\dots,v_i+\lambda u_i,\dots, v_l)= \omega(v_1,\dots,v_i,\dots, v_l)+\lambda\omega(v_1,\dots,u_i,\dots, v_l).$$
Множество всех полилинейных отображений будем обозначать как $\Hom_K(U_1,\dots,U_l;V)$. Здесь одновременно зашифрованы свойства типа 1')  и  2').
\edfn


\dfn[Форма]
Полилинейное отображение $\omega \colon  V^l\to K $ называется полилинейной формой степени $l$ на $V$.
\edfn

\rm Вообще, формой принято называть любое отображение из векторного пространства в базовое поле.
\erm


\dfn
Полилинейная форма  $\omega \colon V^l\to K$ на пространстве $V$ над полем $K$ называется:
\enm 
\item антисимметричной или кососимметричной, если $\omega(v_1,\dots,v,\dots,v,\dots, v_l)=0$.
\item симметричной, если $\omega(v_1,\dots,v_i,\dots,v_j,\dots, v_l)=\omega(v_1,\dots,v_j,\dots,v_i,\dots, v_l)$
\eenm
\edfn



Теперь стоит  отметить основные свойства, характерные для полилинейных форм. 

\lm Пусть $V$ -- пространство размерности $n$. Для  полилинейного отображения $\omega \colon V^l \to K $ и любого $e_1,\dots,e_n$ -- базиса $V$  выполнено, что
$$\omega(v_1,\dots,v_l)=\sum_{1\leq i_1,\dots,i_l\leq n}\omega(e_{i_1},\dots,e_{i_l})\prod_{j=1}^l a_{i_j,j}, \text{ где $a_{ij}$ -- это $i$-ая координата вектора $v_j$ в базисе $e$.}$$ 
\elm
\proof По условию $v_j=\sum_{i=1}^n a_{ij}e_i$. Тогда $$\omega(v_1,\dots,v_l)=\sum_{i_1=1}^n a_{i_1,1}\omega(e_{i_1},v_2,\dots,v_l)= \dots = \sum_{1\leq i_1,\dots,i_l\leq n}\omega(e_{i_1},\dots,e_{i_l})\prod_{j=1}^l a_{i_j,j},$$
\endproof

Предыдущую лемму можно доказать и в более общей форме -- для произвольных полилинейных отображений. Но сейчас нам это не нужно. Посмотрим, какие дополнительные ограничения накладывает условие кососимметричности. 


\lm Пусть $V$ -- пространство размерности $n$. Для  полилинейного отображения $\omega \colon V^l \to K $ выполнено: \\
1) Если $\omega$ кососимметрично, то $\omega(\dots,u,\dots,v,\dots)= -\omega(\dots,v,\dots,u,\dots)$.\\
2) В случае, если $\chr K \neq 2$, то из  заключения пункта 1), следует кососимметричность.\\
3) Если $\omega$ кососимметрична, то для любой перестановки $\sigma \in S_l$ выполнено, что $\omega(v_{\sigma(1)},\dots,v_{\sigma(l)})= \sgn(\sigma) \omega(v_1,\dots,v_n)$.\\
4) В предположении кососимметричности $\omega(\dots,v,\dots,u,\dots)=\omega(\dots,v,\dots,u+\lambda v,\dots)$.\\
5) В предположении кососимметричности и условия $l=n$ для набора векторов $v_1,\dots,v_n$ и базиса $e_1,\dots,e_n$ выполнено
$$\omega(v_1,\dots,v_n)=\omega(e_1,\dots,e_n)\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{j=1}^n a_{\sigma(j),j}=\omega(e_1,\dots,e_n)\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{i=1}^n a_{i,\sigma(i)},$$
где $a_{i,j}$ -- это $i$-ая координата $j$-ого вектора в базисе $e_1,\dots, e_n$.
\proof Докажем первое утверждение. Распишем 
\begin{align*} 
0&=\omega(\dots,u+v,\dots,u+v,\dots)=\\&=\omega(\dots,u,\dots,u,\dots)+\omega(\dots,v,\dots,u,\dots)+\omega(\dots,u,\dots,v,\dots)+\omega(\dots,v,\dots,v,\dots)=\\ &=\omega(\dots,v,\dots,u,\dots)+\omega(\dots,u,\dots,v,\dots),
\end{align*}
что и доказывает утверждение. Для доказательства второго утверждения предположим, что $\chr K\neq 2$. Тогда, переставляя одинаковые вектора в выражении $\omega(\dots,v,\dots,v,\dots)$ получаем
$$\omega(\dots,v,\dots,v,\dots)=-\omega(\dots,v,\dots,v,\dots).$$
То есть $2\omega(\dots,v,\dots,v,\dots)=0$. Осталось поделить на 2. покажем третий пункт. По определению знака перестановки
$$\sgn(\sigma)=(-1)^{k}, \text{ где $k$ -- число транспозиций в разложении $\sigma$}.$$
Откуда получаем, что применить $\sigma$ это тоже самое, что применить $k$ транспозиций, то есть изменить знак $k$ раз, что и требовалось. Докажем пункт 4)
$$\omega(\dots,v,\dots,u+\lambda v,\dots)=\omega(\dots,v,\dots,u,\dots)+\lambda\omega(\dots,v,\dots, v,\dots)=\omega(\dots,v,\dots,u,\dots).$$
Для доказательства свойства 5) воспользуемся общим выражением для полилинейной формы из предыдущей леммы
$$\omega(v_1,\dots,v_n)=\sum_{i_1,\dots,i_n} \omega(e_{i_1},\dots,e_{i_n}) \prod_{j=1}^n a_{i_j,j}.$$
Если два индекса совпадают, то $\omega(e_{i_1},\dots,e_{i_n})=0$, а вместе с ним и всё слагаемое. Остаются только наборы с разными $i_{\alpha}$, которые однозначно задают перестановку $\sigma(k)=i_k$. Теперь заметим, что $\omega(e_{\sigma(1)},\dots,e_{\sigma(n)})=\sgn(\sigma)\omega(e_1,\dots,e_n)$, что доказывает первое равенство. Теперь
$$\prod_{j=1}^n a_{\sigma(j),j}=\prod_{i=1}^n a_{i,\sigma^{-1}(i)}.$$
Если вместо $\sigma$ поставить сумму по $\sigma^{-1}$, то с одной стороны сумма не поменяется, а с другой стороны по тождеству выше, можно будет перекинуть $\sigma$ на другой индекс.
\endproof
\elm







\begin{defn}
Пусть $n=\dim V$. Антисимметричная полилинейная форма $\omega \colon V^n \to K $ называется формой объёма на $V$. Если такая форма не равна 0, то будем говорить, что она невырождена.
\end{defn}



До этого момента мы говорили про объекты которых, возможно, просто не существует. Настала пора предъявить для них конструкцию и доказать её единственность. Для этого достаточно заметить, что все формы объёма, если их расписать в координатах пропорциональны фиксированной функции от координат векторов. Дадим название этой функции:

\dfn  Определителем $\det$ называется отображение $\det \colon M_n(K) \to K$, такое, что $$\det(A)=\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{1\leq i\leq n} a_{i\sigma(i)}.$$
\edfn

Используя понятие определителя можно попытаться определить форму объёма, связанную с фиксированным базисом. Значение получившегося отображения на  самом исходном базисе (с учётом порядка) будет равно единице.

\dfn Пусть $e_1,\dots,e_n$ базис пространства $V$. Определим отображение $\Vol_e \colon V^n \to K$, такое, что
$$\Vol_e(v_1,\dots,v_n)=\det(e(v_1),\dots, e(v_n)),$$
где $e\colon V \to K^n$ -- отображение сопоставления координат.
\edfn

\begin{thm} Верны следующие свойства:\\
1) Определитель является формой объёма на $K^n$, такой, что $\det E=1$.\\
2) Если $V$ -- пространство размерности $n$, то любая форма объёма на $V$ имеет вид $$\omega=\omega(e_1,\dots,e_n)\Vol_e.$$
В частности, если есть два базиса $e$ и $f$, то $\Vol_{f}=\det(C_{f \to e}) \Vol_{e}$.\\
3) Пространство форм объёма одномерно.\\
4) Для любой невырожденной формы объёма $\omega$ верно утверждение: $\omega(v_1,\dots,v_n)=0$ тогда и только тогда, когда $v_1,\dots,v_n$ линейно зависимы.
\proof Определитель является полилинейной функцией столбцов, так как  каждое слагаемое содержит ровно одну координату в первой степени из каждого столбца матрицы $A$. Осталось показать, что если в матрице $A$ пара столбцов одинакова, то её определитель равен нулю.
Пусть равны столбцы $k$ и $l$. Рассмотрим транспозицию $\tau=(kl)$. Тогда все перестановки разбиваются на классы $A_n$ и $A_n\tau$. Выпишем теперь сумму
$$\sum_{\sigma \in S_n} \sgn(\sigma)\prod_{j=1}^n a_{\sigma(j)j} = \sum_{\sigma \in A_n} \prod_{j=1}^n a_{\sigma(j)j}- \sum_{\sigma\tau \in A_n\tau}\,\,\, \prod_{j=1}^n a_{\sigma(\tau(j))j}.$$
Посмотрим на слагаемое в первой и во второй сумме, соответствующие одной и той же перестановке $\sigma \in A_n$. Заметим, что $a_{\sigma(\tau(j))j}=a_{\sigma(j)j}$, если $j\neq k,l$. Но при  $j=k$, благодаря тому что $\tau(k)=l$  и равенству столбцов матрицы $A$, имеем $a_{\sigma(\tau(k))k}=a_{\sigma(l)k}=a_{\sigma(l)l}$. Аналогично $a_{\sigma(\tau(l))l}=a_{\sigma(k)k}$, что означает, что произведения для одной и той же $\sigma$ в правой и левой части суммы одинаковы.\\

Покажем теперь пункт 2. Используя предыдущую лемму, получаем $\omega(v_1,\dots,v_n)=\omega(e_1,\dots,e_n)\Vol_e(v_1,\dots,v_n)$. То есть, форма $\omega$ пропорциональна форме $\Vol_e$ с коэффициентом $\omega(e_1,\dots,e_n)$. Для завершения доказательства осталось вычислить указанный коэффициент пропорциональности между $\Vol_e$ и $\Vol_f$ для базисов $e$ и $f$. Для этого надо вычислить $\Vol_f(e_1,\dots,e_n)$. То есть, надо построить матрицу из координат столбцов $e_i$ в базисе $f$ и посчитать её определитель. Но это матрица перехода $C_{f\to e}$. Итого $\Vol_{f}=\det(C_{f\to e}) \Vol_{e}$. Здесь мы получили, в частности, что любая форма объёма на пространстве $V$  пропорциональна форме $\Vol_e$, что доказывает пункт 3).


Пусть теперь $v_1,\dots,v_n$ набор векторов. Тогда, если $v_1,\dots,v_n$ линейно зависимы, то существует нетривиальная линейная комбинация $\sum \lambda_i v_i=0$. Пусть $\lambda_1\neq 0$, тогда можно считать, что $\lambda_1=1$. Тогда, прибавляя к первому аргументу остальные с соответствующими коэффициентами, получаем набор с нулевым первым вектором. но при таких преобразованиях значение формы объёма не должно меняться. Итого $\omega(v_1,\dots,v_n)=\omega(0,\dots)=0$.
Обратно, если $v_1,\dots,v_n$ независимы, то $\omega=\omega(v_1,\dots,v_n)\Vol_v$. Так как $\omega \neq 0$, то коэффициент пропорциональности $\omega(v_1,\dots,v_n)\neq 0$.
\endproof
\end{thm}


Как видно построение форм объёма сводится к построению определителя. Технически, мы всегда будем обращаться с определителями. Поэтому сформулируем важные свойства: 

\lm Для определителей квадратных матриц верны следующие свойства:\\
0) $\det(A)=\det(A^{T})$.\\
1) Определитель не меняется при элементарных преобразованиях первого типа для строк и столбцов. При смене строк местами меняется знак определителя. При домножении строки на $\lambda$, определитель домножается на $\lambda$.\\
2) $\det(AB)=\det(A)\det(B)$.\\
3) $\det \left(\begin{matrix}A & B\\
0 & C \\
\end{matrix}\right)= \det(A)\det(C)$.\\
4) Определитель верхнетреугольных или нижнетреугольных матрицы равен произведению диагональных элементов.\\
5) $\det (A^{-1})=(\det A)^{-1}.$\\
6) $\det \colon \GL(V) \to K^*$
является гомоморфизмом групп.
\proof Свойство 0) мы уже доказали. Из него сразу же следует свойство 1). Докажем 2).\\
Заметим, что форма $B \to \det AB$ линейна по столбцам $B$. Тогда это форма объёма и следовательно она пропорциональна форме $B \to\det B$. Для того, чтобы найти коэффициент пропорциональности, подставим $B=E_n$. Имеем $\det AE_n=\det A= c \det E_n=c$. Откуда для любого $B$ получаем
$$\det AB=\det A \det B.$$\\
Покажем 3). Сначала посчитаем $$\det \pmat E_n& B\\
0& E_m\epmat.$$
Заметим, что с помощью элементарных преобразований 1-го типа из неё можно сделать $E_{n+m}$. Тогда определитель равен 1.
Теперь заметим, что форма $$A \to \det \pmat A & B\\ 0& E_m\epmat $$ форма объёма на $K^n$. Как и раньше подставив $A=E_n$ получаем что коэффициент пропорциональности с $A \to \det A$ равен 1.\\
Теперь отображение $$C \to \det \pmat A& B\\ 0 & C \epmat $$  есть форма объёма, по строчкам $C$. Подставляя $B=E_m$ находим коэффициент пропорциональности $\det A$, что и даёт
$$\det \pmat A&B\\ 0&C \epmat = \det A \det C.$$
 Применяя предыдущее условие несколько раз приходим к пункту 4).\\
Беря равенство $AA^{-1}=E_n $ и считая определитель получаем $\det A \det A^{-1}=E_n$, что завершает доказательство 5). Пункт 6) следует из 2) и того факта, что определитель обратимой матрицы обязан быть обратимым.
\endproof
\elm



С вычислительной точки зрения формула для определителя бесполезна -- в ней $n!$ слагаемых. Но она даёт некоторые важные следствия. Например, то, что определитель есть однородный многочлен степени $n$ с целыми коэффициентами от элементов матрицы (следовательно, понятие определителя можно ввести над любым коммутативным кольцом по этой формуле).

Обычно, для вычисления определителя матрицу приводят элементарными преобразованиями к ступенчатому виду. При таких преобразованиях не сложно проконтролировать, как определитель меняется. В ступенчатом же виде определитель так же легко сосчитать. Посмотрим на конкретные примеры вычисления определителей.



\exm
\enm 
\item Определитель  $\det \left(\begin{smallmatrix} a&b \\ c& d\end{smallmatrix}\right)=ad-bc$.
\item Определитель матрицы $3\times 3$ тоже можно выписать. 
\item Приведём пример, вычислив определитель Вандермонда.
$$\det \pmat 1 & \dots & 1\\
\lambda_1 & \dots & \lambda_n\\
\vdots &&\vdots\\
\lambda_1^{n-1}& \dots & \lambda_n^{n-1} \epmat= \prod_{i>j}(\lambda_i-\lambda_j).$$
Мы не будем слепо следовать методу Гаусса, а используем элементарные преобразования в другом порядке. Сначала вычтем из $n$-ой строки $n-1$-ую с коэффициентом $\lambda_1$. Потом из $n-1$-ой $n-2$-ую и т.д. Получим:
$$\det \pmat 1 & 1& \dots & 1\\
0 & \lambda_2-\lambda_1 & \dots & \lambda_n-\lambda_1\\
\vdots& \vdots &&\vdots\\
0& \lambda_2^{n-2}(\lambda_2-\lambda_1)& \dots & \lambda_n^{n-2}(\lambda_n-\lambda_1) \epmat= \prod_{i>1}(\lambda_i-\lambda_1)\det \pmat 1 & \dots & 1\\
\lambda_2 & \dots & \lambda_n\\
\vdots &&\vdots\\
\lambda_2^{n-2}& \dots & \lambda_n^{n-2} \epmat.$$
\eenm

Мы стартовали с понятия объёма параллелепипеда в $\mb R^n$. Отображение $A \to |\det A|$ удовлетворяет всем предполагаемым свойствам объёма. Может ли тем не менее, определить дать другое определение объёма параллелепипеда, которое даёт отличные от определителя результаты? \\
Оказывается, что нет. И для этого не нужно требовать все свойства объёма. Достаточно знать, что модуль определителя и объём одинаково изменяются при элементарных преобразованиях. Точнее:

\utv Пусть дано отображение $\Volume \colon M_n(\mb R) \to \mb R $, обладающее некоторыми свойствами объёма параллелепипеда: \\
1) $\Volume(E_n)=1$\\
2) $\Volume(\dots,u+\lambda v,\dots,v,\dots)=\Volume(\dots,u,\dots,v,\dots)$\\
3) $\Volume(\dots,\lambda v,\dots)=|\lambda|\Volume(\dots,v,\dots)$\\
Тогда $\Volume(A)=|\det A|$.
\proof Покажем, что эти свойства однозначно позволяют вычислить $\Volume(A)$. Так как отображение $A\to |\det A|$ тоже им удовлетворяет, то это гарантирует их совпадение.  Действительно, если матрицы $A$ вырождена, то элементарными преобразованиями первого типа можно сделать так, чтобы один столбец $A$ стал нулевым. По третьему свойству $$\Volume(v_1,\dots,0,\dots,v_n)=\Volume(v_1,\dots,0\cdot 0,\dots,v_n)=0\cdot \Volume(v_1,\dots,0,\dots,v_n)=0.$$
Если же $A$ невырождена, то элементарными преобразованиями первого типа её можно привести к диагональному виду не меняя определителя. Теперь вынося по свойству 3) диагональные элементы приводим матрицу $A$ к единичному виду. Осталось воспользоваться свойством 1).
\endproof
\eutv


\subsection{Ориентация}

Настало время отдать долги за школьный курс геометрии и поговорить о важном понятии -- понятии ориентации пространства.  Вспомним, что понятие определителя выросло аксиоматизации объёма и немного его переросло. А именно, для набора элементов из пространства $\mb R^n$ можно посчитать не только объём параллелепипеда на них натянутого (заведомо неотрицательное число), но ещё и некоторый знак. О смысле этого знака и пойдёт сейчас речь.



\dfn Будем говорить, что два базиса пространства $V$ над $\mb R$ одинаково ориентированы, если матрица перехода между ними имеет положительный определитель.
\edfn

\rm Отношение одинаковой ориентированности есть отношение эквивалентности.
\erm

\dfn Выбор одного из классов эквивалентности базисов вещественного векторного пространства $V$ называется заданием ориентации.
\edfn

\exm
\enm
\item Рассмотрим базисы  плоскости $(0,1),(1,0)$ и $(1,0),(0,1)$.  Они имеют разную ориентацию. Ориентация на $\mb R^n$, заданная стандартным базисом называется стандартной. 
\item Вообще, если один базис плоскости получен из другого при помощи поворота плоскости, то они будут одинаково ориентированы.
\item Аналогично, смена порядка двух базисных векторов приводит к смене ориентации. Растяжение одного вектора на положительную константу  не меняет ориентации, а домножение на отрицательное число -- меняет ориентацию.
\eenm


\utv Пусть есть два базиса $e_1,\dots,e_n$ и $f_1,\dots,f_n$ в пространстве $V$ над $\mb R$. Если они имеют разную ориентацию, то их нельзя продеформировать один в другой (внутри пространства базисов).
\proof Определимся с тем, что доказываем. Прежде всего перейдём к координатам. Можно считать, то $V=\mb R^n$ и множество всех базисов -- это множество обратимых матриц. Надо показать, что не бывает непрерывного отображения $f\colon [0,1] \to \GL_n(\mb R)$, так что $f(0)$ и $f(1)$ по разному ориентированы. 

Действительно, посмотрим на $\det (f(t))$. Пусть при $t=0$ выполнено $\det f(0)>0$,  а при $t=1$ выполнено $\det f(1)<0$. Заметим, что $\det f(t)$ непрерывная функция $[0,1] \to \mb R$. Тогда между $0$ и $1$ у неё есть корень. Но если, $\det f(t)=0$, то $f(t)$ вырождена, чего не может быть, так как по условию $f(t)$ -- обратимая матрица для любого $t$.
\endproof
\eutv

\upr Покажите, наоборот, что любые два одинаково ориентированных базиса можно продеформировать друг в друга.
\eupr

\subsection{Линейные операторы и ориентация}

Особо ценным классом линейных отображений являются отображения пространства в само себя. Я буду называть такие отображения линейными операторами на $V$. Сразу отмечу, что в литературе под словом оператор часто подразумевают линейное отображение между произвольными двумя пространствами. В этой ситуации есть специальное слово, говорящее о том, что речь идёт об отображениях из пространства в себя  -- эндоморфизм пространства. Мы, тем не менее, будем использовать узкое значение слова оператор, имеющее тот же смысл, что и понятие эндоморфизм.

\dfn Пусть $V$ -- пространство. Тогда линейное отображение $L\colon V \to V$ называется (линейным) оператором  на пространстве $V$. Пусть $e_1,\dots, e_n$ базис $V$. Тогда матрицей оператора $L$ в базисе $e$ называется матрица $[L]_e^e$ (базисы с разных концов взяты одинаковыми).
\edfn

Это определение удобно, потому что при фиксированном базисе композиции операторов $L_1\circ L_2$ соответствует произведение их матриц $A_1A_2$ в базисе $e$. В частности, если $A$ матрица $L$ в базисе $e$, то оператору $L^n$ соответствует матрица $A^n$. Если бы базисы были выбраны не согласовано, это было бы неверно. 

\dfn Пусть $L\colon V \to V$ -- линейный оператор. Тогда определим $\det L=\det A$, где $A$ -- матрица оператора в каком-то базисе.
\edfn

Априори неясно, не зависит ли данное определение от выбора базиса. Однако сразу можно заметить, что линейный оператор $L$ обратим тогда и только тогда, когда $\det L \neq 0$. Это даёт уверенность в том, что понятие определителя имеет безкоординатный смысл и, значит, инвариантно, относительно замены базиса. Давайте это покажем:

\utv Определитель корректно определён.
\proof Корректность получается из соображения, что при смене базиса $e\to f$ матрица $A$ заменяется на $C_{f\to e}A C_{f \to e}^{-1}$. Считая определитель получаем
$$\det C_{f\to e}A C_{f \to e}^{-1}= \det C_{f\to e} \det A \det C_{f\to e}^{-1}= \det A.$$
\endproof
\eutv



\dfn Пусть $V$ -- векторное пространство над $\mb R$. Будем говорить, что линейный оператор $L\colon V \to V$ сохраняет ориентацию, если $\det L>0$ и не сохраняет, если $\det L<0$.
\edfn

\lm Сохраняющее ориентацию отображение переводит одинаково ориентированные базисы в одинаково ориентированные.
\proof Пусть $e$ -- базис. Если матрица $L$  в базисе $e$ это $A$, то $A$ -- это то же самое, что матрица перехода от $e$ к $Le$, что и доказывает утверждение. Уcловие сохранения ориентации говорит, что $\det A > 0$, а значит $e$ и $Le$ одинаково ориентированы.
\endproof
\elm

\exm\\
1) Симметрия относительно прямой в $\mb R^2$ меняет ориентацию.\\
2) Поворот сохраняет ориентацию.\\
3) Центральная симметрия в $\mb R^n$ (домножение на $-1$) меняет или нет ориентацию в зависимости  от чётности $n$.\\





Проинтерпретируем наши результаты с случае $\mb R^n$. Возьмём базис $v_1,\dots,v_n$ и пусть $L$ -- задан матрицей $A$. Пусть $B$ -- это матрица составленная из $v_1,\dots,v_n$. Мы знаем, что $\det AB= \det A \det B$. Это значит, что $\det L= \det A$ есть коэффициент изменения между объёмом параллелепипеда, натянутого на $v_i$ и объёма параллелепипеда, натянутого на $Lv_i$. То есть определитель оператора есть коэффициент изменения объёма любого параллелепипеда под действием $L$. Это причина того, что при многомерной замене координат в интеграле вылезает определитель Якобиана -- то есть линейного приближения к исходному отображению.

Стоит отметить, что это отношение не зависит и от выбора невырожденной формы объёма так как любые две формы объёма пропорциональны.


\dfn Определим группу операторов $\SL(V)=\{ L\colon V \to V \,|\, \det L=1\}$. Если $V$ -- вещественное векторное пространство, то это операторы, которые сохраняют понятие объёма и выбор ориентации пространства. $\SL_n(K)$ называется группа матриц с определителем единица.
\edfn







\section{Явные формулы в линейной алгебре}

Ещё одно применение определителя -- это возможность выписать явное решение для многих задач линейной алгебры. Посмотрим, как это проявляется. Для начала взглянем на сам определитель. Заметим, что так как определитель линеен по столбцам, то должно иметь место равенство $\det (\dots,v,\dots)= \sum c_i v_i$,  где $v_i$ -- координат столбца $v$, а $c_i$ -- некоторые на зависящие от столбца $v$ коэффициенты. Мы сейчас найдём эти коэффициенты $c_i$. Для того, чтобы сформулировать ответ нам понадобятся несколько определений.

\dfn Пусть $A \in M_{m\times n}(K)$. Пусть $I\subseteq \{1,\dots,m\}$, а $J\subseteq \{1,\dots n\}$. Подматрицей $A_{I,J}$ будем называть матрицу, составленную из элементов $A$, стоящих в строках с номерами из $I$ и в столбцах с номерами из $J$. Минором порядка $k$ матрицы $A$ называется определитель некоторой квадратной подматрицы $M_{I,J}=\det A_{I,J}$, где $|I|=|J|=k$. Если $A\in M_n(K)$, то алгебраическим дополнением элемента $a_{ij}$ называется $A^{ij}=(-1)^{i+j} M_{\ovl{i},\ovl{j}}$.
\edfn



\lm При разложении по $j$-ому столбцу имеет место формула  $$\det(A)=\sum_{i=1}^n a_{ij} A^{ij}.$$
\proof Прежде всего установим эту формулу в простейшем случае $$A=\pmat 1& *\\
0& A_{\ovl{1},\ovl{1}}\epmat. $$
Это следствие вычисления определителя блочной матрицы. Итак, пусть фиксирован столбец $j$. Как же теперь найти коэффициенты при $a_{ij}$ в разложении? Для этого надо в качестве $j$-го столбца поставить стандартный базисный столбец $e_i$ и посчитать определитель. Сделаем это. Напишем матрицу
$$B= \bordermatrix{
 & &       &  j &      & \cr
 & a_{11} && 0 &  & a_{n1}    \cr
 & \vdots& & \vdots& & \vdots\cr
 i &a_{i1}&\dots & 1 &\dots & a_{in} \cr
 & \vdots& & \vdots& & \vdots\cr
 & a_{1n} &  & 0 &      &a_{nn} } $$
и переставим по циклу столбцы, чтобы $j$-ый столбец стал первым, и $i$-ая строчка стала первой. Такое преобразование изменит знак определителя на $(-1)^{i-1+j-1}=(-1)^{i+j}$. А в правом нижнем блоке будет стоять нужная подматрица $A$.
\endproof
\elm
Это свойство называется разложением  определителя матрицы  по столбцу. Аналогичная формула верна и для разложения по строке.


В прошлом разделе мы научились по некоторой формуле определять обратима матрица или нет. Это полезно в различных задачах с параметром, где метод Гаусса напрямую не применим для вычисления определителя. Можно поставить новый вопрос: а можно ли написать формулу для решения системы линейных уравнений при некоторых ограничениях? Ответ на  этот вопрос дают формулы Крамера.

\thrm[Формула Крамера]
Пусть дана система линейных уравнений $Ax=b$ с квадратной матрицей $A$ над полем $K$. Если матрица $A$ --- обратима, то единственное решение этой системы имеет вид $$x_i=\frac{\Delta_i}{\Delta}, \text{ где } \Delta=\det A, \text{ а } \Delta_i \text{ --- определитель матрицы, полученной из  $A$ заменой $i$-го столбца на столбец $b$}.$$
\proof Пусть $x$ такой, что $Ax=b$. За $v_1,\dots,v_n$ обозначим столбцы $A$. Тогда $b=x_1v_1+\dots+x_nv_n$.  Теперь посчитаем определитель $\Delta_i$. Это определитель матрицы $(v_1,\dots,v_{i-1},b,v_{i+1},\dots,v_n)$. Подставим $b=x_1v_1+\dots+x_nv_n$ и раскроем скобки в опредеделителе, пользуясь его полилинейностью. Останется ровно одно слагаемое с $x_iv_i$. Итого
$$\Delta_i=\det (v_1,\dots,v_{i-1},b,v_{i+1},\dots,v_n)= x_i \det(v_1,\dots,v_{i-1},v_i,v_{i+1},\dots,v_n)=x_i\det A=x_i \Delta.$$
\endproof
\ethrm

Итак мы научились явно решать систему линейных уравнений. Заметим, в свою очередь, что столбец $u_j$ в матрице $A^{-1}$ есть решение уравнения $Au_j=e_j$. Тогда по формуле Крамера
$$(A^{-1})_{ij}=\frac{\det(v_1,\dots,v_{i-1}, e_j,v_{i+1},\dots,v_n)}{\det A}=\frac{A^{ji}}{\det A}.$$
Это приводит нас к определению:

\dfn
Присоединённой матрицей к матрице $A$ называется матрица $(\Adj A)_{ij}= A^{ji}$ где $A^{ij}$ -- алгебраическое дополнение элемента $a_{ij}$.
\edfn

\thrm
Пусть $A\in M_n(K)$. Тогда имеет место соотношение
$$\Adj A \cdot A= A\cdot \Adj A= \det(A)\cdot E.$$
\proof[Первое доказательство] Для определённости будем доказывать $A\cdot \Adj A= \det(A)\cdot E$. Посчитаем диагональные элементы. Они имеют вид $\sum_{k=1}^n a_{ik} A^{ik}=\det A$ по формуле разложения по $i$-ой строке для матрицы $A$. 

Посмотрим теперь на внедиагональный элемент в произведении. Он представим суммой $\sum_{k=1}^n a_{ik}A^{jk}$ при $j\neq i$. Рассмотрим матрицу $A'$, в которой все строки, кроме $j$-ой такие же как в матрице $A$, а на место $j$-ой строки поставлена копия $i$-ой. Понятно, что $\det A'= \sum_{k=1}^n a_{ik}A^{jk}$ по формуле разложения по строке. С другой стороны матрица $A'$ вырождена. Поэтому это $0$.

\proof[Второе доказательство] Заметим, что если матрица $A$ обратима, то это уже у нас в кармане. Что же делать для необратимых $A$?
Оказывается, что вместо того, чтобы пытаться как-то свести к тождеству для всех матриц над тем же полем $K$ проще доказать это тождество над любым кольцом. Точнее, заметим, что это тождество равносильно равенству нулю некоторых целочисленных многочленов от коэффициентов матрицы. Это тождество имеет смысл над любыми кольцами. Тогда, если мы доказали это тождество для кольца $R$ и матрицы $B$, и есть гомоморфизм $\ffi \colon R \to S$, то это тождество верно для матрицы $\ffi(B)$ над $S$.

Рассмотрим кольцо $\mb Z[a_{11}, \dots, a_{nn}]$ многочленов от $n^2$ независимых переменных. Над этим кольцом есть матрица $A$, что $A_{ij}=a_{ij}$. Эта матрица называется общей матрицей (размера $n\times n$). Потому что для любой матрицы $B$ размера $n\times n$ над кольцом $R$ есть гомоморфизм $\ffi \colon \mb Z[a_{11},\dots,a_{nn}]\to R $, что $\ffi(A)=B$. Иными словами в эту матрицу можно подставить любые значения.

Таким образом, достаточно доказать наше тождество для одной матрицы $A$. Для этого вложим кольцо
$$\mb Z[a_{11},\dots,a_{nn}] \to Q(\mb Z[a_{11},\dots,a_{nn}]).$$ Тогда достаточно доказать тождество для матрицы $A$ в $Q(\mb Z[a_{11},\dots,a_{nn}])$. Для этого покажем, что матрица $A$ невырождена. Заметим, что её определитель -- это ненулевой многочлен $$\sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n a_{i\sigma(i)} \in Q(\mb Z[a_{11},\dots,a_{nn}]),$$ который мы обычно и называем определителем. Раз определитель $A$ не ноль, то $A$ -- невырожденная матрица над полем $Q(\mb Z[a_{11},\dots,a_{nn}])$ и, следовательно для неё тождество выполнено. Что и требовалось.
\endproof
\ethrm

В этом доказательстве мы пользовались возможностью перейти к полю частных и тем, что общая матрица обратима хотя и не над кольцом $\mb Z[a_{11},\dots,a_{nn}]$, но в его поле частных. Если посмотреть, когда ещё можно провернуть похожее рассуждение, то можно сформулировать принцип: если есть некоторое полиномиальное тождество, верное для всех значений переменных, кроме, возможно, тех, которые задаются нетривиальным полиномиальным уравнением $p(x_1,\dots,x_n)=0$, то это тождество выполнено без исключений. Под словом, нетривиальный здесь подразумевается следующее: есть такой набор координат, для которых $p(x_1,\dots,x_n)\neq 0$. 

\upr Покажите, что над любым кольцом $\det(AB)=\det A \det B$.
\eupr


\section{Алгебры}

{\bf {\color{red} Внимание!!!}} Начиная с этого момента под словом кольцо я буду понимать ассоциативное кольцо с единицей. Коммутативно кольцо или нет теперь придётся уточнять.

Настала пора познакомиться с самой навороченной структуры, которая будет у нас в курсе. Начнём издалека. Если есть два пространства $V_1$ и $V_2$ размерностей $n$ и $m$, то имеет место изоморфизм векторных пространств $\Hom(V_1,V_2)\cong M_{m\times n}(K)$. Ничего лучше не сказать, потому что никаких других структур на этих пространствах в общем случае нет. Однако если $V=V_1=V_2$, то пространство $\Hom(V,V)=\End(V)$, то есть множество линейных операторов на $V$ является ещё и кольцом относительно сложения и композиции.  Структура векторного пространства и кольца связаны (кроме дистрибутивности, см. прошлый семестр) ещё одним соотношением:
$$\forall \lambda \in K \text{ и } L_1, L_2 \in \End(V) \text{ верно, что } (\lambda L_1)\circ L_2= \lambda (L_1 \circ L_2)=L_1 \circ(\lambda L_2).$$


\dfn(Алгебра над полем) Пусть $K$ -- поле. Кольцо $S$ вместе с отображением $K \times S \to S$ называется алгеброй, если \\
1) $\forall k \in K$, $\forall u,v \in S$ $k(uv)=(ku)v=u(kv)$.\\
2) $S$ является  векторным пространством над $K$ относительно указанных операций.
\edfn


\rm По нашему соглашению получается, что алгебры всегда ассоциативные кольца с единицей. Стоит понимать, что это всего лишь соглашение в рамках нашего курса. Общепринятое определение не подразумевает этих свойств. Есть важные примеры алгебр, которые не имеют единицы или неассоциативны. Это алгебры функций с компактным носителем в $\mb R^n$ (нет единицы) и так называемые алгебры Ли (нет ассоциативности и единицы). В нашем курсе они не будут систематически появляться.

Можно определить понятие алгебры над произвольным коммутативным кольцом. Дать аналогичное определение над некоммутативным кольцом затруднительно. Причина в свойстве 1).
\erm

Многие из колец, которые появлялись в курсе были алгебрами над каким-то полем:

\exm
\enm
\setcounter{enumi}{-1}
\item Поле $K$ есть алгебра над собой
\item Если $L$ -- расширение поля $K$, то $L$ является алгеброй над $K$.
\item Например, $\mb C$ -- это алгебра над $\mb R$
\item Кольцо эндоморфизмов $\End_K(V)$ векторного пространства $V$ над полем $K$ является алгеброй над $K$. В частности, кольцо матриц над полем $M_n(K)$ есть алгебра над $K$. Это некоммутативная алгебра над $K$ при $n\geq 2$. 
\item То же можно сказать про кольцо верхнетреугольных(нижнетреугольных) матриц $UT_n(K)$.
\item Кольцо многочленов $K[x_1,\dots,x_n]$ есть алгебра над $K$.
\item Любой фактор кольца многочленов $K[x_1,\dots,x_n]/I$ есть алгебра над $K$.
\item Разберём для разнообразия ещё одну конструкцию, которая приводит к интересным некоммутативным алгебрам. Представим себе векторное пространство $V$ с базисом $e_1,\dots,e_n$. Допустим, хочется завести на $V$ структуру алгебры. То есть необходимо научиться умножать два элемента из $V$. Произвольный элемент из $V$ выглядит как сумма $\lambda_1 e_1+\dots+\lambda_n e_n$. Посмотрим, что должно происходить при перемножении двух таких элементов:
$$(\sum_{i=1}^n \lambda_i e_i)\cdot (\sum_{j=1}^n \mu_j e_j)=\sum_{i,j} \lambda_i\mu_j (e_i\cdot e_j).$$
Таким образом видно, что произведение на самом деле достаточно определить только на базисных элементах, а дальше продолжить на все элементы пользуясь указанной выше формулой. Это гарантированно даст структуру кольца. Однако, нам стоит разобраться, когда такая операция даст ассоциативную кольцо. Оказывается, что для этого достаточно ассоциативности умножения на базисных элементах $(e_i\cdot e_j)\cdot e_k=e_i\cdot (e_j\cdot e_k)$. Действительно
$$\left((\sum_{i=1}^n \lambda_i e_i)\cdot (\sum_{j=1}^n \mu_j e_j)\right)\cdot(\sum_{k=1}^n \nu_k e_k)= \sum_{i,j,k} \lambda_i\mu_j \nu_k (e_i\cdot e_j)\cdot e_k=\sum_{i,j,k} \lambda_i\mu_j \nu_k e_i\cdot (e_j \cdot e_k)=(\sum_{i=1}^n \lambda_i e_i)\cdot \left((\sum_{j=1}^n \mu_j e_j)\cdot(\sum_{k=1}^n \nu_k e_k)\right)$$
Теперь осталось привести конкретный пример: пусть $G$ группа из $n$ элементов (можно конечно и не конечную брать -- к чему это приведёт подумайте сами). Групповой алгеброй $K[G]$ над полем $K$ назовём следующую алгебру: возьмём пространство столбцов размера $n$, занумеруем элементы стандартного базиса элементами группы $G$ (любым способом). Соответствующий $g\in G$ базисный вектор будем обозначать $e_g$. Унаследуем умножение на базисных векторах с элементов группы:
$$e_g e_h=e_{gh}.$$
Это определяет структуру алгебры на $K^n$, которая и называется групповой алгеброй. $K[G]$ некоммутативна тогда и только тогда, когда $G$ некоммутативна. Это центральный объект теории представлений групп -- важной области, использующейся в физике.
\eenm





\dfn Отображение $f \colon S_1 \to S_2$, где $S_1$ и $S_2$ являются $K$-алгебрами, называется гомоморфизмом $K$-алгебр, если $f$ -- гомоморфизм колец и линейное отображение.
\edfn



\rm Алгебра эндоморфизмов $\End_K (V)$  изоморфна,  алгебре матриц $M_{\dim V}(K)$.
\erm

В теории групп любая конечная группа могла быть реализована как подгруппа в $S_n$. Оказывается, что мы давно знаем алгебру, которая играет аналогичную $S_n$ роль.



\thrm[Теорема типа Кэли] Любая алгебра $A$, конечномерная над полем $K$, вкладывается в $\End_K(A)$ (то есть фактически в кольцо матриц).
\proof Пусть $x\in A$. Тогда рассмотрим отображение $L_x \colon A \to A$, заданное по правилу $L_x(z)=xz$. Тогда $L_x\circ L_y=L_{xy}$. Так же $L_x+L_y=L_{x+y}$, а $L_1=\id_A$. Заметим, что, если $x\neq 0$, то $L_x$ не нулевое, потому что $L_x(1)=x\neq 0$. Таким образом отображение $A \to \End_K(A)$ заданное правилом $x\to L_x$ является инъективным гомоморфизмом алгебр над $K$, что и требовалось.
\endproof
\ethrm

\rm[Дополнительно] В частности, $\mb C$ вкладывается в алгебру матриц $M_2(\mb R)$ по правилу $a+bi \to \left( \begin{smallmatrix} a &-b\\ b & a \end{smallmatrix}\right) $. Для любой алгебры указанное вложение позволяет ввести понятие нормы: $norm (y)=\det L_y$ -- численная характеристика для любого элемента алгебры за бесплатно. 
\erm



Нам извести, что, если  два коммутативных кольца $R$ и $S$  связаны гомоморфизмом $f\colon R \to S$, то есть единственный гомоморфизм $R[x] \to S$ переводящий $x$ в заданный элемент $s \in S$, а на константах, совпадающий с $f$. Он имеет вид $p(x) \to f(p)(s)$. Я буду апеллировать к аналогичной конструкции в несколько другой ситуации.

\rm
Пусть $K$ -- поле, $A$ -- алгебра над $K$. Заметим, что для элемента $y \in A$ и многочлена $p(x)=a_0+\dots+a_n x^n\in K[x]$ можно определить элемент $p(y)=a_0+\dots+a_n y^n \in A$. Соответствие $p(x) \to p(y)\in A$ определяет единственный гомоморфизм $K$-алгебр $\ffi \colon K[x]\to A$ такой, что $\ffi(x)= y$.
\erm



\rm[Дополнительно] Однако не всё так просто, если переменных больше. Пусть $a,b$ два элемента алгебры $A$, которые не коммутируют между собой. Тогда не существует гомоморфизма $K[t_1,t_2]$ переводящего $t_1\to a$, и $t_2 \to b$.
\erm

\upr Является ли условие коммутирования необходимым условием для существования гомоморфизма? Каков ответ для $K[x_1,\dots,x_n]$?
\eupr




\utv Для любого элемента $y$ конечномерной алгебры $A$ существует $p(x)\in K[x]$, $p(x)\neq 0$ такой, что $p(y)=0$.
\proof Рассмотрим набор элементов $1,y,\dots, y^{\dim A}$. Они линейно зависимы. Следовательно, их нетривиальная линейная комбинация равна нулю, что и даёт искомое уравнение.
\endproof
\eutv


\dfn Ядро гомоморфизма $K[x] \to A$, переводящего $x \to y$ является идеалом  $Ann_y\leq K[x]$. Его элементы называют аннуляторами для элемента $y$ из $A$. Если этот идеал не 0 (есть нетривиальный многочлен, аннулирующий $y$), то образующую этого идеала (со старшим коэффициентом 1) называют минимальным многочленом для элемента $y\in A$ и обозначают $\mu_y(x)$. По другому, это многочлен минимальной степени со старшим коэффициентом 1, аннулирующий $y$.
\edfn

Мы только что показали, что в конечномерной алгебре у любого элемента есть минимальный многочлен. Получим из этого простое следствие:

\thrm Любой элемент $y$ конечномерной алгебры $A$ над полем $K$ либо обратимым, либо делитель нуля (с любой стороны).
\proof Рассмотрим минимальный многочлен $\mu_y(x)= x^n+ \dots+a_0$. Пусть $a_0$ свободный член $p(y)$. Если $a_0=0$, то $$y(y^{n-1}+
\dots + a_1)=(y^{n-1}+
\dots + a_1)y=0.$$ Благодаря минимальности оба выражения отличны от 0, что показывает, что $y$ делитель нуля с любой стороны. Если же $a_0\neq 0$, то $$y^{-1}=\frac{1}{-a_0}(y^{n-1}+
\dots + a_1).$$ В частности, обратный элемент в конечномерной алгебре всегда есть многочлен от исходного. Одновременно оба условия выполнены быть не могут так как, если $yb=0$ и $y^{-1}y=1$, то $b=y^{-1}yb=0$.
\endproof
\ethrm

\rm В частности, обратная матрица всегда многочлен от исходной.
\erm

\upr Для бесконечномерных алгебр это неверно.
\eupr

\upr Покажите, что если $L$ расширение поля $K$, то минимальный многочлен любого ненулевого элемента $y\in L$ неприводим.
\eupr





\section{Линейные операторы}

Посмотрим более пристально на алгебру матриц, или, в бескоординатной форме -- алгебру операторов. Конструкции, которые относятся к структуре кольца на пространстве матриц уже встречались нам. Это прежде всего возведение матрицы в степень -- с его помощью мы считали количество путей в графе, распределение людей в городах. Вот ещё один пример:

Рассмотрим последовательность $x_{n+2}=x_{n+1}+x_{n}$, $x_0=a,x_1=b$. Как посчитать $x_{1000}=?$. Для того, чтобы воспользоваться рекуррентой, надо сделать 1000 операций. Можно ли меньше? С одной стороны вы знаете ответ -- надо найти характеристический многочлен и посчитать его корни, а потом свести всё к вычислению геометрической прогрессии.  Однако в этом случае для получения точного ответа придётся возиться с  иррациональными корнями. Попробуем сделать по другому. Заменим наше соотношение системой
$$ \begin{cases} x_{n+1}=x_n+y_n \\
y_{n+1}=x_{n}
\end{cases}.$$
Перепишем её в следующем виде
$$ \pmat x_{n+1}\\ y_{n+1}\epmat = A \pmat  x_{n}\\ y_n \epmat, \text{ где } A=\pmat 1& 1\\ 1& 0 \epmat.$$
Тогда  $x_{1000}$ это первая координата столбца $A^{1000} \pmat b\\ a\epmat $. Итого достаточно просто возвести матрицу в 1000 степень. Заметим, что это частный случай общей задачи: имеется последовательность $x_n$ из $K^m$ удовлетворяющая соотношению $x_{n+1}=Ax_n$. Требуется найти её поведение в зависимости от $n$. Например -- дан  набор состояний $s_1,\dots,s_k$ и даны вероятности перехода между состояниями $a_{ij}$ за один шаг. Вопрос: что произойдёт с системой после $n$ шагов?
Или, сводя к произведению матриц: как ведёт себя степень матрицы в зависимости от $n$? Оказывается, что этот вопрос удобнее решать не на языке матриц, а на бескоординатном языке операторов, по той причине, что в этой ситуации выбор подходящих координат остаётся за нами. 


Какие величины, связанные с оператором $L \colon V \to V$ не зависят от системы координат. Все свойства оператора $L$ можно определить по его матрице в каком-нибудь базисе. Заметим, что если есть два базиса $e$ и $f$, то матрицы $A=[L]_e^e$ и $A'=[L]_f^f$ связаны соотношением $A'=CAC^{-1}$ для некоторой обратимой матрицы $C$. Наоборот, если $A=[L]_e^e$  и $A'=CAC^{-1}$, то $A'$  тоже матрица $L$, но в другом базисе.

\upr Докажите это.
\eupr

Получается, что свойства оператора $L$, не зависящие от выбора координат, это такие свойства его матрицы $A$, которые не меняются когда мы $A$ заменяем на $A'=CAC^{-1}$. Это приводит нас к стандартному определению:  



\dfn Две матрицы $A, B \in M_n(K)$ подобны если существует матрица $C \in \GL_n(K)$, что $A=CBC^{-1}$. Матрицы одного оператора в разных базисах подобны.
\edfn

Изучение операторов -- это тоже самое, что изучение матриц с точностью до подобия. Нам будет удобно прыгать между двумя этими языками. Вернёмся к операторам. Попробуем придумать какие-то свойства не зависящие от выбора базиса.



\dfn Пусть $V$ -- пространство с оператором $L$. Пусть $U\leq V$. Тогда $U$ называется инвариантным подпространством, если $L(U) \leq U$.
\edfn

\rm Это условие позволяет сузить оператор $L$ с $V$ на $U$. Наличие или отсутствие инвариантных подпространств какой-нибудь размерности не зависит от выбора системы координат.
\erm

\lm Пусть $U\leq V$ -- подпространство, а $L \colon V \to V$ -- линейный оператор. Тогда $U$ инвариантно относительно $L$ тогда и только тогда, когда в базисе $e_1,\dots,e_k,e_{k+1},\dots,e_n$, где $e_1,\dots,e_k$ базис $U$ матрица оператора имеет блочно диагональный вид
$$\pmat A&B\\
0&C \epmat$$
\proof Образ $L(e_i)$ при $i \leq k$ лежит в $U$ и раскладывается по базису $U$, то есть по первым $k$ векторам. Значит коэффициенты при последних $n-k$ векторах нулевые. Но это и есть коэффициенты в левом нижнем блоке матрицы $A$.
\endproof
\elm


\rm У нас снова всплыли блочные матрицы и на этот раз нам необходимо обсудить как перемножаются матрицы в таком виде. Общая формулировка выглядит так. Если есть две матрицы
$$A=\pmat A_{11} & A_{12}\\
A_{21}& A_{22}
\epmat \text{ и } B=\pmat B_{11} & B_{12}\\
B_{21}& B_{22}
\epmat $$
которые перемножаемы (размеры $A_{ik}$ и $B_{kj}$ согласованы), то тогда
$$AB= \pmat A_{11}B_{11}+ A_{12}B_{21} & A_{11}B_{12}+ A_{12}B_{22}\\
A_{21}B_{11}+ A_{22}B_{21}& A_{21}B_{12}+ A_{22}B_{22}\epmat. $$
То есть матрицы перемножаются по блокам.
\erm

Посмотрим на простейший случай, когда инвариантное пространство одномерно. Пусть оно порождено вектором $v$. Тогда условие инвариантности перепишется как $L(v) = \lambda v$ для некоторого $\lambda \in K$.


\dfn Пусть $V$ -- пространство с оператором $L$. Тогда вектор $0\neq v\in V$ называется собственным вектором с собственным числом $\lambda$ относительно оператора $L$, если $Lv=\lambda v$.
\edfn





\exm \\
1) Рассмотрим пространство  последовательностей и оператор сдвига $S(x)_n= x_{n+1}$. Тогда собственный вектор -- это геометрическая прогрессия.\\
2) Рассмотрим тот же контекст. Тогда несложно увидеть, что подпространство последовательностей, удовлетворяющих линейному рекуррентному соотношению с постоянными коэффициентами является инвариантным относительно оператора сдвига.\\
3) В задаче про поисковую систему нам нужен был вектор $w$, такой что $P_G w=w$. Это собственный вектор $P_G$ с собственным числом $1$.\\
4) Подпространство многочленов степени меньшей или равной $n$ инвариантно относительно оператора дифференцирования.\\
5) Если $p(x)$ многочлен, то $\Ker p(L)$ инвариантно относительно $L$.\\
6) Пусть $v \in V$. Тогда  $V'=\lan v, Lv, L^2v,\dots \ran$ является инвариантным пространством, порождённым $v$. Такое пространство называется циклическим.\\
7) Рассмотрим алгебру $K[x]/p(x)q(x)$. Тогда подпространство многочленов делящихся на $p(x)$ является инвариантным относительно оператора $f(x) \to x f(x)$ домножения на $x$.\\




Как найти собственные векторы и соответствующие им собственные числа? Оказывается, что проще найти сначала собственные числа.

\dfn Определим характеристический многочлен оператора $L$ как $\chi_L(t)=\det(A-tE_n)$, где $A$ -- матрица $L$ в некотором базисе.
\edfn

\lm Характеристический многочлен корректно определён.
\elm
\proof Пусть $A$ матрица оператора $L$ в базисе $e$, $A'$ -- в базисе $f$, а $C$ матрица перехода откуда-то куда-то. Тогда $A'=CAC^{-1}$. Рассмотрим $\chi_A(t)$, как элемент $K(t)$. Тогда $C$ -- тоже матрица над $K(t)$ и
$$\det(A'-tE)=\det(CAC^{-1}-tCC^{-1})=\det(C(A-tE)C^{-1})=\det(C)\det(A-tE)\det C^{-1}=\det(A-tE).$$
Раз эти выражения равны как элементы $K(t)$, то и как элементы $K[t]$.
\endproof

\utv Элемент $\lambda \in K$ является собственным числом оператора $L$ тогда и только тогда, когда $\lambda$ корень $\chi_L(t)$.
\proof $\lambda$ собственное число тогда и только тогда, когда есть ненулевой $v$, что $Lv=\lambda v$ тогда и только тогда, когда $(L-\lambda \id)v=0$ тогда и только тогда, когда матрица этого оператора вырождена тогда и только тогда, когда $\det (A-\lambda E) =0$.
\endproof
\eutv



\section{Диагонализация}

В прошлый раз мы остановились на том, что собственные числа оператора $L$ -- это корни характеристического многочлена $\chi_L(t)$. В этот раз наша цель -- показать, что  для почти всех операторов, характеристический многочлен есть единственная величина, которая не зависит от выбора базиса. То есть, почти всё определяется характеристическим многочленом. В конце параграфа мы покажем, какие бывают исключения.


Первый вопрос, который мы зададим про характеристический многочлен, следующий: насколько общим $\chi_L(t)$ бывает? Верно ли, что любой многочлен $f(x)\in K[x]$ ($\deg f \geq 1$) может быть реализован с точностью до обратимой константы как характеристический многочлен некоторой матрицы над $K$?

\dfn Пусть $f(x)\in K[x]$ многочлен $\deg f \geq 1$. Тогда сопровождающей матрицей к $f(x)=x^n+a_{n-1}x^{n-1}+\dots+a_0$ называется 
$$ \pmat  &&&&-a_0\\
1&&&&-a_1\\
&1&&&-a_2\\
&&\ddots&&\vdots\\
&&&1&-a_{n-1}\epmat.$$
\edfn

\utv Характеристический многочлен сопровождающей матрицы равен $(-1)^n f(t)$.
\eutv
\proof Рассмотрим матрицу $A-tE$, где $A$ -- сопровождающая матрица:
$$A-tE= \pmat -t&&&&-a_0\\
1&-t&&&-a_1\\
&\ddots&\ddots&&\vdots\\
&&1&-t&-a_{n-2}\\
&&&1&-t-a_{n-1}\epmat.$$
Прибавим последнюю строку к $n-1$-ой с коэффициентом $t$. Имеем:
$$\det A-tE = \det \pmat -t&&&&-a_0\\
1&-t&&&-a_1\\
&\ddots&\ddots&&\vdots\\
&&1&0&-t^2-ta_{n-1}-a_{n-2}\\
&&&1&-t-a_{n-1}\epmat$$
Поступая так и далее мы придём к определителю
$$\det \pmat 0&&&&-f(t)\\
1&0&&&*\\
&\ddots&\ddots&&\vdots\\
&&1&0&*\\
&&&1&*\epmat=(-1)^{n-1}(-f(t))=(-1)^nf(t).$$
\endproof

\upr Найдите в примерах оператор, матрицей которого является данная матрица или её транспонирование.
\eupr



Коэффициенты многочлена $\chi_L(t)$ являются инвариантами оператора $L$. Давайте посмотрим на них чуть внимательнее.
Прежде всего в глаза бросается, что свободный член это $\chi_L(0)=\det L$.
 

\dfn Пусть $A$ -- матрица размера $n$, тогда $$\Tr A=\sum_{i=1}^n a_{ii}.$$ 
Cлед оператора $L$ -- это след его матрицы. Это определение не зависит от выбора базиса.
\edfn

\rm След матрицы $A$ это $(-1)^{n-1}a_{n-1}$, где $\chi_A(t)= \sum a_{i}t^i$. 
\erm

След оператора обладает массой интересных свойств:

\lm След обладает следующими свойствами: \\
1) Пусть $A$ -- квадратная матрица. Тогда $\tr CAC^{-1}= \tr A$ для обратимой матрицы $C$.\\
2) $\tr AB = \tr BA$ для $A\in M_{n\times m}(K)$ и $B\in M_{m\times n}(K)$.\\
3) След равен сумме собственных чисел с учётом кратностей, как корней характеристического многочлена (над любым полем, где характеристический многочлен раскладывается на линейные множители).\\
4) $\tr A= \tr A^{\top}$.\\
5) $\tr( A + \lambda B) = \tr A + \lambda \tr B$.
\elm
\proof
1) следует из того, что характеристический многочлен не зависит от выбора базиса и, следовательно, его коэффициенты. 2) Если расписать, то равенство эквивалентно
$$\sum_{i,k} a_{ik}b_{ki}=\sum_{j,l} b_{jl}a_{lj}.$$
Которое верно, особенно если взять $k=j$ и $i=l$. 3) Многочлен $\chi(t)=\prod (\lambda_i-t)$. Осталось раскрыть скобки. 4),5) ясно. \endproof


\rm Аналогично следу, определитель так же выражается через собственные числа. А именно, он равен произведению собственных чисел с учётом их кратностей, как корней характеристического многочлена.
\erm


\rm Для матриц $2\times 2$ знание определителя и следа равносильно знанию характеристического многочлена. Точнее, имеет место формула $\chi_A(t)=t^2-(\Tr A)\, t + \det A$. Для матриц большего размера есть формула для коэффициентов характеристического многочлена через миноры исходной матрицы.
\erm


Поставим следующий вопрос: насколько простой можно выбрать матрицу оператора $L$? Давайте заметим, что как ни крути, матрица линейного отображения должна хранить информацию о характеристическом многочлене, в частности -- о его корнях. Итого, не менее чем $k=\dim V$ параметров должно остаться. Наше чувство прекрасного или же чувство лени подсказывает нам, что самая удобная форма матрицы  -- это диагональная матрица:
$$\pmat \lambda_1 & 0 &\dots & 0\\
0 & \lambda_2 & &0 \\
\vdots && \ddots &\vdots \\
0 & \dots & 0 & \lambda_k \epmat$$

В такой системе координат матрица $L^n$ будет иметь вид:
$$\pmat \lambda_1^n & 0& \dots & 0\\
0 & \lambda_2^n & &0 \\
\vdots && \ddots &\vdots \\
0 & \dots & 0 & \lambda_k^n \epmat.$$
Теперь начнём подробный разбор:


\dfn Оператор называется диагонализуемым,  если в некотором базисе $V$ его матрица диагональна. Матрица $A\in M_n(K)$ называется диагонализуемой, если соответствующий оператор $x\to Ax$ диагонализуем. Иными словами, должна существовать обратимая $C\in M_n(K)$, что $CAC^{-1}$ -- диагональна. 
\edfn



\lm Матрица оператора $L$ в базисе $v_1,\dots,v_n$ диагональна тогда и только тогда, когда все $v_i$ -- собственные вектора $L$. В этом случае на диагонали матрицы стоят собственные числа оператора $L$.
\proof Если $Lv_i=\lambda_iv_i$, то в $i$ столбце $\lambda_i$ стоит на диагонали, а остальное -- $0$.
\endproof
\elm



\lm Пусть $v_1,\dots,v_n$ собственные вектора $L$ c собственными числами $\lambda_1,\dots,\lambda_n$. Пусть числа $\lambda_i$ попарно различны. Тогда $v_1,\dots,v_n$ линейно независимы.
\proof Пусть есть нетривиальная линейная комбинация $$\sum c_i v_i=0,$$
состоящая из минимально возможного числа векторов (пусть они нумеруются от 1 до $k$). Это означает, что все $c_i\neq 0$. Тогда, применив отображение $L$ получаем
$$0=L\left(\sum c_i v_i\right)= \sum c_i \lambda_i v_i$$
Умножая на $\lambda_1$ и вычитая получаем $$0=\sum_{i=2}^k c_i(\lambda_1-\lambda_i)v_i$$
Если в последней сумме есть хоть одно ненулевое слагаемое, то приходим к противоречию. но это бывает только если $\lambda_i=\lambda_1$ для всех $i$, что невозможно из условия леммы.
\endproof
\elm

\dfn[Алгебраическая и геометрическая кратность] Пусть $L$ оператор на пространстве $V$. Алгебраической кратностью собственного числа $\lambda$ называется его кратность как корня характеристического многочлена $\chi_L(t)$. Геометрической кратностью $\lambda$ называется размерность $\Ker L-\lambda \id$.
\edfn

\lm Пусть $L$ -- линейный оператор на пространстве $V$, а $\lambda$ -- его собственное число. Тогда алгебраическая кратность $\lambda$ не меньше его геометрической кратности.
\elm
\proof Пусть $k$ -- это геометрическая кратность $\lambda$. Тогда есть $k$ линейно независимых собственных векторов $e_1,\dots,e_k$ с собственным числом $\lambda$. Их можно дополнить до базиса. Полученная матрицы оператора $L$ будет иметь блочный вид, в первом блоке которого будет стоять матрица $\lambda E_{k}$. Следовательно характеристический многочлен делится на $(t-\lambda)^k$. Таким образом, алгебраическая кратность не меньше $k$.
\endproof

\rm Несложно найти матрицу. с разными алгебраической и геометрической кратностями для собственных чисел:
$$\pmat \lambda & 1 \\ 0& \lambda \epmat.$$
Собственное число $\lambda$ имеет алгебраическую кратность $2$, но геометрическую кратность $1$.
\erm



\thrm[Критерий диагонализуемости] Пусть $K$ -- поле. Пусть все корни $\chi_L(t)$ лежат в $K$.  Тогда оператор $L$ диагонализуем в том и только том случае, когда для всякого собственного числа $\lambda$ его алгебраическая кратность равна его геометрической кратности.
\ethrm
\proof Пусть $L$ диагонализуем. Рассмотрим его базис из собственных векторов и диагональную матрицу $L$ в этом базисе. Заметим, что $\dim \Ker L- \lambda \id $ совпадает с количеством  $\lambda$ на диагонали у этой матрицы, что, с другой стороны, равно алгебраической кратности собственного числа $\lambda$.

Обратно. Сумма алгебраических кратностей $k_i$ собственных чисел равна $$\sum k_i=\deg \chi_L(t)=n=\dim V.$$
Вспомним, что по условию $k_i=\dim \Ker L - \lambda_i \id$. По условию для каждого собственного числа $\lambda_i$, можно выбрать $k_i$ линейно независимых собственных векторов $v_{i1},\dots,v_{ik_i}$.  Если объединить эти наборы, то будет ровно $n$ штук векторов. Покажем их линейную независимость.
Рассмотрим их нетривиальную линейную комбинацию $$0=\sum_{\substack{1\leq i \leq s \\ 1\leq j\leq k_i}} c_{ij} v_{ij}.$$
Разобьём эту сумму сгруппировав собственные векторы с одним и тем же собственным числом. Сумма собственных векторов с одним и тем же собственным числом есть либо собственный вектор с этим же числом, либо 0. Если какое-то слагаемое $\sum_{1\leq j \leq k_i}c_{ij} v_{ij}=0$, то по независимости векторов $v_{ij}$ при фиксированном $i$ получаем, что $c_{ij}=0$. Такие слагаемые можно выкинуть из суммы. Если слагаемых не осталось, значит все коэффициенты с самого начала были нулевыми. 

Пусть в сумме по $i$ есть ненулевые слагаемые -- то есть, слагаемые вида $\sum_{1\leq j \leq k_i}c_{ij} v_{ij}$.  Это собственные векторы для различных собственных чисел $\lambda_i$. Значит их сумма не может быть равна нулю по предыдущей лемме.

Так как число векторов $v_{ij}$ равно размерности пространства $V$, то  из линейной независимости следует, что они образуют базис $V$.
\endproof



\crl Пусть $K$ -- алгебраически замкнутое поле. Если характеристический многочлен не имеет кратных корней, то оператор $L$ диагонализуем.
\ecrl
\proof В этом случае алгебраическая кратность совпадает с геометрической, потому что обе они равны~1.
\endproof






\crl \label{gprog}
Пусть дана последовательность $x_n\in \mb C$ удовлетворяющая линейному рекуррентному соотношению 
$$x_{n+k}+a_{k-1}x_{n+k-1}+\dots+a_0x_n=0,$$
где $a_i \in \mb C$. Рассмотрим многочлен $f(t)=t^k+a_{k-1}t^{k-1}+\dots+a_0$. Пусть у $f(t)$ нет кратных корней. Тогда $x_n=c_1 \lambda_1^n+\dots+c_k\lambda_k^n$, где $\lambda_i$-- корни $f(t)$.
\ecrl

\proof  Пусть $V$ -- пространство последовательностей, заданных линейным рекуррентным соотношением $x_{n+k}+a_{k-1}x_{n+k-1}+\dots+a_0x_n=0$. Оно имеет размерность $k$: в качестве базиса можно взять последовательности, которые начинаются с вот таких вот упорядоченных $k$-шек:
$$(0,\dots,1,\dots,0).$$
Посмотрим как на них действует оператор сдвига: последовательность с началом $(0,\dots,1,\dots,0)$, где $1$ стоит на позиции $1\leq i\leq k$ переходит в последовательность с началом $(0,\dots,1,\dots, -a_{i-1})$, где $1$ стоит на позиции $i-1$, либо $1$ вообще нет, если $i=1$.
Таким образом, матрица оператора сдвига имеет вид: 
$$ \pmat 
&1&& \\
&&\ddots&\\
&&&1 \\
-a_0 & -a_1& \dots & -a_{k-1}
\epmat $$
Но это просто транспонированная матрица к присоединённой для многочлена $f(t)$. Её характеристический многочлен равен $(-1)^nf(t)$. Многочлен $f(t)$ не имеет кратных корней.  По критерию диагонализуемости у оператора сдвига есть базис из собственных векторов, с собственными числами $\lambda_i$ -- корнями $f(t)$. Но собственные вектора для оператора сдвига -- это геометрические прогрессии. Значит в $V$ есть базис из геометрических прогрессий и частные этих прогрессий -- это $\lambda_i$.
\endproof

Даже если все корни характеристического многочлена матрицы  лежат в данном поле, это не значит, что матрица диагонализуется: вот простейший пример 
$$\pmat 0&1\\ 0&0 \epmat.$$

Теперь мы так же можем отчасти ответить на самый первый вопрос: если две матрицы (над $\mb C$) имеют одинаковый характеристический многочлен, и у него нет кратных корней, то они подобны: действительно, они обе подобны диагональной матрице, у которой на диагонали стоят собственные числа их характеристического многочлена.

С другой стороны, если кратные собственные числа есть, то ситуация усложняется. Вот две матрицы, имеющие одинаковый характеристический многочлен, но не подобные:
$$\pmat 0&1\\ 0&0 \epmat\,\,\,\,\, \pmat 0&0\\ 0&0 \epmat.$$
Первая недиагонализуема (по критерию), а вторая уже диагональна.


\section{Подготовка}

Прежде чем дать общий ответ на вопрос о том, что может происходить в случае, если оператор не диагонализуется обсудим несколько лемм и конструкций:



\lm Пусть $L$ -- оператор на пространстве $V$, а  многочлен $g(t)=p(t)q(t)$ аннулирует $L$ (то есть $g(L)=0$). Причём $(p(t),q(t))=1$. Тогда подпространство $V$ раскладывается в прямую сумму инвариантных подпространств
$$V = \Ker p(L)\oplus \Ker q(L).$$
\elm
\proof Рассмотрим линейное разложение $1=a(t)p(t)+b(t)q(t)$. Тогда любой вектор $v$  представим в виде
$$v=a(L)p(L)v+ b(L)q(L)v.$$
Тогда $q(L)a(L)p(L)v=0=p(L)b(L)q(L)v$. Следовательно, $V= \Ker p(L)+\Ker q(L)$. Покажем, что ядра пересекаются по нулю. Пусть $v\in \Ker p(L) \cap \Ker q(L)$. Тогда $v=a(L)p(L)v+ b(L)q(L)v=0$.
\endproof

\upr Как мы уже отмечали, если пространство разложилось в сумму двух $V=U_1\oplus U_2$, то есть оператор проекции, который вектору $v=u_1+u_2$ сопоставляет $u_1$ -- его $U_1$ компоненту. Это отображение называется проекцией на $U_1$ вдоль $U_2$.
Где в доказательстве всплыл оператор проектирования на $\Ker p(L)$ вдоль $\Ker q(L)$?
\eupr


Начнём с того, почему разложение пространства в прямую сумму инвариантных так удобно:

\utv Пусть $L$ -- оператор на $V$, пространство $V=U_1\oplus U_2$, где $U_1,U_2$ инвариантны. Если $e_1,\dots,e_k$ и $f_1,\dots,f_l$ -- базисы $U_1$ и $U_2$, то матрица $L$ в базисе $e_1,\dots, f_l$ имеет вид
$$\pmat A_1 & 0 \\ 0 & A_2 \epmat.$$
Угадайте, что это за матрицы $A_1$ и $A_2$.
\eutv




Прекрасно, когда всё пространство раскладывается в прямую сумму двух инвариантных подпространств. Но так тоже не всегда бывает. Вот более общая конструкция. Надеюсь, за ней вы увидите ранее знакомый алгебраический принцип:

\dfn Пусть $U$ -- подпространство в $V$. Определим на факторе $V/U$ структуру векторного пространства следующим $\lambda \ovl{v}=\ovl{\lambda v})$.
\edfn

\upr Проверьте, что это действительно векторное пространство.
\eupr

\upr Как найти базис $V/U$ зная базис $U$ и базис $V$?
\eupr

\upr Рассмотрим пространство $V=\mb R^4$ и в нём подпространства $U=\lan (1,1,1,1)^\top, (2,2,1,1)^\top \ran$. Найдите базис факторпространства.
\eupr


\dfn Пусть $V$ -- пространство с оператором $L$, а $U$ -- инвариантное подпространство. Тогда определим оператор $\ovl{L}$ на $V/U$ следующим образом:
$$\ovl{L}(\ovl{v})=\ovl{L(v)}.$$
\edfn

\rm Разумеется, вычислять многочлен от оператора на факторпространстве можно на представителях. Формально, если $p(x)$ -- многочлен, а $v\in V$, то $p(\ovl{L})\ovl{v}=\ovl{p(L)v}$.
\erm

\rm Мы уже знаем, что инвариантное подпространство приводит к тому, что в подходящем базисе матрица линейного оператора становится блочно-верхнетреугольной и верхний блок -- это матрица сужения оператора. Дадим интерпретацию нижнего правого блока:  Пусть $e_1,\dots,e_n$ базис $V$ и $\lan e_1,\dots,e_k\ran$ -- инвариантное пространство относительно $L$. Если  матрица $L$ в этом базисе имеет вид $$\pmat A& B \\ 0 & C\epmat,$$
то $C$ -- это матрица $\ovl{L}$ в базисе
$\ovl{e_{k+1}},\dots,\ovl{e_n}$.
Следовательно, $$\chi_L(t)=\chi_{L|_{V'}}(t)\cdot \chi_{\ovl{L}}(t).$$
\erm

Итак, если мы нашли инвариантное подпространство $U$ внутри $V$ относительно $L$, то исследование $L$ в некотором смысле сводится к исследованию $L|_U$ на  $U$ и $\ovl{L}$ на $V/U$. Покажем пример, как это работает.


\thrm[Теорема Гамильтона-Кэли] Пусть $L$ -- оператор на $V$. Пусть многочлен $\chi_L(L)$ раскладывается в $K$ на линейные множители. Тогда $\chi_L(L)=0$.
\ethrm
\proof Докажем по индукции. Случай $\dim V=1$ ясен. Шаг. Так как $K$ алгебраически замкнуто, то у характеристического многочлена есть корень $\lambda_1$ и собственный вектор $e_1$. Рассмотрим фактор $V/\lan e_1\ran$. Для него теорема выполнена. Заметим, что $$\chi_L(t)= -(t-\lambda_1)\chi_{\ovl{L}}(t).$$
Пусть $v \in V$. Тогда $\chi_{\ovl{L}}(L)v = ce_1$ так как в факторе этот элемент равен 0. Но тогда
$$\chi_L(L)v= -(L-\lambda_1 E)\chi_{\ovl{L}}(L)v=-(L-\lambda_1 E) ce_1=0$$
\endproof

\upr Избавьтесь от предположения о том, что многочлен раскладывается на линейные множители (используя какой-нибудь  алгебраический трюк).
\eupr


\rm Теорему Гамильтона-Кэли очень легко доказать, когда $L$ -- диагонализуем.
\erm


\section{Жорданова форма}

В этом разделе я буду предполагать, что  поле $K$ алгебраически замкнуто. Ровно те же результаты можно получить, предполагая, что поле $K$ содержит все корни характеристического многочлена рассматриваемого оператора. В этом разделе мы получим полную классификацию операторов над алгебраически замкнутым полем $K$, то есть для каждого оператора мы построим некоторую матрицу -- его каноническую <<модель>> и научимся строить базис, в котором матрица оператора -- это его <<модельная>> матрица.

\dfn
Матрица $k\times k$
$$J_k(\lambda) = \begin{pmatrix}
\lambda& 1&& \\
& \lambda &1& \\
&&\ddots &\ddots& \\
&  && \lambda & 1\\
&  &&& \lambda
\end{pmatrix}
$$
называется жордановой клеткой размера $k$ с собственным числом $\lambda$.
\edfn

\rm Заметим, что для того, чтобы в базисе $e_1,\dots,e_n$ матрица оператора $L$ была жордановой клеткой необходимо и достаточно, чтобы $(L-\lambda E)e_i=e_{i-1}$ для $i\geq 2$ и $(L-\lambda E)e_1=0$. В частности, оператор $L-\lambda E$ должен быть нильпотентным.
\erm







\thrm Пусть $L\colon V \to V$ --- оператор на конечномерном пространстве над алгебраически замкнутым полем $K$.
Тогда существует базис $e_1,\dots, e_n$ в котором матрица $L$ имеет вид
$$A=\begin{pmatrix}
J_{k_1}(\lambda_1) &&&\\
& J_{k_2}(\lambda_2) &&\\
&& \ddots& \\
&&& J_{k_s}(\lambda_s)

\end{pmatrix}.
$$

Более того, такая матрица единственна с точностью до перестановки блоков. Эта матрица называется матрицей оператора в форме Жордана. Базис, в котором матрица оператора имеет такой вид называется жордановым базисом.
\ethrm
\proof

Сначала докажем единственность. Прежде всего, если нам дана матрица в жордановой форме, то мы легко можем вычислить её характеристический многочлен. Он равен $\prod (t-\lambda_i)$ где $\lambda_i$ -- все числа на диагонали, откуда сразу становится ясно, что $\lambda_i$ -- собственные числа $L$. Более того, алгебраическая кратность
$k$ собственного числа $\lambda$ равна сумме размеров клеток с этим собственным числом.

Пусть сами размеры клеток для собственного числа $\lambda$ заданы как набор чисел $h_i$. Имеем
$\sum h_i =k.$
Итак, набору клеток соответствует разбиение числа $k$ в сумму некоторого числа слагаемых. Удобно упорядочить эти слагаемые по возрастанию $h_i\geq h_{i+1}$.

Можно ли как-то лучше визуализировать себе структуру жордановой формы?
Каждому разбиению числа на слагаемые однозначно соответствует картинка:


\begin{figure}[hhh]
\begin{center}
\begin{tikzpicture}[xscale=0.7, yscale=0.7]
\draw (1,0) -- (1,3) -- (0,3) -- (0,0) -- (4,0) -- (4,1) -- (0,1);
\draw (0,2) -- (3,2) -- (3,0);
\draw (2,0) -- (2,2);


\end{tikzpicture}
\end{center}
\caption{$8=3+2+2+1$, соответствует одной клетке размера 3, двум клеткам размера 2, одной клетке размера 1}
\end{figure}

А именно, сопоставим каждому слагаемому $h_i$ столбик высоты $h_i$. Такие (обычно, правда, перевёрнутые) картинки называются диаграммами Юнга.

Как же восстановить эту картинку зная оператор $L$ и собственное число $\lambda$?

Рассмотрим оператор $N=L-\lambda E$. Понятно, что сколько клеток с $\lambda$ было у $L$ в указанном базисе, столько же клеток с с.ч. 0 будет и у $N$ и они будут такого же размера.
Заметим, что жордановы клетки с собственным числом 0 являются нильпотентными матрицами.



Вспомним, что каждой клетке соответствует кусок базиса из векторов вида  $v_i, N v_i, \dots,N^{h_i} v_i$. Заметим, что эти вектора образуют базис пространства $\Ker N^k$. Действительно, $N^k$ обнуляется на векторах $v_i$ и их образах. С другой стороны $N^k$ обратим на дополнительном слагаемом.

Пририсуем эти вектора к нашей картинке следующим образом -- поместим $v_i$ наверху соответствующего клетке  столбика, $N v_i$ на ступень ниже и т.д. Таким образом заполним все ячейки диаграммы. При действии оператора $N$ на диаграмму происходит следующее -- все вектора съезжают на единицу вниз, кроме самых нижних, которые переходят в $0$.
\begin{figure}[hhh]
\begin{center}
\begin{tikzpicture}[xscale=1, yscale=1]
\draw (1,0) -- (1,3) -- (0,3) -- (0,0) -- (4,0) -- (4,1) -- (0,1);
\draw (0,2) -- (3,2) -- (3,0);
\draw (2,0) -- (2,2);
\draw[->] ( -0.5, 2.5) -- (-0.5, 0.5);
\node at (0.5, 2.5) {$v_1$};
\node at (0.5, 1.5) {$Nv_1$};
\node at (0.5, 0.5) {$N^2 v_1$};
\node at (1.5, 1.5) {$v_2$};
\node at (1.5, 0.5) {$Nv_2$};
\node at (2.5, 1.5) {$v_3$};
\node at (2.5, 0.5) {$Nv_3$};
\node at (3.5, 0.5) {$v_4$};
\node at (-1, 1.5) {$N$};
\end{tikzpicture}
\end{center}
\caption{$8=3+2+2+1$, расставляем базисные вектора}
\end{figure}
Итого, количество ячеек в диаграмме Юнга для собственного числа $\lambda$ оператора $L$ на высоте не более $s$ равно $\dim \Ker(L - \lambda E)^s $.
Это  позволяет однозначно восстановить разбиение числа и, следовательно, конфигурацию клеток, если мы знаем числа $\dim \Ker(L - \lambda E)^s $.

Точнее, число ячеек в строке уровня $s$ равно $\dim \Ker(L - \lambda E)^s -\dim \Ker(L - \lambda E)^{s-1} $.







\proof[Существование]


Для начала надо разбить всё пространство на куски с одним и тем же собственным числом. По теореме Гамильтона-Кэли оператор $L$ аннулируется многочленом $\chi_L(t)$. Разложив последний на простые множители $\chi_L(t)=\prod (x-\lambda_i)^{\alpha_i}$ получим разложение $V=\bigoplus V_i$ на примарные инвариантные подпространства $V_i=\Ker (L-\lambda_i E)^{\alpha_i}$. Ограничимся на пространства $V_i$. Эти пространства называются корневыми. Заметим, что оператор $N=L-\lambda_i E$ нильпотентен. Осталось применить следующую  лемму к операторам $N|_{V_i}$.

\lm[Основная] Для любого нильпотентного оператора $N$ на пространстве $V$ существует базис $e_1,\dots,e_n$ в котором матрица $N$ имеет вид
$$A=\begin{pmatrix}
J_{k_1}(0) &&&\\
& J_{k_2}(0) &&\\
&& \ddots& \\
&&& J_{k_s}(0)
\end{pmatrix}.
$$
\elm
\proof
Докажем по индукции. Если $V=\Ker N$, то матрица просто 0 и всё доказано. Иначе рассмотрим пространство $\Ker N$. Тогда для $V/\Ker N$ теорема верна. Рассмотрим требуемый базис $V/\Ker N$ $\ovl{\ovl{e_{ij}}}$, где первый индекс обозначает номер клетки $1\leq i \leq s$, а второй -- номер вектора(сверху вниз) в  диаграмме Юнга для $V/\Ker N$ -- $0\leq j\leq h_i-1$.
Рассмотрим вектора $e_{i0}$, которые лежат в классе $\ovl{\ovl{e_{i0}}}$ и определим $$e_{ij}=N^{h_i-j}(e_{ih_i}),$$ где  $j\in \ovl{0,h_i}$. Имеет место равенство $$\ovl{e_{ij}}=\ovl{\ovl{e_{ij}}}.$$
Рассмотрим набор векторов $e_{i,h_i}$ -- это вектора из $\Ker N$.
Покажем, что они линейно независимы и, в частности, не 0. Пусть $\sum_{i=1}^s c_ie_{i,h_i}=0$. Это значит $N(\sum_{i=1}^s c_i e_{i,h_i-1} )=0$. Тогда $\sum_{i=1}^s c_i e_{i,h_i-1} $ лежит в ядре $N$. Но в этом случае $\sum c_i \ovl{e_{i,h_i-1}}=0$ в $V/\Ker N$, чего не может быть, так как они там линейно независимы.

Теперь дополним набор $e_{1,h_1},\dots,e_{s,h_s}$ до базиса $\Ker N$  элементами $e_{s+1,0},\dots,e_{k,0}$. Я утверждаю, что  дополненный набор 
$$e_{ij}, \text{ где } 1\leq i\leq k, \,\, 0\leq j\leq h_i (\text{ при } j\leq s) \text{ и } j=0 \text{ иначе }$$
является нужным базисом. Для этого необходимо показать, что он базис. Рассмотрим отображение $V \to V/\Ker N$. Тогда часть этого набора образует базис образа, а оставшаяся часть -- базис ядра. Тогда размерность пространства, порождённого этими векторами равна $\dim \Ker N + \dim V/\Ker N = \dim V$. Откуда получаем, что это базис.

\endproof

\endproof








Допустим мы нашли характеристический многочлен, то есть все его собственные числа. Далее нашли размерности $\dim \Ker(L-\lambda E)^s$. Как теперь найти сам жорданов базис? Для этого нам необходимо заполнить верхушки всех столбцов, остальное заполнится автоматически.



Как расставить векторы в верхней строке диаграммы? Векторы $v_{i_1}, \dots, v_{i_s}$  в верхней строке определяются тем, что их образы при $(L-\lambda E)^{k-1}$ линейно независимы (в частности, не лежат в ядре). Или, (что эквивалентно) система $v_{i_1}, \dots, v_{i_s}$ вместе с базисом (любым) $\Ker (L-\lambda E)^{k-1}$ образуют линейно независимую систему. Напомню, что их число равно
$$s=\dim \Ker (L-\lambda E)^k - \dim \Ker (L-\lambda E)^{k-1}.$$



Что делать с теми клетками, чьи столбики в диаграмме Юнга начинаются не на самом верху? Пусть мы уже заполнили все строки на высоте больше $i$. Заполним остаток строки на высоте $i$.  Очевидно, что оставшиеся векторы лежат в ядре $(L-\lambda E)^{i}$ и при этом их образы при $(L-\lambda E)^{i-1}$ линейно независимы. Однако вектора из уже заполненных клеток на уровне $i$ тоже подходят под это описание. Можно однако заметить, что образы системы <<старые вектора на уровне $i$>>, <<новые вектора на уровне $i$>> при $(L-\lambda E)^{i-1}$ линейно независимы все вместе. Это даёт необходимые условия на оставшиеся вектора в строке $i$.

\lm Выполнено равенство:
$$J_k(\lambda)^n= \pmat \lambda^n & n\lambda^{n-1} & \dots & C_n^{k-1}\lambda^{n-k+1}\\
 &  \lambda^n & &\vdots \\
 &            & \ddots & n\lambda^{n-1}\\
 &&&  \lambda^n \epmat,$$
\elm
\proof Жорданова клетка представима в виде суммы $J_k(\lambda)= \lambda E + N$, где $N$ -- нильпотентная матрица, причём $N^k=0$. На самом деле степени $N$ выглядят следующим образом:
$$N^l= \pmat  & &0& 1& \dots &0 \\
   & && \ddots &\ddots& \vdots\\
 &&&&\ddots& 1\\
 &&&&& 0 \\
 &&&&&  \epmat $$
Теперь
$$(\lambda E+N)^n= \sum_{i=0}^{k-1} C_n^i\lambda^{n-i}N^i,$$
что и даёт требуемое.
\endproof

\crl Пусть $A$ -- матрица из $M_n(K)$. Тогда существует такая обратимая матрица $C$, что $A^n=CJ^nC^{-1}$, где $J$ -- жорданова форма $A$. Причём, матрица $J^n$ составлена из блоков как в предыдущей лемме.
\ecrl
\proof Достаточно взять матрицу $C$ равной матрице перехода из стандартного базиса в жорданов базис для оператора, заданного $A$.
\endproof

\crl Для всякой матрицы $A$ коэффиент её степени $A^n$ есть сумма последовательностей вида $C_n^s\lambda^{n-s}$ с независящими от $n$ коэффициентами. Здесь $\lambda$ -- произвольное собственное число $A$, а $s$ --  меньше, чем максимальный размер жордановой клетки с собственным числом $\lambda$.
\ecrl



\crl Пусть дана последовательность $x_n\in \mb C$ удовлетворяющая линейному рекуррентному соотношению 
$$x_{n+k}+a_{k-1}x_{n+k-1}+\dots+a_0x_n=0,$$
где $a_i \in \mb C$. Рассмотрим многочлен $f(t)=t^k+a_{k-1}t^{k-1}+\dots+a_0$. Тогда $x_n$ есть сумма последовательностей вида $n^s\lambda^n$, где $\lambda$ -- корень $f(t)$, а $s$ -- строго меньше, чем кратность $\lambda$, как корня $f(t)$.
\ecrl
\proof Как мы помним, матрица оператора сдвига на $V$ -- пространстве последовательностей с заданным линейным рекуррентным соотношением -- имеет характеристический многочлен равный $\pm f(t)$. Пусть $A$ -- матрица оператора сдвига (в стандартном для этого пространства базисе см. следствие \ref{gprog}). Тогда все коэффициенты $A^n$ есть суммы $C_n^s\lambda^{n-s}$.

Если $v$ -- начальный вектор для последовательности, то  нижний элемент столбца $A^nv$ равен  $x_n$. Значит последовательность $x_n$ есть сумма последовательностей вида $C_n^s\lambda^{n-s}$. $C_n^s$ -- это многочлен степени $s$ от $n$. Разложив все такие слагаемые в сумму мономов получаем требуемое.
\endproof

\rm Последовательностей $C_n^s\lambda^{n-s}$ ровно $k$ штук. Любой элемента $k$-мерного пространства лежит в пространстве, порождённом $C_n^s\lambda^{n-s}$. Значит последовательности $C_n^s\lambda^{n-s}$ лежат в $V$ и являются базисом этого пространства.
\erm

\upr Покажите, что $C_n^s\lambda^{n-s}$ -- это жорданов базис оператора сдвига.
\eupr


\thrm
Пусть $L$ -- оператор на векторном пространстве $V$ над полем характеристики $0$. Тогда матрица оператора $p(L)$ в жордановом базисе $L$ составлена из блоков вида
$$ \pmat p(\lambda) & p'(\lambda) & \dots & \frac{p^{(k-1)}(\lambda)}{(k-1)!}\\
 &  p(\lambda) & &\\
 &            & \ddots & \\
 &&&  p(\lambda) \epmat,$$
где $\lambda_i$ собственные числа, а число и размер блоков c $\lambda_i$ равны числу и размеру блоков в жордановой форме $L$.
\proof Благодаря линейности по многочлену, достаточно проверить равенство только для возведения $L$ в степень. 
\endproof
\ethrm

\crl Пусть $A$ матрица, тогда $p(A)=C p(J) C^{-1}$, где $p(J)$ составлена из блоков как выше, а $C$ составлена из жорданового базиса для $A$.
\proof Если $J$ -- жорданова форма для матрицы $A$, то $A=CJC^{-1}$, где матрица $C$ составлена из собственных векторов $A$.
\endproof
\ecrl

Для дальнейшего нам понадобится понятие аналитической функции. Это понятие из математического анализа.


\dfn Пусть $D\subseteq K$ -- открытый диск с центром в точке $z_0$ радиуса $r>0$ в $K$, где $K$ -- это либо $\mb C$, либо $\mb R$ (то есть круг или интервал). Будем говорить, что функция $f\colon D \to K$ аналитична, если существует последовательность $a_n
\in K$, что $f(z)=a_0+a_1(z-z_0)+\dots+ a_n(z-z_0)^n+\dots$ для любого $ z\in D$. 
\edfn

\exm\\
1) $e^z$ на всём $\mb C$ или на всём $\mb R$\\
2) Да и вообще, любая элементарная функция на любом открытом диске в области определения.

\fct Если $f(z)$ аналитична в $D$, то ряд $a_1+2a_2(z-z_0)+\dots+na_n(z-z_0)^{n-1}+\dots$ сходится в $D$ и равен $f'(z)$. То есть производная существует и аналична в том же диске.
\efct

\dfn Пусть $A$ квадратная матрица над $K=\mb C$ или $\mb R$.  Пусть $f(z)$ -- аналитическая функция в диске $D$, а все собственные числа $A$ так же лежат в $D$. Тогда определим 
$$f(A)=a_0+a_1(A-z_0E)+\dots+a_n(A-z_0E)^n+\dots,$$
относительно покоэффициентной сходимости на $M_n(K)$.
\edfn






\rm  В этом случае матрицу $f(A)$ корректно определена и её можно посчитать следующим образом
$$f(A)= C f(J) C^{-1}.$$
\erm

\rm Особенную популярность функция от матриц получает при решении системы линейных однородных дифференциальных уравнений 
$$x'(t)=Ax(t).$$
Решение даётся в виде $e^{At}C_0$, где $C_0$ -- вектор начальных данных. Это корректно определённое выражение так как ряд для экспоненты сходится везде.
\erm

Есть возможность избежать вычисления самой жордановой формы, обойдясь вычислением характеристического многочлена для того, чтобы посчитать функцию от матрицы. Разберёмся со случаем многочлена от матрицы. 

Допустим мы хотим посчитать $p(A)$, где $p(x)$ -- многочлен c коэффициентами $K$. Рассмотрим $\chi_A(t)$. Допустим нам удалось найти остаток $$p(t)=q(t)\chi_A(t)+r(t).$$
Подставим $A$ и воспользуемся теоремой Гамильтона-Кэли. Тогда
$$p(A)=r(A).$$
Например, для вычисления $A^n$ необходимо знать $x^n \mod \chi_A(t)$. Такой подход может дать сокращение в матричных умножениях, когда $n$ велико. Например, если вы хотите вычислить $n$-ый член последовательности, удовлетворяющей линейному рекуррентному соотношению можно посчитать $r(x)\equiv x^n \mod f(x)$, а потом посчитать начальный член последовательности  $r(L)v$, где $L$ -- оператор сдвига. Вычислить степени оператора сдвига легко -- сдвинуть последовательность несколько раз.


\upr Предложите способ вычисления характеристического многочлена. 
\eupr


\utv Пусть $A$ -- вещественная (или комплексная) матрица с собственным числом $\lambda_1=1$ кратности 1, а все остальные собственные числа $A$ по модулю строго меньше 1. Если вектор $v= \sum c_i e_i$, где $e_i$ жорданов базис, то $$\lim_{n \to \infty}A^nv= c_1 e_1.$$
\eutv

Где мы видели такие матрицы?


\section{Неотрицательные матрицы и теория Перрона-Фробениуса}
\dfn

В каких задачах нам может пригодиться наблюдение про предельный переход? Вспомним старые примеры: для каждого графа $G$ можно построить  несколько  различных матриц, которые кодируют его структуру. Прежде всего это три квадратные матрицы  размера $n\times n$, где $n$ -- это число вершин $G$. 
Первая -- матрица смежности  $A(G)$, которая полностью определяет граф $G$
$$a_{ij}=\begin{cases} 1, \text{ если вершины $i$ и $j$ соединены ребром}\\
0, \text{ иначе }
\end{cases}.$$

Так же нам уже встречалась матрица случайного блуждания  $P(G)$

$$P(G)_{ij}=\begin{cases}
\frac{1}{d_j}, \text{ если есть ребро $j\to i$}\\
1, \text{ если $i=j$ и из вершины $j$ не исходит рёбер} \\
0, \text{ иначе }
\end{cases}.$$
\edfn

Выражение  $P_G^n v$ вычисляет распределение после $n$ шагов случайного блуждания, если начальное распределение было равно $v$. След матрицы $A(G)^n$ считает количество циклов длины $n$ в графе $G$. Кроме того, спектр графа не зависит от порядка вершин графа и значит по нему можно указать, что два графа не изоморфны.

Не остановимся на этом. Это, не единственные примеры, где нужно знать собственные числа матрицы. Вот ещё один: модель Лесли для распределения по возрастам в популяции.

Зададимся следующим вопросом как можно промоделировать эволюцию распределения людей по возрастам? Прежде всего необходимо завести разбиение людей на группы $F_i$ -- одного возраста. $F_i$ можно выбрать, например, группой людей с одинаковым годом рождения. Или $F_i$ может быть группой людей, родившихся в определённое десятилетие. Для каждой  группы мы можем посчитать два параметра: $f_i$ -- ожидаемое количество потомства от члена группы $F_i$ за выбранный временной промежуток; $s_i$ -- процент от общего числа индивидов группы $F_i$ которые выжили за фиксированный промежуток времени и перешли в группу $F_{i+1}$. Пусть $n_1,\dots,n_k$ -- количества индивидов в группах $F_1,\dots,F_k$. Тогда для тех же самых количеств но в следующий промежуток времени имеет место место соотношение:
$$\pmat n_1 \\ \vdots \\ n_k \epmat_{new}=
\pmat f_1 & \dots &f_{k-1} & f_k \\ 
s_1 && &\\
& \ddots & \\
& &s_{k-1} & \epmat \pmat n_1 \\ \vdots \\ n_k \epmat$$
Мы представляем себе, что популяция в целом может расти и убывать. Так же логично предположить, что при стабильных условиях (то есть тогда, когда коэффициенты $f_i$ и $s_i$ не зависят от времени) наблюдается некоторое равновесие. Точнее распределение населения по возрастам должно стабилизироваться. Это означает, что определённый процент популяции составляют старики, определённый процент -- дети и т.д.
Что всё это означает на матричном языке? Прежде всего ясно, что речь идёт о предельном поведении  последовательности векторов $A^n v_0$, где $A$ -- это матрица Лесли, а $v_0$ -- начальное состояние. 
Сделаем несколько предположений про матрицу $A$. Первое предположение -- у матрицы $A$ есть положительное вещественное число $\lambda$, которое больше по модулю всех остальных  собственных чисел (над $\mb C$).  Так же, будем считать, что это собственное число $\lambda$ не кратное (речь об алгебраической кратности) и собственный вектор для этого числа можно выбрать с положительными компонентами.

В этой ситуации оказывается, что $\lambda$ -- это есть скорость роста, а соответствующий ему собственный вектор $e_1$ отвечает за распределение популяции. Точнее, пусть $e_1,\dots,e_k$ -- жорданов базис для $A$, $e_1$ -- тот самый собственный вектор для $\lambda$. Пусть вектор $v_0$ имеет столбец координат $x=(x_1,\dots,x_k)^\top$ относительно базиса $e$. Последнее предположение состоит в том, что $x_1$ -- коэффициент при $e_1$ не равен нулю. Если мы хотим узнать координаты $A^n v_0$ в жордановом базисе, то нужно найти произведение 
$$J^n x= \pmat 
\lambda^n &&&\\
& \lambda_2^n & O(n\lambda_2^n)&\\
&& \ddots \\
&&& \lambda_k^n 
\epmat
\pmat x_1 \\ \vdots \\ x_k\epmat$$
По нашему предположению все остальные $\lambda_i$ по модулю меньше $\lambda$. Тогда видно, что первая координата есть $\lambda^n x_1$, а остальные есть $o(\lambda^n)$. Это означает, что $A^n v_0= x_1 \lambda^n e_1 + o(\lambda^n)$. Значит, за один шаг размер популяции в пределе меняется в $\lambda$ раз, а предельное соотношение координат для $A^n v_0$ такое же, как у вектора $e_1$. Такое отношение существует, потому что компоненты $e_1$ все не равны нулю.


Итого для понимания того, как устроена последовательность $A^nv$, необходимо представлять себе как устроены собственные числа матрицы $A$ и её собственные вектора. В связи с этим возникает несколько вопросов:\\
1) Нас интересует максимальное по модулю собственное число. Хочется, чтобы это число было вещественным и положительным. Когда это выполнено?\\
2) Какова алгебраическая кратность максимального по модулю вещественного собственного числа (если оно есть)?\\
3) Есть ли другие собственные числа, равные по модулю максимальному?\\
4) В указанных задачах хотелось бы, чтобы собственный вектор $v$ для максимального собственного числа был положительным? Всегда ли можно так сделать?

Понятно, что в общем случае ответ на эти четыре вопроса <<нет>> даже в рамках поставленных задач.

\exm
\enm
\item Рассмотрим граф $1 \rightarrow 2$. У матрицы $P(G)$ собственный вектор для собственного числа $1$ имеет нулевую координату.
\item Рассмотрим граф 
\begin{center}
\begin{tikzpicture}
\node (A) at (0,0) {3};
\node (B) at (1,0.5) {1};
\node (C) at (1,-0.5) {2};
\path[->,font=\scriptsize,>=angle 60]
(A) edge (B)
(A) edge (C);
\end{tikzpicture}
\end{center}
У матрицы $P(G)$ есть два собственных вектора $(1,0,0)$ и $(0,1,0)$ с собственным числом 1.
\item Рассмотрим граф $C_n$ -- ориентированный цикл длины $n$. Его спектр -- это корни степени $n$ из единицы.
\eenm



Сейчас мы докажем, что при некоторых предположениях на матрицу $A$ для неё ответы на все четыре вопроса имеют желаемый ответ. Эти предположения не будут выполнены для матриц $A(G)$ и  $P(G)$ непосредственно, однако мы тем не менее сможем извлечь пользу. Что же общего между указанными в наших примерах матрицами?

\dfn Назовём матрицу $A$ положительной (не путать с положительно определённой матрицей квадратичной формы), если все её элементы $A_{ij}$ строго положительны. Будем писать в этом случае $A>0$.
\edfn

\dfn Назовём матрицу  $A$ неотрицательной, если $A_{ij}\geq 0$. Обозначение $A \geq 0$.
\edfn



\dfn В дальнейшем нам будут удобны следующие обозначения: если $A\in M_n(\mb C)$, то $|A|$ -- это матрица составленная из $|a_{ij}|$. Про вещественные матрицы $A$ и $B$ будем говорить, что $A>B$ или $A\geq B$, если $A-B>0$ или $A-B \geq 0$ соответственно.
\edfn



\thrm[Перрон, 1907] Если матрица $A$ положительна, то наибольшее по модулю собственное число $A$ единственное и является вещественным и положительным. Это собственное число не является кратным корнем характеристического многочлена. Собственный вектор для этого собственного числа положителен.
\ethrm
\proof Пусть $\lambda$ -- наибольшее по модулю собственное число и $Ax=\lambda x$. Можно считать, что $|\lambda|=1$ разделив всю матрицу на $|\lambda|$. Тогда покажем, что $A|x|=|x|$.

Прежде всего мы имеем цепочку неравенств $|x|=|Ax|\leq |A||x|=A|x|$, где все неравенства подразумеваются покомпонентными. Обозначим за $z=A|x|$. Это вектор состоящий из положительных координат и рассмотрим вектор $y=z-|x|$. Вектор $y$ неотрицателен. При этом если $y=0$, то мы доказали то, что хотели. Предположим, что есть координата $y_i>0$. Тогда $Ay$ -- положительный вектор, то есть существует такое $\eps>0$, что $Ay>\eps z$. Распишем это неравенство: $Az - z= Az-A|x|> \eps z$ или же $\frac{A}{1+\eps}z>z$. Ввиду положительности правой и левой части мы без сомнений можем применить оператор $\frac{A^n}{(1+\eps)^n}$ к правой и левой части и получить верное неравенство. Итого имеем цепочку 
$$\frac{A^n}{(1+\eps)^n}z>\frac{A^{n-1}}{(1+\eps)^{n-1}}z> \dots > z.$$
Но оператор $\frac{A}{1+\eps}$ имеет собственные числа по модулю меньшие 1 и поэтому, предел левого выражения равен 0. Противоречие!

Итак, в частности, единица собственное число. Покажем теперь, что нет отличных от единицы собственных чисел с модулем $1$. Пусть $\lambda\in \mb C$ собственное число $A$ с $|\lambda|=1$ и $x \in \mb C^n$ -- соответствующий собственный вектор. Тогда $A|x|=|x|=|Ax|$. Заметим, что все координаты $x$ отличны от нуля. Рассмотрим $i$-ую координату. Имеем $\sum A_{ij}|x_j|=x_i=|\sum A_{ij}x_j|$. Посмотрим на это равенство как на равенство между нормой суммы и суммой норм векторов в $\mb C =\mb R^2$. Хорошо известно, что сумма норм больше или равна нормы суммы и равенство достигается тогда и только тогда, когда вектора сонаправлены. Итого все $x_i$ должны быть сонаправлены, но это означает, что $x=e^{i\ffi} |x|$ и следовательно $\lambda=1$. 

Покажем, что единица не кратный корень. Допустим противное. Есть два случая -- либо есть две клетки с собственным числом $1$, либо клетка ровно одна, но при этом размера по крайней мере $2$. Если есть две клетки, то есть два линейно независимых собственных вектора  $x_1$ и $x_2$. Тогда подберём $c$, так что $x_1-cx_2$ имеет нулевую координату. Получаем противоречие, так как $|x_1-cx_2|$ неотрицательный вектор для 1, но при этом с нулевой координатой. Осталось разобрать случай с клеткой размера $k$. В этом случае все коэффициенты матрицы $A^n$ имеют вид $cn^{k-1}+o(n^{k-1})$. При этом $c$ не всегда $0$. Значит некоторые коэффициенты $A^n$ растут при $n\to \infty$. Но тогда некоторые коэффициенты $A^nx_1=x_1$ тоже растут, что очевидно не так.
\endproof

Бывает полезно ещё одно утверждение, которое позволяет установить максимальность собственного числа для заданной неотрицательной матрицы. Оказывается, что для этого достаточно положительности собственного вектора. Нам удобно будет сформулировать и уточнить это соображение не для матрицы $A$, а для матрицы $A^\top$.

\utv Пусть $A\geq 0$, и у матрицы $A^{\top}$ есть положительный собственный вектор для собственного числа $\lambda$. Тогда $\lambda$ -- наибольшее по модулю собственное число $A$. Если у матрицы $A$ есть собственный вектор $y\geq 0$, то $y$ собственный вектор для числа $\lambda$. 
\eutv
\proof Рассмотрим матрицу $A^{\top}$. Пусть $x$ -- положительный  собственный вектор, соответствующий собственному числу $\lambda$. Пусть $\mu$ -- собственное число для собственного вектора $v$. Тогда 
$$\lambda x^{\top}|v|= x^{\top}A|v|\geq x^{\top}|\mu| |v|=|\mu| x^{\top}|v|.$$
Так как $x^{\top}|v| >0$, то $\lambda\geq |\mu|$. Если же, $y=v=|v|$, то имеет место равенство. Так как для неотрицательного вектора $y$ собственное число $\mu$ тоже неотрицательно
\endproof

\dfn Неотрицательная матрица $A\in M_n(\mb R)$ называется стохастической (по столбцам),  если сумма всех коэффициентов в каждом её столбце равна $1$ (иногда дефолтным считается условие на строки). 
\edfn

\rm Такие матрицы встречаются в теории вероятностей, когда речь идёт о марковских цепях -- процессах с дискретным временем в которых последующие события зависят только от текущего положения, а не от того, как вы в него попали.  
\erm

\crl У стохастической матрицы $A$ единица является максимальным по модулю собственным числом.
\ecrl
\proof Вектор $(1,\dots,1)$ является положительным собственным вектором для $A^{\top}$ с собственным числом $1$. Значит у матрицы $A$ есть собственное число $1$ и оно максимальное. 
\endproof


Вообще говоря матрица $P(G)$ имеет довольно много нулевых компонент. И, строго говоря, заключение теоремы Перрона не всегда верно для $P(G)$, как следует из примеров. Как же наша теория может помочь? Для этого мы схитрим и немного поменяем задачу. А именно, рассмотрим матрицу $$P_{\alpha}(G)=(1-\alpha) P(G) + \alpha\tfrac{1}{n}J_n,$$
где $J_n$ -- матрица из одних единиц, а $\alpha \in (0,1)$. Тогда матрицы $P_{\alpha}(G)$ являются положительными. С точки зрения блуждающего пользователя это означает, что у него есть два режима -- первый, в котором он находится с вероятностью $1-\alpha$ -- это режим брожения по ссылкам, а второй режим -- переход на случайную страницу. Для матрицы $P_{\alpha}(G)$ выполнены условия теоремы и поэтому она имеет единственное не кратное максимальное собственное число, равное $1$. Соответствующий собственный вектор можно выбрать положительным.


То, что у $P_{\alpha}(G)$ все собственные числа по модулю меньше единицы означает, что $P_{\alpha}(G)^kv \to cx$, при $k \to \infty$, где $x$ -- положительный вектор с собственным числом равным 1. Это позволяет приближённо найти $x$, что даёт желаемое распределение весов. Практически, для этого можно взять $k\sim \log n$. Количество точных знаков линейно зависит от $k$. Это позволяет заметно сэкономить на вычислениях по сравнению с теоретическим нахождением собственных векторов. Изучая предел $P_{\alpha}(G)$ при $\alpha \to 0$ можно получить информацию и про исходную матрицу.


Можно ли тем не менее что-то сказать в случае неотрицательных матриц? Ответ на этот вопрос дал Фробениус.

\dfn Пусть $A$ -- неотрицательная вещественная матрица размера $n$. Свяжем с матрицей $A$ ориентированный граф $G$ (возможно с петлями). Вершины $G$ есть числа от $1$ до $n$, а ребро между $j\to i$ есть только в том случае, когда коэффициент $A_{ij}\neq 0$. 
\edfn

\dfn Неотрицательная матрица $A$ называется неприводимой, если связанный с ней граф сильно связен.
\edfn

\rm Это равносильно тому, что нельзя так перенумеровать координаты, чтобы в новых координатах матрица $A$ имела блочно-верхнетреугольный вид 
$$\pmat B & C \\ 0 & D \epmat .$$
\erm

\dfn Неотрицательная матрица $A$ называется эргодической или перемешивающей, если существует такая степень $k$, что $A^k>0$. Ещё такие матрицы называют примитивными.
\edfn

\rm Элемент с индексами $i,j$ матрицы $A^k$ не равен нулю только если в графе $G(A)$ есть путь из $i\to j$ длины ровно $k$. В частности, это означает, что эргодическая матрица неприводима. Так  же, понятно, что если матрица $A^k>0$, то это выполнено и для больших степеней.
\erm

На самом деле оба эти понятия ещё более тесно связаны между собой. 

\lm Пусть $A$ -- неотрицательная неприводимая матрица размера $n$. Тогда для любого $\eps>0$  матрица $A+\eps E$ эргодическая.
\elm
\proof Заметим, что  в графе $G(A)$ есть пути $j\to i$ между любыми двумя вершинами. Их длину можно ограничить числом $n-1$. Граф $G(A+\eps E)$ отличается от $G(A)$ только тем, что около каждой его вершины есть петля. Но наличие петель в каждой вершины позволяет из пути длины $k\leq n-1$ сделать путь длины ровно $n-1$. Значит $(A+\eps E)^{n-1}>0$, что и требовалось.
\endproof



\thrm[Фробениус, 1912] Пусть $A$ -- эргодическая матрица. Тогда у $A$ есть единственное максимальное по модулю собственное число  $\lambda$ и оно вещественно и положительно. Кроме того, число $\lambda$ не является кратным для $A$. Числу  $\lambda$ соответствует положительный собственный вектор.
\ethrm
\proof Пусть $\mu\in \mb R_{>0}$ -- наибольшее собственное число для $A^k$, а $v$ -- соответствующий ему собственный вектор. Тогда 
$$\mu Av= A\cdot A^k v= A^k\cdot A v.$$
Откуда получаем, что $Av$ либо $0$, либо собственный вектор $A^k$ с тем же собственным числом $\mu$. Нулём вектор $Av$ быть не может. Тогда нулём был бы и $A^kv=\mu v$. Значит, это собственный вектор для $A^k$ c тем же $\mu$. Значит он пропорционален $v$, то есть $Av=\lambda v$. Так как $v$ вещественный положительный, матрица $A$ неотрицательна, то $\lambda>0$. То есть $v$ -- положительный собственный вектор для собственного числа $\lambda>0$. 

Заметим, что, $\lambda^k=\mu$. Кроме того, если $\lambda_1,\dots,\lambda_n$ -- все собственные числа матрицы $A$, выписанные с учётом кратности, то $\lambda_1^k,\dots,\lambda_n^k$ -- все собственные числа для $A^k$ с учётом кратности. 

В частности, отсюда следует, что у матрицы $A$ не может быть никаких собственных чисел по модулю равных  $\lambda$ и при этом отличных от $\lambda$. Посмотрев на кратности так же можно понять, что $\lambda$ -- не кратное для $A$ число. 
\endproof

\crl Пусть $A$ -- неприводимая матрица. Тогда у $A$ есть вещественное собственное число $\lambda>0$, которое больше или равно всех остальных собственных чисел по модулю. Это собственное   число не кратно. Соответствующий собственный вектор можно выбрать положительным. 
\ecrl
\proof Матрица $A+\eps E$ эргодическая и значит у неё есть положительный собственный вектор $v$, соответствующий собственному числу $\lambda_{\eps}>0$. Тогда $v$ -- собственный вектор матрицы $A$ с собственным числом $\lambda_{\eps}-\eps$. Так как $v$ положительный, а матрица $A$ неотрицательна, то $\lambda_{\eps}-\eps>0$.

Покажем, что $\lambda_{\eps}-\eps$ не зависит от $\eps$. Действительно, если бы у матрицы $A$ было бы другое вещественное собственное число, большее $\lambda_{\eps} - \eps$ (могло взяться от другого $\eps'$), то это дало бы собственное число матрицы $A+\eps E$, большее $\lambda_{\eps}$, чего не может быть. Обозначим за $\lambda=\lambda_{\eps}-\eps$.   Посмотрим на другое собственное число $\mu$ матрицы $A$. Тогда $\mu+\eps$ -- собственное число матрицы $A+\eps E$. Мы знаем, что $\lambda >|\mu+\eps|$. Это выполнено для всех $\eps$. Значит $\lambda \geq |\mu|$. Завершая доказательство отметим, что если $\lambda$ -- было бы кратным собственным числом для $A$, то $\lambda_{\eps}$ было бы кратным для $A+\eps E$, чего не может быть.
\endproof

\rm Если величины $f_i$ при $1<i\leq k$ и $s_i$ при $1\leq i<k$ в матрице Лесли положительны, то матрица Лесли эргодическая. Если, скажем, все $s_i>0$ и $f_k>0$, то матрица Лесли, по крайней мере, неприводима.

Если граф $G$ сильно связен, то матрица $P_G$ неприводима. А что на языке графов означает эргодичность матрицы $P_G$?
\erm


\upr Покажите, что если $A$ неотрицательная матрица, то у $A$ есть вещественное собственное число $\lambda\geq 0$, которое больше или равно всех остальных собственных чисел по модулю. Для него есть неотрицательный собственный вектор. Как видно из примеров, большего требовать не получится.
\eupr


\section{Дополнительно: модель Леонтьева}
Модель затраты-выпуск Леонтьева. Предположим, что у нас есть несколько типов товаров. Для выпуска каждого товара нужно вложить сколько-то товаров из представленных типов. Например, для производства угля нужна сталь (для стальных инструментов), а для производства стали нужен уголь и сама сталь (в виде инструментов для добычи железной руды). 

Пусть числа $a_{ij}$ характеризуют, сколько нужно взять единиц $j$-го товара для производства товара $i$. Обозначим за $A$ матрицу, составленную из $a_{ij}$. Если $x$ -- это совокупный выпуск всех товаров за фиксированный промежуток времени. Назовём $x$ вектором производств. Пусть $y$ -- это вектор итогового выпуска товаров для внешнего рынка (то, что не нужно для внутреннего производства). Тогда $y$ и $x$ будут связаны уравнением
$$x=y+Ax.$$
Из этого соотношения легко найти $x=(E-A)^{-1}y$. Таким образом, если мы хотим произвести для внешнего рынка $y$ товаров, то для этого совокупно надо произвести $x$ товаров. 

Модель называется продуктивной, если по любому запросу $y\geq 0$ можно предъявить подходящий вектор  производств $x$. При этом вектор $x$ должен быть неотрицательным. 

Продуктивность модели равносильна неотрицательности матрицы $(E-A)^{-1}$. Как можно переформулировать это условие? Заметим, что $\frac{1}{1-x}$ -- аналитическая функция в диске $|z|<1$. Поэтому, если $\lambda$ -- максимальное собственное число $A$ меньше $1$, то $$(E-A)^{-1}=E+A+A^2+\dots +A^n+\dots$$
является неотрицательной матрицей. Если $\lambda=1$, то $E-A$ просто не обратима. Если $\lambda>1$, то $E-A$ может быть необратимой, а может иметь обратную. Покажем, что тем не менее это не приводит к продуктивной модели. Заметим, что у $A$ есть неотрицательный собственный вектор с собственным числом $\lambda$. Но тогда $v$ -- собственный вектор и для $(E-A)^{-1}$ c собственным числом $\frac{1}{1-\lambda}<0$. Но это значит, что матрица $(E-A)^{-1}$ точно не положительная, так как домножив её на $v$ мы получили вектор с отрицательными компонентами.

\section{Дополнительно: метод итераций и оценки на собственные числа}

Как найти приближённо или оценить максимальное по модулю собственное число матрицы. На самом деле, мы уже видели ответ: необходимо возвести матрицу в степень. Точнее: пусть $A$ -- вещественная матрица, которая имеет единственное максимальное по модулю собственное число $\lambda$ (не кратное). Пусть $e_1,\dots,e_n$ -- жорданов базис для $A$, а $e_1$ -- соответствует собственному числу $\lambda$. Рассмотрим случайный вектор $v\in \mb R^n$. С вероятностью $1$ его $e_1$ компонента не $0$. Тогда $A^nv \sim \lambda^n v$. Пусть $x_1(n)$ -- первая компонента $A^nv$. Тогда, если первая компонента у вектора $e_1$ не ноль, то $$\frac{x_1(n)}{x_1(n-1)} \to \lambda.$$
Для оценки скорости сходимости хотелось бы иметь представление об отношении  $\frac{\lambda}{|\lambda_2|}$.

\begin{comment}

Похожая технология применяется, когда необходимо приближённо найти решение системы $x=Ax+b$, где $A$ -- квадратная матрица. Точнее, пусть собственные числа $A$ по модулю меньше $1$. Возьмём случайный вектор $x_0$ и построим последовательность $$x_i=Ax_{i-1}+b.$$
Покажем, что при указанных условия, эта последовательность стремится к решению системы. Прежде всего заметим, что матрица $E-A$ обратима, и, следовательно, система имеет единственное решение. Пусть это решение --- $x'$. Обозначим $h_i=x_i-x'$. Для $h_i$ получается соотношение:
$$x'+h_i=Ax'+Ah_{i-1}+b.$$
Откуда получаем, что $h_i=Ah_{i-1}$. Но тогда $h_i$ стремятся к нулю. Значит $x_i\to x'$.

\end{comment}

Можно ли как-то из теоретических соображений оценить собственные числа матрицы? Оказывается, что такой способ есть. Пусть $A$ -- вещественная или комплексная квадратная матрица. Рассмотрим строки $A$. Зададим элементы $r_i=\sum_{j\neq i} |a_{ij}|$. Рассмотрим множество замкнутых кругов радиуса $r_i$ с центром в $a_{ii}$ на комплексной плоскости. Эти круги называются кругами Гершгорина. 

\utv Все собственные числа матрицы $A$ лежат в объединении кругов Гершгорина. 
\eutv
\proof Пусть $\lambda$ -- собственное число $A$, а $v$ -- соответствующий собственный вектор. Тогда $Av=\lambda v$. Посмотрим на максимальный по модулю элемент $v$ -- $v_i$ и соответствующую ему строчку. Покажем, что $\lambda$ лежит в круге Гершгорина с номером $i$. Имеем $\sum a_{ij}v_j =\lambda v_i$  и, значит, $\sum_{j\neq i} a_{ij}v_j= (\lambda - a_{ii})v_i $. Имеем неравенство $$|\lambda -a_{ii}||v_i| \leq |v_i|\sum_{j\neq i} |a_{ij}|$$
Сокращая на $v_i$ получаем требуемое. 
\endproof

\rm В частности, это рассуждение даёт критерий невырожденности матрицы -- матрица $A$ невырождена, если сумма модулей её внедиагональных элементов каждой строки меньше модуля диагонального элемента. Такие матрицы называются матрицами с диагональным преобладанием
\erm

\rm
Переходя от матрицы к транспонированной можно получить ещё одну оценку, которая получается из рассмотрения столбцов $A$.
\erm


\chapter{Полилинейная алгебра}

\section{Билинейные формы}

Одним из основных примеров нормы на векторном пространстве над $\mb R$ является корень из суммы квадратов координат $\|x\|=\sqrt{x_1^2+\dots +x_n^2}$. Однако вместе с такой нормой <<в комплекте>> идёт отображение от двух переменных $(x,y) \to \sum x_iy_i$, которое называется скалярным произведением. Оно обладает свойством билинейности, то есть линейности по каждой координате. Наша задача разработать единый подход к таким объектам над произвольным полем и разобраться с самыми важными примерами.

\dfn Пусть $V$ -- векторное пространство над $K$. Отображение $h\colon V\times V \to K$ называется билинейной формой, если\\
1) $\forall \lambda \in K$ $\forall u,v,w \in V$ верно, что $h(u+\lambda v, w) = h(u,w)+\lambda h(v,w)$,\\
2) и по второй координате: $h( w, u+\lambda v) = h(w,u)+\lambda h(w,v)$.
\edfn

\exm\\
1) Рассмотрим пространство $K^n$ и определим на нём билинейную форму $h(u,v)=\sum_{i=1}^n x_iy_i$.\\
2) Пусть $A\in M_{n}(K)$. Тогда на $K^n$  можно задать билинейную форму $(x,y)\to x^{\top}Ay$.\\
3) Рассмотрим пространство многочленов $\mb R[x]$ или пространство непрерывных функций на промежутке $[a,b]$, а так же какую-нибудь непрерывную функцию $w(x)$ и зададим билинейную форму $h(f,g)=\int_a^b f(x)g(x)w(x)dx$. Часто  в качестве промежутка выступает вся вещественная прямая, а в качестве веса выбирается $w(x)=\frac{1}{\sqrt{\pi}}e^{-x^2/2}$. Обычно, такой вес возникает, когда речь идёт о нормально распределённых случайных величинах.\\
4) Рассмотрим пространство один раз непрерывно дифференцируемых функций на отрезке $C^1[a,b]$ и введём на нём билинейную форму по правилу $h(f,g)=\int_a^b f'(x)g(x)dx$.\\
5) Рассмотрим пространство матриц $M_n(K)$ и введём на нём билинейную форму $h(A,B)= \Tr(AB)$.\\
6) Рассмотрим конечномерную алгебру $A$ над полем $K$. Тогда любой элемент $a\in A$ задаёт линейное отображение $L_a \colon A \to A$, переводящее $x\to ax$. У этого линейного отображения есть след. Для простоты обозначим его как $\tr a$.
Тогда отображение $A\times A \to K$ заданное по правилу $\tr_{A/K}(u,v)=\tr(uv)$ является билинейной формой. Замечу, что конструкция из предыдущего пункта не является частным случаем этой, а отличается на константу.\\

Как обычно, поведение объекта линейной алгебры определяется его взаимодействием с каким-либо базисом.

\dfn Пусть $e_1, \dots, e_n$ базис $V$, а $h$ -- билинейная форма на $V$. Тогда матрица $A$ составленная из элементов $h(e_i,e_j)$ называется матрицей билинейной формы.
\edfn

\lm Пусть $V$ -- пространство с выбранным базисом $e_1,\dots,e_n$. Тогда имеет место взаимооднозначное соответствие между билинейными формами $h$ на $V$ и матрицами  $A\in M_n(K)$. В частности, если вектор $v$ имеет столбец координат $x$, а вектор $u$ -- столбец $y$, то значение $h(u,v)$ можно найти по формуле $y^{\top}Ax$.
\elm
\proof Пусть $u=\sum x_i e_i$  и $v=\sum y_ie_i$. Тогда $h(u,v)=\sum x_i h(e_i,v)=\sum x_iy_j h(e_i,e_j)=x^{\top}Ay$.
\endproof

\lm Пусть $V$ -- пространство с билинейной формой $h$ и базисом $e_1,\dots,e_n$. Пусть матрица $h$ в этом базисе -- это $A$. Если выбрать другой базис $f$ с матрицей перехода $C$, то в новом базисе матрица $A$ будет иметь вид 
$A'=C^{\top}AC.$
\elm
\proof Распишем: $x^{\top}Ay=(Cx')^\top A Cy'= {x'}^{\top}C^{\top}AC y'= {x'}^\top A' y'$. Тогда матрицы $C^{\top}AC$ и $A'$ равны.
\endproof



\dfn Ранг билинейной формы -- это ранг её матрицы. 
\edfn

\dfn Будем говорить, что элемент $u$ ортогонален (слева)  элементу $v$, если $h(u,v)=0$, и записывать это как $u\bot v$. 
\edfn

\dfn Билинейная форма $h$ называется невырожденной, если $\forall v \neq 0$ существует $u \in V$, что $h(u,v)\neq 0$.
\edfn

\utv Билинейная форма невырождена тогда и только тогда, когда её матрица в некотором базисе невырождена.
\eutv
\proof Пусть $A$ -- матрица билинейной формы $h$ в некотором выбранном базисе. Пусть $A$ -- невырождена. Проверим условие для формы $h$ в координатах. Если столбец $x\neq 0$, то столбец $Ax$ ненулевой. Значит, есть столбец $y$ (с одной единицей на подходящей позиции), что $y^\top Ax\neq 0$. Но это и есть свойство невырожденности для $h$ в координатах. Обратно, если $h$ невырождена, что для любого столбца $x\neq 0$ можно подобрать столбец $y$, что $y^{\top}Ax\neq 0$. Значит $Ax\neq 0$, для любого $x\neq 0$. Значит $\Ker A=0$ и $A$ -- невырождена.
\endproof

\dfn[Ортогональное дополнение справа] Пусть $h$ -- билинейная форма на $V$. Если $U$ -- подпространство $V$, то определим множество $$U^{\bot}=\{v\in V\,|\, \forall u \in U \, \, u\bot v\}.$$ 
Это множество называется правым ортогональным дополнение к $U$ (внутри $v$ относительно $h$). Аналогично можно говорить про ортогональное дополнение слева. Оно обозначается как ${}^\bot U$
\edfn

\rm Если $e_1,\dots,e_k$ базис $U$, то условие $v\in U^{\bot}$ равносильно $e_i\bot v$ по всем $i$.  
\erm

Что можно ожидать от ортогонального дополнения относительно билинейной формы? Можем ли мы что-то сказать про его размерность?

\utv Пусть $U$ -- подпространство $V$, $h$ -- билинейная форма на $V$. Тогда $\dim U^{\bot}\geq \dim V - \dim U$.
Если форма $h$ невырождена, то имеет место равенство $\dim U^{\bot}= \dim V- \dim U$ и верно, что ${{}}^\bot(U^{\bot})=U$.
\proof Если $e_1,\dots,e_k$ базис $U$, то принадлежность $v$ ортогональному дополнению задаётся  $k$ уравнениями $h(e_i,v)=0$. Отсюда немедленно вытекает неравенство. Равенство выполнено, если матрица для указанной системы (в каком-нибудь базисе) имеет ранг ровно $k$. Давайте проверим, что для невырожденной формы это имеет место. Дополним набор векторов $e_1,\dots,e_k$ по базиса $V$ и распишем систему уравнений на координаты $x$ вектора $v$ в этом базисе. Она имеет вид:
$$\pmat E_k & 0 \epmat \cdot A x=0$$
Форма невырождена, поэтому матрица $A$ обратима. Ранг правой матрицы равен $k$. При домножении на обратимую матрицу ранг не меняется. Итого ранг равен $k$. 

Для того, чтобы доказать последнее утверждение заметим, что $U\leq {U^{\bot}}^{\bot}$. Но размерность ${}^{\bot}(U^{\bot})=n-(n-k)=k=\dim U$. Откуда получаем равенство.
\endproof 
\eutv

Можно поставить вопрос: при каком условии на подпространство $U\leq V$ пространство $V$ раскладывается в виде $U\oplus U^{\bot}$?


\utv Пусть $U\leq V$  и $h$ --  билинейная форма на $V$ тогда $V=U\oplus U^{\bot}$ тогда и только тогда, когда $h|_{U}$ невырождена.
\proof Проверим, что $U \cap U^{\bot}= \{0\}$. Пусть это не так и есть такой $0\neq x \in U$, что $x \in U^{\bot}$. Но тогда $\forall y \in U$ $h(y,x)=0$ так как $x\in U^{\bot}$, что противоречит невырожденности ограничения. Значит вместе они порождают подпространство размерности по крайней мере $\dim U+\dim U^{\bot}$. Но, как мы знаем $\dim U +\dim U^{\bot} \geq \dim V$. Значит имеет место равенство и, следовательно, пространство раскладывается в прямую сумму. 

Обратно, если $V=U \oplus U^{\bot}$, то $x\in \Ker h|_{U}$ лежит одновременно в $U$ и в $U^{\bot}$, что противоречит определению прямой суммы.
\endproof
\eutv

\rm Даже в случае невырожденных форм условие, что $h|_U$ невырождена выполнено далеко не всегда. Рассмотрим билинейную форму на $\mb R^2$, заданную  матрицей 
$$ \pmat 0& 1\\ 1& 0\epmat.$$
Её ограничения на первую ось $\lan e_1\ran$ имеет нулевую матрицу. Несложно вычислить и ортогональное дополнение к $\lan e_1 \ran $ -- это само это пространство. Разложения в прямую сумму точно нет.
\erm


\dfn В случае, если пространство $V$ разложилось в виде прямой суммы подпространств $V=U\oplus U'$, таких, что $ U'\leq U^{\bot}$, то будем говорить, что имеет место разложение в ортогональную сумму подпространств  $V=U\oplus^{\bot} U'$.
\edfn

\rm Если форма $h$ невырождена, то для данного подпространства $U$ может найтись не более одного пространства $U'$, что $V=U\oplus^{\bot} U'$ -- ортогональная прямая сумма. А именно, $U'=U^{\bot}$.
\proof Если сумма ортогональная, то $U' \leq U^{\bot}$. Осталось заметить, что их размерности должны быть равны. 
\endproof
\erm

 


















\section{Симметричные билинейные формы и квадратичные формы}
Наибольший интерес среди билинейных форм вызывают формы со специальными свойствами. В этом разделе речь пойдёт про симметрические формы, хотя мы вскользь обсудим и кососимметричные формы. 



\dfn Билинейная форма $h$ называется симметричной, если $h(u,v)=h(v,u)$. Форма $h$ называется кососимметричной, если $h(u,v)=-h(v,u)$(это неправильное определение в случае $\chr k=2$).
\edfn

Для симметричных и кососимметрических билинейных форм условие, что $x\bot y$ и $y\bot x$ совпадают. Это позволяет говорить про ортогональное дополнение, не упоминая, с какой стороны мы его берём. Кроме того изучение билинейной формы можно свести (хотя и не самым удобным образом) к изучению симметричных и антисимметричных форм благодаря замечанию:

\rm Любая билинейная форма $h$  над полем, характеристика которого отлична от $2$ может быть единственным образом представлена в виде суммы $h^+$ и $h^-$, где $h^+$ -- симметрическая форма, а $h^-$ -- кососимметрическая. ($h^+(u,v)=\frac{h(u,v)+h(v,u)}{2}$, $h^-(u,v)=\frac{h(u,v)-h(v,u)}{2}$)
\erm

\lm  Билинейная форма $h$ симметрична тогда и только тогда, когда её матрица в некотором базисе симметрична, то есть $A^{\top}=A$ и кососимметрична, если $A^{\top}=-A$.
\elm
\proof Разберём случай, когда $h$ симметрична. Пусть $e_1,\dots,e_n$ -- некоторый базис. Тогда $$A_{ij}=h(e_i,e_j)=h(e_j,e_i)=A_{ji},$$
Что и означает симметричность. Обратно, пусть $x=[u]_e$, а $y=[v]_e$. Тогда $$h(u,v)=x^{\top}Ay=(x^\top A y)^{\top}=y^{\top}A^{\top} x=y^\top Ax=h(v,u).$$
\endproof

\exm\\
1) Если матрица $A$ -- симметричная, то билинейная форма $x^{\top}Ay$ на $K^n$ -- симметричная. В частности, обычное скалярное произведение -- симметричная билинейная форма.\\
2) Отображение $(f,g) \to \int_a^b f(x)g(x)\omega(x)dx$ является симметричной билинейной формой на $\mb R[x]$.\\
3) Если посмотреть на пространство $V$, состоящее из функций $f\in C^1([a,b])$, что $f(a)=f(b)=0$, то форма $f,g \to \int_a^b f'g dx$ является кососимметричной. Причина -- интегрирование по частям.\\


У нас больше не будет идти речи про билинейные формы общего вида, а только про симметричные формы. Предложим теперь альтернативный взгляд на симметричные билинейные формы. {\color{red} Внимание!} С технической точки зрения удобно предполагать, что характеристика поля, над которым мы работаем отлична от $2$. Сохраним это предположение до конца раздела про билинейные формы.

\dfn Квадратичная форма -- это отображение $q\colon V \to K$, такое, что в некоторой линейной системе координат это отображение есть однородный многочлен степени 2, то есть имеет вид $\sum_{i\leq j} b_{ij} x_i x_j$. Матрицей квадратичной формы в указанной системе координат называется матрица $$a_{ij}=\begin{cases} b_{ii}, \text{ если $i=j$},\\
\frac{b_{ij}}{2}, \text{ если $i\neq j$}.
\end{cases}.$$
Если вектор $v$ имеет столбец координат $x$, то $q(v)=x^{\top}Ax$.
\edfn

Матрица $A$ квадратичной формы -- это единственная симметричная матрица, что $q(v)=x^{\top} A x$. Действительно, единственный способ решить уравнения $b_{ij}=a_{ij}+a_{ji}$ и $a_{ij}=a_{ji}$  есть $a_{ij}=a_{ji}=\frac{b_{ij}}{2}$.
Таким образом, при выборе базиса возникает взаимооднозначное соответствие 
$$\text{ Симметричные билинейные формы } \leftrightarrow \text{ Симметричные матрицы } \leftrightarrow \text{ Квадратичные формы } $$
Покажем, что соответствие симметричных билинейных и квадратичных форм не зависит от выбора системы координат. Для этого достаточно предъявить бескоординатные формулы для этого соответствия.


\utv Пусть $h$ -- билинейная симметричная форма на $V$. Тогда $q(v)=h(v,v)$ -- это квадратичная форма. При этом, в любой системе координат матрица $q$ есть $A$ -- матрица $h$.
\proof $q(v)=h(v,v)= x^{\top}Ax$. Матрица $A$ -- симметричная и, следовательно, она и есть матрица для квадратичной формы $q$.
\eutv




\rm
Пусть $q$ -- квадратичная форма. Тогда форма $h(u,v)=\frac{q(u+v)-q(u)-q(v)}{2}$ -- симметричная билинейная. Эта конструкция обратна к конструкции из предыдущего факта. В этом случае, форма $h$ называется поляризацией квадратичной формы $q$.
\erm 


\dfn Квадратичная форма невырождена, если соответствующая ей симметричная билинейная форма невырождена.
\edfn

\subsection{Ортогонализация и метод Лагранжа}

\dfn Пусть $h$ -- симметричная билинейная форма на $V$, тогда система векторов $e_1,\dots,e_k$ называется ортогональной, если $h(e_i,e_j)=0$, при $i\neq j$. Если указанная система векторов является базисом, то такой базис называют ортогональным.
\edfn

Например, стандартный базис в $\mb R^n$ ортогонален относительно билинейной формы $x,y \to x^\top y$.

\rm Матрица симметричной билинейной формы в ортогональном базисе имеет диагональный вид, а выражение для квадратичной формы есть сумма квадратов координат вектора с коэффициентами $\sum \lambda_i x_i^2$.
\erm

\dfn Будем говорить, что симметрические билинейные (или квадратичные) формы эквивалентны, если в некоторых базисах они имеют одинаковые матрицы.
\edfn

Вопросы: всегда ли можно найти ортогональный базис и насколько форма матрицы зависит от выбора ортогонального базиса? На первый вопрос ответ положительный.

\thrm Пусть $V$ -- пространство с симметричной билинейной формой $h$. Тогда в $V$ существует ортогональный относительно $h$ базис. 
\ethrm
\proof
Если пространство $V$ одномерно или $h=0$, то подойдёт просто любой базис. Пусть $h$ -- не ноль. Тогда существует вектор $e_1$, что $h(e_1,e_1)=q(e_1)\neq 0$, потому что форма $q$ не нулевая. Теперь $h|_{\lan e_1\ran}$ невырождена и следовательно $V=\lan e_1 \ran \oplus \lan e_1 \ran^{\bot}$. По индукционному предположению на пространстве $\lan e_1 \ran^{\bot}$ есть ортогональный базис $e_2,\dots,e_n$. Тогда подходящий базис -- это $e_1,\dots,e_n$. 
\endproof







Обсудим алгоритм который стоит за этим доказательством. Для это удобнее будет работать с формой $q$ и представлять её в виде однородного многочлена второй степени. После такого отождествления описанный алгоритм можно условно назвать выделением полного квадрата. Пусть форма $q(x)$ в координатах имеет вид
$$q(x)= a_{11}x_1^2+ 2a_{12}x_1x_2 + \dots + 2a_{1n}x_1x_n  + q'(x_2, \dots, x_n).$$

\noindent{\bf Первый случай} Предположим, что $a_{11}\neq 0$. Тогда представим $q(x)$ в виде, выделив полный квадрат 
$$q(x)= a_{11}\left(x_1+\frac{a_{12}}{a_{11}}x_2 + \dots +\frac{a_{1n}}{a_{11}}x_n\right)^2 - \frac{a_{12}^2}{a_{11}}x_2^2 - 2\frac{a_{12}a_{13}}{a_{11}}x_2x_3 - \cdots - \frac{a_{1n}^2}{a_{11}}x_n^2 + q'(x_2,\dots,x_n).$$

Новые переменные выглядят следующим образом:
\begin{align*}
y_1&=x_1+\frac{a_{12}}{a_{11}}x_2 + \dots +\frac{a_{1n}}{a_{11}}x_n,\\
 y_2&=x_2, \\
&\vdots\\
 y_n&=x_n.
\end{align*}
Или в подходящую сторону
$$ \pmat x_1 \\ x_2 \\ \vdots \\ x_n \epmat = \pmat 1 & -\frac{a_{12}}{a_{11}} & \dots & -\frac{a_{1n}}{a_{11}} \\
& 1 && \\
& & \ddots & \\
&&& 1
\epmat \pmat y_1 \\ y_2 \\ \vdots \\ y_n \epmat.
$$

Видно, что кроме формы $q'$ возникает ещё поправка, которая содержит слагаемые $\lambda x_ix_j$ $i,j\geq 2$. Таким образом мы обнулили $a_{1j}$ и сделали первый вектор новой системы координат ортогональным остальным, как и в доказательстве. Заметим так же, что указанное преобразование над матрицей эквивалентно одновременному применению одинаковых элементарных преобразований строк и столбцов.\\




\noindent{\bf Второй случай.} $a_{11}=0$. Если $a_{ii}\neq 0$, то меняем первую и $i$-ую координаты местами  и продолжаем как раньше. \\


\noindent{\bf Третий случай.} Все $a_{ii}=0$. Если есть такое $i$, что $a_{1i}\neq 0$, то можно перенумеровать координаты, чтобы $i$ стало равно $2$. Пусть $a_{12}\neq 0$. Тогда сделаем замену $x_1=y_1+y_2$, $x_2=y_1-y_2$, $y_i=x_i$, $i\geq 3$. Получим $2a_{12}$ при $y_1^2$ и $-2a_{12}$ при $y_2^2$. Теперь находимся в ситуации первого случая. Если так получилось, что $a_{12}=0$, но $a_{1i}\neq 0$, то опять же, можно перенумеровать координаты и применить указанную конструкцию.\\


\noindent{\bf Четвёртый случай.} Все $a_{ii}=0$ и все $a_{1i}=0$. Тогда форма не зависит от первой переменной и можно смело переходить к следующей переменной.\\ 



\subsection{Канонический вид квадратичной формы и критерий Сильвестра}

\dfn Пусть $A$ -- матрица. Числа $d_i=\det A_i$, где $A_i$ -- подматрица $A$ составленная из элементов первых $i$ строк и столбцов  называются главными минорами матрицы $A$. Будем считать $d_0=1$.
\edfn


Оказывается, что если посмотреть на алгоритм приведения к диагональному виду, то для итоговой формы есть выражение через числа $d_i$.
\thrm[Теорема Якоби]
Пусть $V$ -- векторное пространство, $q$ -- квадратичная форма, $A$ -- её матрица в некотором базисе $e_1,\dots,e_n$. Пусть главные миноры $d_i$ не равны 0. 
Тогда матрица $A$ -- невырожденная и может быть приведена к диагональному виду с числами $\frac{d_{i}}{d_{i-1}}$ на диагонали.
\proof Заметим, что если мы всё время пользуемся первым случаем из алгоритма, то домножение слева на матрицу перехода $C^{\top}$ будет эквивалентно прибавлению строки ко всем остальным строкам матрицы $A$ c коэффициентами $-\frac{a_{1i}}{a_{11}}$ (аналогично при домножении на $C$ справа происходит преобразование столбцов). Заметим, что при таком преобразовании главные миноры матрицы вообще не меняются (внутри каждого минора происходят преобразования первого типа, которые не меняют определитель). Покажем, что только первый случай реализуется. 

Пусть мы доказали это для шага $i$. На шаге $i+1$ первые $i$ столбцов и строк содержат только ненулевые диагональные элементы $a_{11},\dots, a_{ii}$. Тогда $d_{i+1}=a_{11}\dots a_{i+1 i+1}$. Так как $d_{i+1}$ не  поменялось и, следовательно не равно нулю, то и $a_{i+1 i+1} \neq 0$. Следовательно реализуется первый случай. 

Теперь посмотрим, что происходит, после приведения матрицы к диагональному виду. Заметим, что для всех $i$ $d_i=a_{11}\dots a_{ii}$. Тогда $a_{ii}=\frac{d_i}{d_{i-1}}$, что и требовалось.
\endproof
\ethrm



Есть ситуации, где мы можем полностью описать канонический вид, к которому можно привести квадратичную форму. Проще всего дать описание над алгебраически замкнутым полем, скажем $\mb C$. Над $\mb C$ любая квадратичная форма определяется своим рангом, так как приводится к виду $$q(x)=x_1^2+\dots+x_r^2.$$


Рассмотрим поле вещественных чисел $\mb R$. К какому виду можно привести форму над $\mb R$? 

\utv Пусть $q$ -- квадратичная форма на вещественном векторном пространстве $V$. Тогда существует линейная система координат в которой форма имеет вид $$q(x)= x_1^2+\dots + x_k^2 - x_{k+1}^2-\dots-x_{k+l}^2.$$
Такой вид квадратичной формы будем называть каноническим. 
\eutv
\proof Мы уже знаем, что можно найти такие координаты, что $$q(x)= \lambda_1 x_1^2+ \dots + \lambda_k x_k^2+ \lambda_{k+1} x^{k+1} + \dots + \lambda_{k+l} x_{l+k}^2.$$
Здесь все нулевые слагаемые соответствуют последним переменным и выброшены. Нумерация  координат выбрана так, что первые $k$ коэффициентов положительные, а следующие $l$ отрицательные. Тогда выберем новые координаты $y_i=\sqrt{|\lambda_i|} x_i$ при $i\leq k+l$ и $y_i=x_i$ иначе. Это и есть нужная система координат.
\endproof

\dfn Сигнатурой формы над $\mb R$ называется пара чисел $(k, l)$ -- число плюсов и число минусов в каноническом виде. Заметим, что сумма $l+k= \rk q$.
\edfn

\dfn Квадратичная форма называется положительно определённой, если $\forall v\neq 0$ $q(v)>0$. Симметричная билинейная форма называется положительно определённой, если соответствующая форма $q(v)=h(v,v)$ положительно определена. Симметричная матрица называется положительно определённой, если соответствующая форма положительно определена. Аналогично вводится понятие отрицательно определённой формы.
\edfn



\thrm Сигнатура формы $q$ не зависит от способа приведения формы к каноническому виду. Точнее -- число $k$ равно размерности наибольшего подпространства, ограничение формы  на которое положительно определено.
\proof Рассмотрим базис $e_1,\dots,e_k,e_{k+1},\dots,e_{l+k}, \dots, e_n$, что матрица формы $q$ диагональна, и первые $k$ её диагональных компонент положительны, следующие $l$ отрицательны, а остальные 0. 
Пусть $U$ подпространство $\dim U \geq k+1$, что $q|_{U}>0$. Тогда исходя из подсчёта размерности $U\cap \lan e_{k+1},\dots,e_n\ran \neq \{0\}$. Но это приводит к противоречию, так как $q(v)$ для $0\neq v \in U\cap \lan e_{k+1},\dots,e_n\ran $ выполнено, что $q(v)>0$ и $q(v)\leq 0$ одновременно.
\endproof
\ethrm

\crl Пусть $q$ -- форма на вещественном пространстве  $V$ размерности $n$. Тогда канонический вид $q$ однозначно определяется $n$ и её  сигнатурой. 
\ecrl

Можно ли как-то ещё найти сигнатуру не приводя форму к диагональному виду, а воспользовавшись другими знаниями? Ответ: да, можно. А именно:

\crl[Критерий Сильвестра]
Пусть $V$ -- векторное пространство над $\mb R$, $q$ -- квадратичная форма, $A$ -- её матрица в некотором базисе $e_1,\dots,e_n$. Пусть главные миноры  $d_i$ матрицы $A$ все не равны $0$. Тогда число перемен знака в последовательности $1=d_0,d_1,\dots,d_n$ равно числу отрицательных квадратов в каноническом виде.
\proof По теореме Якоби существует система координат в которой форма имеет диагональную матрицу с числами $\lambda_i=\frac{d_i}{d_{i-1}}$ на диагонали. Тогда последовательность $d_i$ меняет знак тогда и только тогда, когда $\lambda_i<0$.
\endproof
\ecrl

Поговорим теперь про частный случай положительно определённых форм:


\lm Положительно определённая билинейная(квадратичная) форма всегда невырождена.
\proof $h(x,x)>0$ и поэтому не равно 0.
\endproof
\elm

\thrm
Пусть дана форма $q$ на вещественном пространстве $V$ и её матрица $A$ в некотором базисе. Следующие условия эквивалентны:\\
1) Форма $q$ положительно определена.\\
2) Главные миноры матрицы $A$ положительны.\\
3) Матрица $A$ представима в виде $A=C^{\top}C$ для некоторой невырожденной верхнетреугольной матрицы $C$.\\
4) Матрица $A$ представима в виде $A=C^{\top}C$ для некоторой невырожденной матрицы $C$.
\proof
1) в 2). Если $q>0$, то $q|_{\lan e_1,\dots,e_k\ran} >0$. Но определитель матрицы этой формы и есть главный минор порядка $k$. Значит все $d_k \neq 0$ и можно применить критерий Сильвестра. Так как форма положительно определена, то  в последовательности $1=d_0,d_1,\dots,d_n$ нет перемен знака. Отсюда $d_i>0$.

2) в 3). Следует из даказательства теоремы Якоби -- всегда реализуется первый случай и, значит, матрицы перехода верхнетреугольные и невырожденные.

3) в 4). Просто забудем, что $C$ -- верхнетреугольная.

4) в 1)  Пусть $A=C^{\top}C$, тогда если $x^{\top}Ax=(Cx)^{\top}Cx\geq 0$. Более того, это выражение равно нулю только если $Cx=0$. Но такое возможно, только если $x=0$ из-за невырожденности $C$.
\endproof
\ethrm

Представление положительно определённой матрицы $A$ в виде $C^{\top}C$ для верхнетреугольной $C$ называется разложением Холецкого. Критерий Сильвестра пригоден в случае, если имеется задача с параметром. Иначе, вычисление определителя практически эквивалентно приведению к диагональному виду. Покажем, как можно применять понятие положительной определённости. Точнее, того, что положительная определённость гарантирует невырожденность.




\zd Рассмотрим множество $\left\{ 1,\dots, n\right\}$. Сколько может быть различных подмножеств $C_1,\dots,C_m$, таких, что $|C_i \cap C_j|=t\geq 1$ одинаково для всех $i \neq j$?
\ezd

Мы покажем, что есть ограничение $m\leq n$. Прежде всего, это надо сделать в случае, если $|C_i|=t$ для некоторого $i$. В этом случае $C_i \subseteq C_j$ для всех $j$. Тогда $C_j'=C_j\setminus C_i$ лежат в множестве $\{1,\dots,n\}\setminus C_i$, которое состоит из $n-t$ элементов. Множества $C_j'$ не пусты, но имеют пустое пересечение друг с другом. Тогда их меньше чем $n-t$ штук, откуда -- каждое множество должно содержать уникальный элемент. Итого
$$m-1\leq n-t \leq n-1 \text{ или, по-другому, } m \leq n.$$

Теперь покажем, что в ситуации $d_i=|C_i|>t$ выполнено то же неравенство. Для этого составим матрицу инцидентности
$$B_{ij}= \begin{cases} 1, \text{ если } i\in C_j. \\
0, \text{ иначе}
\end{cases}. $$
Теперь матрица $B^{\top}B$ есть квадратная симметричная матрица размера $m$. Я утверждаю, что $B^{\top}B$ положительно определена. Для этого найдём её явно.
$$B^{\top}B= \pmat
d_1 & t &t\\ 
t & \ddots & t \\
t & t & d_m
\epmat = \pmat
d_1-t &  &0\\ 
 & \ddots &  \\
0 &  & d_m-t
\epmat + \pmat
t & \dots & t\\ 
\vdots & \ddots & \vdots \\
t & \dots & t
\epmat.$$
Тогда при $x\neq 0$ имеем 
$$x^{\top} B^{\top}B x = \sum (d_i-t) x_i^2 + t(\sum x_i)^2>0.$$
Значит матрица $B^\top B$ имеет ранг $m$. Но сдругой стороны 
$$m=\rk B^{\top}B \leq \rk B \leq n.$$

Дальнейшие оценки и результаты существования для различных конфигураций множеств можно найти по ключевым словам Block Design (аналог в русском языке --  комбинаторные схемы)




\section{Евклидовы и унитарные пространства}

Напомню, что основной нашей мотивацией для изучения билинейных форм было, то, что вместе с понятием расстояния часто идёт вместе некоторая билинейная форма.


\dfn Векторное пространство $V$ над $\mb R$ вместе с заданной на нём положительно определённой симметричной билинейной формой $\lan\cdot \, , \cdot \ran$ называется евклидовым пространством. Форма $\lan\cdot \, , \cdot \ran$ называется скалярным произведением. 
\edfn

\dfn Определим  норму на евклидовом пространстве как $\|v\|=\sqrt{\lan v , v\ran }$. Норма задаёт расстояние по правилу $\rho(u,v)=\|u-v\|$. 
\edfn

\lm[Неравенство Коши-Буняковского] В евклидовом пространстве выполнено неравенство
$$ \lan u,v\ran \leq \|u\|\|v\|.$$
\proof  Квадратный трёхчлен $\lan u,u\ran +2\lambda\lan u,v\ran +\lambda^2\lan v,v\ran=\lan u+\lambda v, u+\lambda v\ran \geq 0$ всегда положителен. Значит у него нет корней, то есть дискриминант отрицателен. То есть $$4\lan u,v\ran^2 \leq 4 ||u||^2||v||^2.$$
\endproof
\elm

\lm Введённая норма действительно является нормой.
\proof Необходимо показать неравенство $||u+v||\leq ||u||+||v||$. Оно эквивалентно $$||u+v||^2 \leq ||u||^2+||v||^2+2||u||||v||$$
Расписывая левую часть получаем эквивалентное
$$ ||u||^2+||v||^2+2\lan u,v\ran \leq ||u||^2+||v||^2+2||u||||v||.$$
Сокращая справа и слева приходим к уже известному неравенству.
\endproof
\elm

 
\lm Пусть $V$ -- евклидово пространство. Тогда для всякого подпространства $U$ имеет место ортогональное разложение $V=U\oplus U^{\bot}$. Если есть такое разложение, то оператор проекции на $U$ называется ортогональной проекцией.
\elm
\proof Положительно определённая форма невырождена. Ограничение положительно определённой формы на любое подпространство положительно определено.
\endproof



Мы с вами помним, что в задачах алгебры  удобнее бывает работать над алгебраически замкнутым полем. Однако, если мы дословно перенесём все определения с $\mb R$ на $\mb C$, то нас постигнет неудача. Прежде всего в плане положительной определённости. А именно, любая комплексная квадратичная форма не будет принимать вещественные значения. Это делает невозможным аналогичное вещественному случаю определение расстояния. 

Это приводит нас к тому, что язык билинейных форм не совсем адекватен в комплексной ситуации. С другой стороны у нас есть пример удачного понятия расстояния на $\mb C$, которое задаётся формулой $\sqrt{\ovl{z}z}$. Такой выражение получается не из билинейной формы $xy$ подстановкой $x=y$, а из менее ожидаемого $\ovl{x}y$. Заметим, что и вообще на пространстве $\mb C^n$ можно ввести операцию $
(x,y) \to \sum_{i=1}^n \ovl{x_i}y_i$, которая даст по стандартной схеме норму $\sqrt{\sum_{i=1}^n |x_i|^2}$. Попробуем разобраться в общей ситуации.

\dfn Пусть $V$ -- комплексное пространство.  Отображение $h\colon V \times V \to \mb C$ называется полуторалинейным, если \\
1) $h(x,y+\lambda z)=h(x,y)+\lambda h(x,z)$. \\
2) $h(x+\lambda y,z)=h(x,z)+\ovl{\lambda} h(y,z)$.
\edfn

\exm\\
1) Основным примером полуторалинейной формы на $\mb C^n$ будет форма $(x,y)\to \sum \ovl{x_i}y_i$\\
2) В более общем виде, если взять матрицу $A\in M_n(\mb C)$  то выражение $\ovl{x}^{\top}Ay$ задаёт полуторалинейную форму на $\mb C^n$.\\
3) Рассмотрим пространство комплекснозначных непрерывных функций на отрезке $C([a,b])$. Определим полуторалинейную форму по следующему правилу:
$$h(f,g)=\int_a^b \ovl{f(x)}g(x)w(x)dx,$$
где $w(x)$ -- непрерывная на $[a,b]$ комплекснозначная функция, которая обычно называется весом.\\



\dfn Матрицей полуторалинейной формы $h$
в базисе $e$ называется матрица $a_{ij}=h(e_i,e_j)$. 
\edfn

\lm Если $x$ и $y$ координаты векторов $u$ и $v$, то $$h(u,v)=\ovl{x}^{\top}Ay$$
и обратно, если $$h(u,v)=\ovl{x}^{\top}Ay,$$
то $A$ -- это матрица $h$.
\elm



Заметим, что понятия симметричности  от таких форм не приходится ожидать. Действительно, если $\lambda h(u,v)=h(u,\lambda v) = h(\lambda v,u)=\ovl{\lambda}h(v,u)=\ovl{\lambda}h(u,v)$ для всех $\lambda$. Тогда, конечно, $h(u,v)=0$. Однако приведённые примеры подсказывают нам необходимое свойство.

\dfn Полуторалинейная форма $h$ называется эрмитовой, если $h(u,v)=\ovl{h(v,u)}$ и косоэрмитовой, если $h(u,v)=-\ovl{h(v,u)}$
\edfn

\lm Полуторалинейная форма эрмитова тогда и только тогда, когда её матрица $A$ удовлетворяет соотношению $\ovl{A^{\top}}=A$ и косоэрмитова, если $\ovl{A^{\top}}=-A$.
\elm

Если коэффициенты матрицы вещественны, то условие эрмитовости -- это условие симметричности. Вообще, эрмитовость формы $h$ означает, что все значения $h(v,v) \in \mb R$. Это позволяет дать опеределение аналог положительной определённости. 


\dfn Эрмитова форма называется положительно определённой, если для всех $v\in V\setminus\{0\}$ выполняется $h(v,v)>0$.
\edfn

\lm Матрица положительно определённой эрмитовой формы невырождена.
\proof От противного, если существует вектор $x\in \Ker A\setminus\{0\}$, то $0<\ovl{x}^{\top}Ax = 0$, противоречие.
\endproof
\elm

Вообще, для эрмитовых форм есть аналог теоремы об ортогонализации, теоремы Якоби, сигнатуры и критерия Сильвестра. То есть при желании можно при помощи координат проверить положительную определённость эрмитовой формы. Перейдём к основному определению, связанному с положительной определённостью в комплексном случае.


\dfn(Унитарное пространство) Пространство $V$ над $\mb C$ вместе с положительно определённой эрмитовой формой $\lan \cdot, \cdot \ran$ называется унитарным пространством. Форма $\lan \cdot, \cdot \ran$ называется скалярным произведением.
\edfn

Попробуем доказать аналог неравенства Коши-Буняковского для унитарного пространства $V$ и вывести из него, что отображение $\|\cdot\| \colon V \to \mb R$, заданное по правилу $v\to \sqrt{\lan v,v\ran}$, так же как и в вещественном случае задаёт  норму на $V$.

\lm Пусть $u,v \in V$ -- два вектора в унитарном пространстве. Тогда имеет место неравенство:
$$|\lan u,v\ran| \leq \|u\| \|v\|.$$
\proof Как и раньше  $\lan u+\lambda v, u+\lambda v\ran \geq 0$ для всех $\lambda \in \mb R$. Раскрывая скобки получаем однако $$\lan u,u\ran +2\lambda \Re\lan u,v\ran +\lambda^2\lan v,v\ran\geq 0.$$  Итого имеем  $$4(\Re\lan u,v\ran)^2 \leq 4 ||u||^2||v||^2.$$
Пусть $\lan u ,v \ran = r e^{i\ffi}$. Тогда $\Re \lan  e^{i\ffi}u , v \ran= e^{-i\ffi} r e^{i\ffi}=r=|\lan u,v\ran|$, а $||e^{i\ffi}u||=||u||$. Применение предыдущего неравенства  к паре $e^{i\ffi} u, v$ доказывает общее неравенство.
\endproof
\elm

\crl Отображение $\|\cdot\| \colon V \to \mb R$, заданное по правилу $v\to \sqrt{\lan v,v\ran}$ задаёт  норму на $V$.
\ecrl


Так же полезно будет проинтерпретировать геометрическое понятие угла между двумя векторами, чтобы отдать дань школьному определению скалярного произведения.

\dfn Пусть $x,y\neq 0$ два вектора в $V$. Если $V$ -- евклидово, то углом между ними называется такое число $0\leq\ffi\leq \pi$, что 
$$\cos\ffi = \frac{\lan x,y\ran}{\|x\| \|y\|}.$$
В случае унитарного пространства $V$ угол $\ffi$ может принимать значения $0\leq \ffi \leq \frac{\pi}{2}$ и определяется соотношением
$$\cos\ffi = \frac{|\lan x,y\ran|}{\|x\| \|y\|}.$$
\edfn

Оба определения корректны благодаря неравенству Коши-Буняковского. По каждому из этих определений угол между векторами равен $\frac{\pi}{2}$ тогда и только тогда, когда $\lan x,y 
\ran=0$, то есть когда векторы ортогональны. 

В дальнейшем мы будем обсуждать  евклидовы и унитарные пространства. Свойства евклидовых и унитарных пространств похожи, поэтому  мы будем стараться доказывать общие утверждения в обоих случаях.



\section{Ортогонализация Грама-Шмидта}

Если мы рассматриваем вектор $x$ на плоскости и некоторую прямую $l$, заданную направлением $v$, то проекция $x$ на прямую $l$ может быть найдена по формуле 

$$pr_l(x)=\cos \alpha  ||x|| \cdot v= \frac{\lan x,v\ran}{||v||^2} v.$$

В случае евклидовых пространств ограничение положительно определённой формы на любое подпространство невырождено и, таким образом, нахождение базиса, в котором форма имеет канонический вид несколько облегчается.  Это приводит к тому, что мы можем уточнить сам результат и немного изменить алгоритм нахождения подходящего базиса. Так же мы покажем, что ровно тот же алгоритм работает в унитарных пространствах и позволяет найти такой базис, что матрица скалярного произведения в этом базисе диагональна и даже единична.

Итак пусть дан набор векторов $e_1,\dots, e_n $ евклидового или унитарного пространства $V$. Ортогонализацией набора $e_1,\dots,e_n$ называется  новый набор векторов $f_1,\dots,f_n$ такой, что\\
1) $f_i \bot f_j$, если $i\neq j$\\
2) $\forall\,\, 1\leq k\leq n\,\,\lan e_1,\dots,e_k\ran=\lan f_1,\dots,f_k\ran$\\
3) $\|f_i\|=1$.

\dfn Набор векторов со свойством 3) называется нормированным. со свойствами 1),3) -- ортонормированным.
\edfn 

\rm Заметим, что если мы нашли набор ненулевых векторов со свойствами 1) и 2), в котором нет нулевых векторов, то несложно сделать из него нормированный набор, взяв вектора $\frac{f_i}{\|f_i\|}$. 
\erm

\thrm Пусть $V$ -- евклидово или унитарное пространство. Задача ортогонализации разрешима для линейно независимого набора векторов из $V$.
\proof

Перейдём к решению задачи добиваясь только условий 1) и 2). Будем последовательно искать вектора $f_i$ в виде $f_i=e_i+\lambda_1 f_1 +\dots + \lambda_{i-1} f_{i-1}$. Этот подход приводит к ответу
$$f_i=e_i-\sum_{j<i} \frac{\lan f_j,e_i\ran}{\lan f_j,f_j\ran}f_j.$$
Так как вектора линейно независимы, то $f_i\neq 0$. Это означает, что можно поделить на его норму и добиться нормированности.
\endproof
\ethrm




\crl В евклидовом и унитарном пространстве любой ортонормированный набор векторов можно дополнить до ортонормированного базиса.
\ecrl

Процесс ортогонализации позволяет строить ортогональный базис для различных подпространств. Кроме того, очень удобно находить координаты в ортогональном базисе.



\utv[Нахождение координат в ортогональном базисе] Пусть набор $e_1,\dots,e_n$ --- ортогональный базис $V$. Если $c_i$ -- это координаты вектора $x$ в базисе $e$, то 
$$c_i= \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}.$$
В случае нормированного базиса формула упрощается -- исчезает знаменатель. Кроме того, в этой ситуации $\|x\|^2=\sum |c_i|^2$
\proof Пусть $x=\sum c_i e_i$. Тогда $\lan e_i, x\ran = c_i \lan e_i,e_i \ran$, что и требовалось.
Предположим, что $e$ -- ортонормированный базис. Тогда, раскрывая скобки в выражении $||x||^2=\lan x, x\ran$ приходим к $\sum |c_i|^2 $.
\endproof
\eutv


\rm Для любого подпространства в унитарном пространстве $U \leq V$ так же определено его ортогональное дополнение $U^{\bot}=\{ v\in V\,|\, \lan u,v\ran =0 \text{ для всех } u\in U\}$. $V$  раскладывается в прямую сумму $V=U\oplus U^{\bot}$ -- действительно их пересечение $0$, а уравнения на элементы из $U^\bot$ по прежнему задаются при помощи базисных элементов из $U$, что даёт нужное соотношение на размерность.
\erm


\crl Пусть $ e_1,\dots, e_n$ --- ортогональный базис $V$, а  подпространство $U$ порождено $ e_1,\dots,e_k$. Тогда 
\enm
\item $ pr_U x= \sum \frac{\lan x,e_i\ran}{\lan e_i,e_i\ran} e_i.$
\item $||pr_{U^{\bot}} x||^2 + ||pr_U x||^2=||x||^2.$
\eenm
\proof Рассмотрим базис $e_1,\dots, e_k$ и дополним его векторами $e_{k+1},\dots,e_n$ до ортогонального базиса всего $V$. Тогда $$x= \sum_{i=1}^k \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}e_i + \sum_{i=k+1}^n \frac{\lan e_i,  x\ran}{\lan e_i, e_i\ran}e_i $$
Первая часть суммы лежит в $U$, а вторая в ортогональном дополнении. По единственности такого разложения получаем требуемое.
\endproof
\ecrl







Для чего может пригодиться понятие ортогональной проекции? Прежде всего для вычисления расстояния. Расстояния между подпространствами. Начнём с общего определения.

\dfn Пусть $A$ и $B$ подмножества метрического пространства. Тогда расстоянием $\rho(A,B)$ положим равным
$$\rho(A,B)=\inf_{\substack{x\in A\\ y \in B}} \rho(x,y).$$
\edfn

Наша задача --- научиться считать расстояние между аффинными подпространствами $A_1$ и $A_2$ в евклидовом пространстве, то есть подмножествами вида $L+x$, где $L$ -- это произвольное подпространство (линейное), а $x$ -- некоторый вектор из $V$. Попробуем это сделать. Представим $A_1=L_1+x$ и $A_2=L_2+y$. 

Разберём сначала случай расстояния от точки до линейного подпространства. 

\thrm Пусть $U \leq V$ подпространство, $x \in V$. Тогда расстояние $\rho(x,U)$ достигается на проекции  $pr_U(x)$ и равно $\|x-pr_U(x)\|=\|pr_{U^{\bot}}(x)\|$.
\proof Рассмотрим $u \in U$, и представим $x=y+z$, где $pr_Ux=y \in U$, $pr_{U^{\bot}}x =z \in U^{\bot}$. Тогда $$\rho(x,u)^2= ||x-u||^2= ||y - u ||^2+ ||z||^2 \geq ||z||^2=||pr_{U^{\bot}}x||^2 .$$
С другой стороны равенство достигается при $u=y=pr_{U} x$.
\endproof
\ethrm



\rm Если размерность $U^{\bot}$ мала, то может быть легче найти проекцию на $U^{\bot}$, найдя ортогональный базис $U^{\bot}$.
\erm

Всё это немедленно приводит к решению общей задачи.

\utv Пусть  $A_1=L_1+x$ и $A_2=L_2+y$ -- аффинные подпространства. Тогда $\rho(A_1,A_2)=\rho(y-x, L_1+L_2)$. То есть задача сводится к ранее разобранной.
\proof $$\inf_{\substack{u+x\in L_1+x\\ v+y \in L_2+y}} ||u+x-v-y||=\inf_{\substack{u-v\in L_1+ L_2}}||u-v - (y-x)||=\inf_{\substack{u\in L_1+ L_2}}||u- (y-x)||.$$
\endproof
\eutv


\dfn Пусть $e_1,\dots, e_k$ набор векторов $V$. Тогда матрицей Грама называется матрица 
$$G(e_1,\dots,e_k)_{ij}= \lan e_i, e_j\ran.$$
Матрица Грама отличается от матрицы скалярного произведения как би(полутора)линейной формы, только тем, что определяется она для произвольного набора векторов. 
\edfn

Как мы помним, процедура ортогонализации Грама-Шмидта аналогична той, которую мы обсуждали в общем контексте билинейных форм. Главные миноры матрицы Грама и, в частности, определитель матрицы Грама дают коэффициенты матрицы после ортогонализации. А что значат эти коэффициенты? Разберёмся в случае $\mb R^n$.

\utv
Пусть $v_1,\dots,v_n$ -- набор векторов в $\mb R^n$. Тогда
$$ \det G(v_1,\dots,v_n)=(\Vol(v_1,\dots,v_n))^2$$
\proof Пусть матрица $A$ составлена из столбцов $v_i$. Тогда $G(v_1,\dots,v_n)=  A^{\top}A$ и
$$ \det G(v_1,\dots,v_n)= \det A^{\top}A=\det A^{\top} \det A= (\det A)^2= \Vol(v_1,\dots,v_n)^2.$$
\endproof
\eutv

\rm Таким образом, видно, что понятие расстояния точно определяет понятие объёма параллелепипеда. Хотя и не даёт возможности задать ориентацию пространства.
\erm

\rm Определитель матрицы Грама обнуляется тогда и только тогда, когда вектора $v_i$ линейно зависимы.
\erm


\upr Пусть $A$ -- это матрица Грама набора векторов $e_1,\dots,e_k \in \mb R^n$, а $B$ -- набора векторов $e_1,\dots,e_{k+1}$. Покажите, что $$\frac{\det B}{\det A}=\rho(e_{k+1},U)^2, \text{ где } U=\lan e_1,\dots,e_k \ran.$$
\eupr





\subsection{Метод наименьших квадратов}


Допустим, мы хотим узнать некоторый закон природы в виде $y=f(x)$. Мы провели много измерений $y_i=f(x_i)$. Если мы предполагаем, что функция $f$ есть, например, многочлен, то на коэффициенты этого многочлена возникает система линейных уравнений. 

К сожалению, не никаких шансов, что эта система разрешима: количество измерений велико а наши измерения не точны. Иными словами выполнены равенства $y_i= f(x_i) + \eps_i$ для маленьких $\eps_i$. Что делать если мы не можем точно решить систему? Будем наиболее близкое её решение. В качестве меры близости разумно выбрать сумму
$$\sum_i |y_i-f(x_i)|^2.$$
Обобщая, мы приходимк следующующей задаче: пусть есть матрица $A\in M_{m\times n}(\mb R)$, столбец $b\in R^m$ и мы хотим найти $x$ такой, что  
$$||Ax-b|| \text{ минимальна.}$$
Заметим, что в качестве $Ax$ может выступать произвольный элемент $\im A$. Это означает, что для решения нашей задачи мы должны найти ближайший к $b$ элемент $y \in \Im A$ и для этого $y$ найти соответствующий ему $x$. У этой задачи есть довольно простое решение. А именно, заметим, что элемент $y=Ax=pr_{\Im A} b$ должен иметь одинаковые с $b$ скалярные произведения со всеми столбцами матрицы $A$ (потому что они порождают $\Im A$). Это равносильно матричному соотношению
$$A^\top A x=A^\top b.$$
Как теперь найти сам вектор $x$? Рассмотрим ситуацию, когда $\Ker A=\{0\}$. В этом случае, матрица $A$ осуществляет взаимооднозначное соответствие между векторами из $\mb R^n$ и векторами из $\Im A \leq \mb R^m$.
Матрица $A^\top A$ -- это матрица Грама для столбцов матрицы $A$ и значит невырождена (столбцы $A$ линейно независимы). Значит у этой системы есть единственное решение 
$$x=(A^\top A)^{-1}A^\top b,$$
которое и даёт искомый единственный вектор $x$. Если же $\Ker A \neq 0$, то ситуация усложняется. 

\begin{comment}

Мы по прежнему ищем $y\in \Im A$ ближайший к $b$. Но для данного $y$ есть много $x\in \mb R^n$, что $Ax=y$. Какой из них выбрать? Выберем $x$ с наименьшей  длиной. Что это означает? Представим произвольное решение этого уравнения как $x_0+z$, где $z\in \Ker A$. Наименьшее длина такого вектора будет достигаться при $z= pr_{\Ker A} x_0$. В этом случае $x=x_0+z$ будет равен проекции $x_0$ на $(\Ker A)^{\bot}$. Способ нахождения такого элемента мы обсудим позже.

\end{comment}

\dfn Если $\Ker A =0$, то матрица $(A^{\top }A)^{-1}A^{\top}$ называется псевдообратной к $A$. В общей ситуации определение более сложное.
\edfn

На математической статистике вам объяснят, почему такой выбор является наилучшим при некотором предположении на распределений ошибок в каждом равенстве. Однако метод наименьших квадратов удобно использовать и для анализа данных и прогнозирования их поведения.


\section{Ортогональные и унитарные операторы}

Основным отличием структуры евклидового и унитарного пространства от просто векторного пространства является понятие расстояние и поэтому некоторое время мы посвятим преобразованиям, это расстояние сохраняющим.




\dfn Пусть $V$ --- евклидово (унитарное) пространство. Ортогональным (соответственно унитарным) оператором на $V$ называется такой линейный оператор $L \colon V \to V$, что $||Lx||=||x||$.
\edfn




\thrm Пусть $L \colon V \to V$ -- линейный оператор на евклидовом или унитарном пространстве $V$. Тогда следующие условия эквивалентны:\\
1) $L$ -- ортогональный (унитарный) оператор.\\
2) $\lan Lx,Ly\ran=\lan x,y \ran$ для всех $x,y \in V$.\\
3) $L$ переводит любой ортонормированный базис в ортонормированный базис.\\
4) В любом ортонормированном базисе $A$ -- матрица $L$ -- удовлетворяет условию $\ovl{A}^{\top} A = E_n$.\\
5) $L$ переводит некоторый ортонормированный базис в ортонормированный базис.\\
6) В некотором ортонормированном базисе $A$ --  матрица $L$ -- удовлетворяет условию $\ovl{A}^{\top} A = E_n$.
\proof Покажем $1\to 2$. Пусть $x$ и $y$ из $V$, тогда $$||x||^2+ ||y||^2+ 2\Re\lan x,y\ran= ||x+y||^2= ||L(x+y)||^2= ||x||^2+||y||^2+2\Re \lan Lx,Ly\ran.$$
Итого вещественные части скалярного произведения сохраняются. Теперь взяв вместо $x$ вектор $ix$ получаем $\Re \lan ix,y\ran = - \Re i\lan x,y\ran=\Im \lan x,y\ran$ и аналогично $\Re \lan L(ix),Ly\ran =\Im \lan Lx,Ly \ran$ откуда и мнимые части совпадают.
$2\to 3$ ясно.


Покажем, что $3 \leftrightarrow 4$ и $5 \leftrightarrow 6$. Для этого покажем, что если фиксировать ортонормированный базис $e_1,\dots, e_n$, то $L(e_i)$ ортонормирован, тогда и только тогда, когда $A$ -- матрица $L$ удовлетворяет условию $\ovl{A}^{\top}A=E_n$. Для этого необходимо и достаточно заметить, что $$\lan Le_i, Le_j \ran = (\ovl{A}^{\top}A)_{ij}.$$
Действительно $v_j$ -- $j$-ый столбец $A$ составлен из координат $Le_j$. $i$-ая строка $\ovl{A}^{\top}$ равна тогда $\ovl{v}_i^{\top}$. Но тогда $$\lan Le_i,Le_j\ran = \ovl{v}_i^{\top}E_n v_j= \ovl{v}_i^{\top}v_j= (\ovl{A}^{\top}A)_{ij}.$$
Во втором равенстве $E_n$ играет роль матрицы Грама ортонормированного базиса $e_1,\dots, e_n$.
Итак $3 \leftrightarrow 4$ и $5 \leftrightarrow 6$. Из 3) очевидно следует 5). $6\to 1$. $$||Lx||^2=\ovl{[x]_e}^{\top} \ovl{A}^{\top} A [x]_e= \ovl{[x]_e}^{\top} [x]_e= ||x||^2.$$
\endproof
\ethrm

\crl В частности, ортогональный оператор  $L$ сохраняет углы между векторами.
\ecrl

\crl Пусть $e_1,\dots,e_n$ --- ортонормированный базис $V$. Линейный оператор $L$, который в базисе $e_i$ имеет матрицу, составленную из столбцов $v_1,\dots,v_n$, является ортогональным (унитарным) тогда и только тогда, когда $v_1,\dots,v_n$ --- ортонормированный базис $\mb R^n$. 
\proof
Пусть $B$ -- матрица $L$ -- составлена из столбцов $v_i$ в ортогональном базисе $e$. Тогда соотношение $\ovl{B}^{\top}B=E$ эквивалентно ортогональности и нормированности $v_i$.
\endproof
\ecrl




\dfn Матрица $A\in M_n(\mb R)$ называется ортогональной, если $A^{\top}A=E_n$. Множество всех ортогональных матриц размера $n$ обозначается $\O_n(\mb R)$. Такие матрицы описывают все линейные изометрии $\mb R^n$  и поэтому образуют подгруппу в группе $\GL_n(\mb R)$.
\edfn

\rm Это определение можно применить и к комплексным матрицам, в результате чего получится группа $\SO_n(\mb C)$. Однако вместо неё популярнее другая группа матриц:
\erm

\dfn Определим группу унитарных матриц $\U_n(\mb C)$, как подгруппу в $\GL_n(\mb C)$, состоящую из матриц, удовлетворяющих равенству $\ovl{A}^{\top}A=E_n$.
\edfn

\rm Заметим, что определитель ортогональной матрицы либо плюс, либо минус единица. Определим подгруппу $\SO_n(\mb R) \leq \O_n(\mb R)$ -- специальную ортогональную группу состоящую из матриц $$\SO_n(\mb R)= \{ A \in \O_n(\mb R)\, | \, \det A=1 \}.$$
сохраняющих ориентацию. Это подгруппа индекса 2. Её называют группой вращений $\mb R^n$.
Аналогично определяется группа $SU_n$
\erm 


\subsection{QR разложение}

Пусть $A \in M_{m\times n}(\mb R)$ -- матрица ранга $n$. Посмотрим на неё как на набор столбцов $e_1,\dots,e_n$. Применим к этим столбцам процедуру ортогонализации. При применении процедуры ортогонализации мы используем элементарные преобразования столбцов. Более того, мы всегда столбцы с меньшим номером прибавляем к столбцам с большим номером. Таким образом, мы всегда домножаем матрицу $A$ на верхнетреугольную матрицу. Итого существует верхнетреугольная матрица $R$, что 
$$AR=Q,$$ 
где столбцы $Q$ образуют ортонормированный базис $\mb R^n$. Тогда  $Q$ -- ортогональная матрица. Домножив на $R^{-1}$ получаем, что 
$$A=QR^{-1},$$
$R^{-1}$ -- верхнетреугольная невырожденная, $Q$ состоит из ортонормированного набора столбцов. Таким образом мы доказали:

\thrm Для любой матрицы  $A\in M_{m \times n}$ существуют матрицы $R\in UT_n(\mb R)$ и $Q\in M_{m\times n}(\mb R)$,  что столбцы $Q$ ортонормированны, $R$ -- невырождена и  
$$A=QR.$$
Такое разложение называется $QR$  разложением. Если $A$ -- квадратная, то матрица $Q$ -- ортогональна. 
\ethrm


Если вам известно $QR$ разложение матрицы, то это позволяет вам легко вычислить псевдообратную (в частности, обратную) матрицу. Действительно: $A^\top A= R^{\top} Q^\top Q R= R^\top R$. Значит
$$(A^\top A)^{-1}A^\top= R^{-1} (R^\top)^{-1}R^\top Q^\top= R^{-1}Q^\top.$$
Процесс ортогонализации считается более устойчивым к ошибкам округления, чем метод Гаусса. Поэтому, такое обращение для вещественных матриц предпочтительней.


Вы можете спросить: есть ли тут что-то общее с QR-кодом? Ответ: ничего кроме букв.
 





\section{Сопряжённые линейные отображения}

Геометрия евклидовых и унитарных пространств позволяет сводить определённые вопросы про билинейные формы к вопросам про операторы и наоборот. А именно, всякому оператору $L$ соответствует билинейная (полуторалинейная) форма $\lan x,Ly\ran$. Несложно понять, что любая билинейная форма имеет такой вид (из соображений, что и оператор и форма однозначно  задаются квадратными матрицами) и по этой форме можно обратно восстановить $L$.

Кроме того, $L$ задаёт другую билинейную (полуторалинейную) форму $\lan Lx,y\ran$. Восстановив по этой форме обратно оператор, используя уже обсуждённое ранее соответствие мы получим вообще говоря отличный от $L$ оператор. Его обозначают обычно как $L^*$. 

Разберёмся с этими соответствиями поближе. Нам удобно будет расширить контекст с операторов на одном евклидовом (унитарном) пространстве, до произвольных линейных отображений между такими пространствами.

\dfn Пусть $L$ -- линейное отображение $L\colon U \to V$ между евклидовыми или унитарными пространствами. Тогда сопряжённым отображением к $L$, называется такое линейное отображение $L^*$, что $\lan L^*x,y\ran = \lan x,Ly\ran$ для всех $x\in V$ и $y \in U$.
\edfn

\thrm Сопряжённое линейное отображение единственно. Более того, если в $U$ и $V$ выбрать ортонормированные базисы $u$ и $v$, \ матрица $L$ в этих базисах есть $A$, то матрица сопряжённого отображения будет равна $\ovl{A}^{\top}$.
\proof Достаточно доказать необходимость и достаточность последнего соотношения, чтобы показать единственность и существование. Выберем ортонормированные базисы в $U$ и $V$ -- $u_i$ и $v_j$. Обозначим матрицу кандидата на $L^*$ за $B$. Тогда для равенства из определения сопряжённости необходимо и достаточно, его выполнения на базисных. Иными словами необходимо и достаточно, чтобы $\lan L^*e_i,e_j\ran=\lan e_i,Le_j\ran$. Но первая часть даёт $\ovl{B_{ji}}$, а вторая -- $A_{ij}$. Итого необходимо и достаточно, чтобы $\ovl{B}^\top=A$, то есть $B=\ovl{A}^\top$.   
\endproof
\ethrm



\crl Сопряжённый оператор к оператору $L$ существует и единственен. Более того, если задан ортонормированный базис $e_1,\dots,e_n$ и $A$ -- матрица $L$, то матрица $L^*$ есть $\ovl{A}^{\top}$.
\ecrl









\lm[Общие свойства]
$(L+T)^*=L^*+T^*$\\
$(LT)^*=T^*L^*$\\
$(\lambda L)^*=\ovl{\lambda}L^*$.\\
$(L^{-1})^*=(L^*)^{-1}$.\\
$L^{**}=L$.
\proof Фиксируем ортонормированный базис. Тогда все свойства следуют из свойств транспонирования и сопряжения ($\ovl{AB} = \ovl{ A} \,\ovl{B}$).
Однако их можно показать и из определения сопряжённого оператора. Например,
$$\lan x, ABy\ran = \lan A^*x, By\ran = \lan B^*A^*x,y\ran,$$
откуда видно, что $(AB)^*=B^* A^*$. 
\endproof
\elm




\dfn Пусть $L$ оператор на евклидовом или унитарном пространстве $V$ называется самосопряжённым, если $L^*=L$.
\edfn


\rm Пусть $e_1,\dots e_n$ ортонормированный базис, тогда оператор $L$ cамосопряжён тогда и только тогда, когда его матрица в этом базисе удовлетворяет условию $\ovl{L}^{\top}=L$.
\proof Равенство операторов равносильно равенству их матриц в ортонормированном базисе.
\endproof
\erm



\exm\\
0) Любая симметричная матрица $A=A^{\top}$ задаёт самосопряжённый оператор на $\mb R^n$ относительно стандартного скалярного произведения.\\
1) Условие ортогональности оператора можно переписать в виде $L^*L=1$ или, что эквивалентно, $L^*=L^{-1}$. Таким образом, сопряжённый оператор к ортогональному -- это обратный оператор.\\
2) Пусть $\mb R[x]$ пространство многочленов и $g(x,y) \in \mb R[x,y]$ -- многочлен. Тогда сопряжённый к оператору $f \to \int_{a}^b f(y)g(x,y)dy$ это оператор $f\to\int_{a}^b f(x)g(x,y)dx$.\\
3) Сопряжённый оператор к ортогональному -- это обратный к нему.




\section{Спектральные теоремы}

Как всегда при обсуждении линейных операторов разумно задать вопрос про их собственные числа. Для того, чтобы облегчить нашу задачу и не повторять несколько раз одни и те же рассуждения, заметим, что унитарные, самосопряжённые и косоэрмитовы операторы обладают следующим свойством.

\dfn Пусть $L$ оператор на евклидовом или унитарном пространстве $V$. Оператор $L$ называется нормальным, если $LL^*=L^*L$.
\edfn



Мы хотим связать инвариантные пространства относительно $L$ и инвариантные пространства относительно $L^*$. Какой бы ни был оператор верен факт.

\lm \label{normal} Если подпространство $U$ инвариантно относительно $L$, то $U^{\bot}$ инвариантно относительно $L^{*}$.
\proof Пусть $v\in U^{\bot}$. Тогда для всех $u\in U$ верно $\lan L^* v, u\ran = \lan v, Lu\ran =0 $ так как $Lu\in U$, что и требовалось.
\endproof
\elm

\lm Пусть $L$ и $T$ два оператора на комплексном векторном пространстве $V$, которые коммутируют между собой, то есть $LT=TL$. Тогда у $L$ и $T$ есть общий собственный вектор. 
\proof Пусть $\lambda$ -- собственное число $L$. Покажем, что $\Ker L -\lambda E$ -- инвариантное пространство, относительно $T$. Действительно, пусть $v\in \Ker L-\lambda E$. Тогда $(L-\lambda E)Tv=T(L-\lambda E)v=0$.

Теперь, так как $\Ker L-\lambda E$ инвариантно относительно $T$, то у $T$ есть собственный вектор в $\Ker L-\lambda E$. Он же собственный для $L$.
\endproof
\elm

\thrm 
Оператор $L$ на  унитарном пространстве $V$ нормален тогда и только тогда, когда существует ортонормированный базис $e_1,\dots,e_n$ в котором матрица $L$ диагональна.
\proof
Доказательство идёт индукцией по размерности. Если оператор нормален, то у $L$ и $L^*$ есть общий собственный вектор $v_1$, так как они коммутируют.

Возьмём к нему ортогональное дополнение $U=\lan v_1\ran^{\bot}$. Это будет инвариантное подпространство для $L$ и $L^*$. При этом ограничение $L^*$ на $U$ -- это сопряжённый к $L|_U$. По индукции это даёт ортонормированный базис для $L$ на $U$, а вместе с $v_1$ базис из собственных векторов на всём $V$.
\endproof
\ethrm

\crl Если $L$ -- нормальный оператор на унитарном пространстве $V$, то $$V=\bigoplus_{\lambda - \text{ с.ч. } L} \Ker L-\lambda E.$$
При этом $\Ker L-\lambda_i E$ попарно ортогональны при различных $\lambda_i$.
\ecrl


Прежде чем перейти к вещественному случаю лемму про общее строение вещественных матриц.



\lm Пусть $A$ вещественная квадратная матрица размера $n$. Тогда если $v \in \mb C^n$ собственный вектор $A$ с собственным числом $\lambda$, то $\ovl{v}$ собственный вектор $A$ с собственным числом $\ovl{\lambda}$. В частности, комплексное сопряжение осуществляет биекцию между $\Ker A-\lambda E$ и $\Ker A-\ovl{\lambda} E$.
\proof Действительно
$$\ovl{\lambda}\ovl{v}=\ovl{\lambda v}=\ovl{Av}=\ovl{A}\ovl{v}=A\ovl{v}.$$
\endproof
\elm 




\thrm
Оператор $L$ на евклидовом пространстве $V$ нормален  тогда и только тогда, когда существует ортонормированный базис в котором его матрица $A$ блочно-диагональная, при этом блоки имеют  или размер $1\times 1$ или $2\times 2$ вида
$$\begin{pmatrix}
a  & b\\
-b & a
\end{pmatrix}.$$
Такое представление единственно с точностью до порядка блоков.
\ethrm
\proof
Прежде всего покажем единственность блочной структуры. Пусть $e_i$ -- ортонормированный базис в котором $L$ имеет матрицу в указанном виде. Тогда блочное представление матрицы даёт возможность посчитать характеристический многочлен. Клетка $1\times 1$ даёт множитель $t-\lambda$, то есть соответствует вещественному собственному числу. Клетке $2\times 2$ соответствует множитель в характеристическом многочлене $t^2-2at+a^2+b^2$. Этот многочлен раскладывается на множители $$t^2-2at+a^2+b^2= (t-a-bi)(t-a+bi).$$
Если $b\neq 0$, то корни этого многочлена не вещественны, а если равно 0, то мы попадаем в уже разобранную диагональную ситуацию. Тогда количество клеток вида 
$$\begin{pmatrix}
a  & b\\
-b & a
\end{pmatrix}$$
с $b\neq 0$ равно кратности корня $a+bi$ у характеристического многочлена.



Перейдём к доказательству существования. Достаточно доказать существование такого базиса для матриц $A\in M_n(\mb R)$, что $A^\top A=AA^\top$. Такая матрица задаёт нормальный оператор на только на $\mb R^n$, но и на $\mb C^n$. Этим и воспользуемся.

Рассмотрим все собственные числа $\lambda$ матрицы $A$. Если $\lambda\in \mb R$, то $\Ker A-\lambda E$ имеет ортонормированный базис из векторов с вещественными компонентами. Пусть  $\lambda = a+bi$ с $b\neq 0$. Пусть  $v=e_1 + i e_2 $ -- собственный вектор для $\lambda$. Здесь $e_1=\Re v$, а $e_2=\Im v$ -- вещественные вектора. Тогда $\ovl{v}=e_1-ie_2$ собственный вектор для $\ovl{\lambda}=a-bi$. Тогда
$$\lan e_1, e_2\ran = \frac{1}{4} \lan v+\ovl{v}, -i( v-\ovl{v})\ran= \frac{-i}{4}(||v||^2-||\ovl{v}||^2)=0.$$
Покажем, что норма $e_i$ равна $\frac{1}{\sqrt{2}}$, например, для случая $e_1$. 
$$||e_1||^2=\lan e_1, e_1\ran = \frac{1}{4} \lan v+\ovl{v},  v+\ovl{v}\ran= \frac{1}{4}(||v||^2+||\ovl{v}||^2)=\frac{1}{2}.$$

Итого, вектора $\sqrt{2}e_1,\sqrt{2}e_2$ вещественны, а так же независимы ортогональны и нормированы над $\mb C$ и, следовательно, над $\mb R$. Подпространство, порождённое ими инвариантно, относительно $A$. На нём в базисе $e_1, e_2$ матрица $A$ действует как 
$$e_1 \to \frac{1}{2}(\lambda v + \ovl{\lambda}\ovl{v})=ae_1 - be_2 $$
$$e_2 \to \frac{1}{2i}(\lambda v - \ovl{\lambda}\ovl{v})=\frac{1}{2i}( 2i b e_1 + 2i  a e_2) $$
Разумеется, она действует так же и в базисе $\sqrt{2}e_1,\sqrt{2}e_2$. Откуда получаем, что пространство $\lan e_1,e_2\ran $ инвариантно относительно $A$ над $\mb R$, и в базисе $\sqrt{2}e_1,\sqrt{2}e_2$ матрица $A$  действует как  
$$\begin{pmatrix}
a & b\\
-b & a
\end{pmatrix}$$
Из этих соображений построим ортонормированный базис всего $\mb R^n$. Для собственных чисел $\lambda \in R$ возьмём вещественный ортонормированный базис $\Ker A-\lambda E$, а для пары сопряжённых комплексных собственных чисел $\lambda,\ovl{\lambda}$ возьмём $v_1,\dots,v_k$ -- ортонормированный базис $\Ker A-\lambda E$ и по каждому вектору построим вещественные вектора $e_{i,1}=\Re v_i$ и $e_{i,2}=\Im v_i$. Тогда $\sqrt{2}e_{1,1}, \sqrt{2}e_{1,2},\dots, \sqrt{2}e_{k,1},\sqrt{2}e_{k,2}$ -- это ортонормированный набор векторов. При этом, подпространства, порождённые каждой парой векторов с одинаковым первым индексом инвариантны относительно $A$ и линейый оператор, заданный $A$ на этом подпространстве имеет матрицу нужного вида.

Вместе, эти вектора дают искомый ортонормированный базис $R^n$.

\endproof






Теперь можно легко получить характеризацию самосопряжённых, унитарных и вещественных ортогональных.

\thrm Пусть $L$ -- оператор в евклидовом (унитарном) пространстве $V$. Тогда $L$ -- самосопряжённый тогда и только тогда, когда существует ортонормированный базис $V$ состоящий из собственных векторов оператора $L$ и все собственные числа $L$ -- вещественны.
\proof Пусть $L$ оператор на унитарном пространстве. Возьмём ортонормированный базис из его собственных векторов и распишем условие самосопряжённости. Оно означает, что $\ovl{A}^{\top}=A$. Но $A$ диагональна и на диагонали стоят собственные числа. Итого на них получается уравнение $\ovl{\lambda}=\lambda$, что гарантирует их вещественность.

Пусть теперь  $L$ оператор на евклидовом пространстве. Тогда есть ортонормированный базис, в котором матрица $L$ составлена из блоков $1\times 1$ или $2\times 2$ вида 
$$\pmat a& b \\ -b & a \epmat.$$
Но матрица $L$ в ортогональном базисе должна быть симметрична. Отсюда $b=-b$, то есть $b=0$, что и требовалось.
\endproof
\ethrm



\zd Докажите спектральную теорему в случае самосопряжённого оператора напрямую.
\ezd

\thrm Оператор $L$ -- унитарный тогда и только тогда, когда его собственные числа по модулю равны 1 и существует ортонормированный базис из собственных векторов. 
\proof Рассмотрим ортонормированный базис из собственных векторов $e$. Матрица $A$ оператора $L$ в этом базисе диагональна и удовлетворяет соотношению $\ovl{A}^{\top}A=E_n$. Для собственных чисел $A$ это означает, что $$|\lambda|^2=\ovl{\lambda}\lambda=1.$$ 
\endproof
\ethrm






\thrm Оператор $L$ на евклидовом пространстве $V$ ортогональный  тогда и только тогда, когда существует ортонормированный базис в котором матрица $A$ блочно-диагональная, при этом блоки имеют размер $1$ и состоят из $\pm 1$ или имеют размера $2$ и имеют вид
$$\begin{pmatrix}
\cos \varphi & \sin \varphi\\
-\sin \varphi &\cos \varphi
\end{pmatrix}.$$
\ethrm
\proof
По вещественной версии спектральной теоремы для самосопряжённых операторов есть ортонормированный базис в котором матрица $L$ состоит из блоков $1\times 1$ или $2\times 2$ вида 
$$\pmat a& b \\ -b & a \epmat.$$
Но матрица $L$ в ортогональном базисе должна быть ортогональной и, следовательно, вещественной и унитарной. Откуда получаем, что её собственные числа по модулю равны 1. Если эти числа вещественные, то они равны $\pm 1$. Если же они не вещественные, то имеют вид $a+bi=\cos \ffi + i \sin \ffi$, что даёт необходимый вид для $a$ и $b$.

Обратно, оператор $L$ ортогонален так как его матрица ортогональна в ортонормированном базисе по условию теоремы. 
\endproof




\bibliographystyle{alpha}
\bibliography{lectures}









\end{document}
